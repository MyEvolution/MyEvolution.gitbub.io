<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Learning From Dataâ€”â€”Multivariate ACE</title>
      <link href="/2018/12/05/Learning-From-Data%E2%80%94%E2%80%94Multi-Dimensional-ACE/"/>
      <url>/2018/12/05/Learning-From-Data%E2%80%94%E2%80%94Multi-Dimensional-ACE/</url>
      
        <content type="html"><![CDATA[<p>ä¸ŠèŠ‚è¯¾é™¤äº†è¯´äº†softmaxä¸HGRï¼Œè¿˜ä»‹ç»äº†ACEç®—æ³•çš„æ‹“å±•ï¼šå¤šå˜é‡ä¸‹çš„ACEã€‚<br><a id="more"></a><br>ä¹‹å‰çš„ACEæ˜¯ä»ä¸¤ä¸ªå˜é‡$X,Y$ä¹‹é—´çš„ä¿¡æ¯æ¨å¯¼å‡ºæ¥çš„ï¼Œè€Œè¿™æ¬¡è¦æ‹“å±•åˆ°dä¸ªå˜é‡ã€‚å¯ä»¥çœ‹åˆ°çš„æ˜¯ï¼Œè¿™æ—¶å€™æˆ‘ä»¬æ²¡æœ‰æŠŠå“ªä¸ªå˜é‡å½“ä½œæ ‡ç­¾äº†ï¼Œå› æ­¤è¿™æ˜¯éç›‘ç£å­¦ä¹ ã€‚å®é™…ä¸Šæˆ‘è®¤ä¸ºä¹‹å‰çš„ACEä¹Ÿå¯ä»¥è¯´æ˜¯éç›‘ç£å­¦ä¹ ã€‚åˆ†æåˆ°ä¿¡æ¯è®ºå±‚é¢éç›‘ç£å­¦ä¹ å’Œç›‘ç£å­¦ä¹ è”ç³»åˆ°ä¸€èµ·äº†ï¼Œå®ƒä»¬ä¹‹é—´çš„ç•Œé™å˜å¾—æ¯”è¾ƒæ¨¡ç³Šäº†ã€‚</p><p>ç°åœ¨ï¼Œæœ‰ï½„ä¸ªç¦»æ•£å˜é‡ï¼š$X_1,X_2,â€¦,X_d$.ç±»ä¼¼äºä¹‹å‰ï¼Œæˆ‘ä»¬è¦åšçš„æ˜¯ï¼š</p><script type="math/tex; mode=display">\max \mathbb{E}[\sum_{ i \ne j} f_i(X_i) f_j(X_j)]\\s.t. \mathbb{E}[f_i(X_i)] = 0, \mathbb{E}[f_i^2(X_i)] = 1</script><p>è¿™æ—¶å€™çš„$\mathbb{E}[f_i(X_i) f_j(X_j)] = \Psi_i^T B_{ij} \Phi_j$.è¿™é‡Œçš„$B_{ij}$è¡¨ç¤ºçš„æ˜¯ä¸€ä¸ªçŸ©é˜µï¼š</p><script type="math/tex; mode=display">B_{ij,x_i,x_j}  \frac{p_{X_iX)j}(x_i,x_j)}{\sqrt{p_{X_i}(x_i)p_{X_j}(x_j)}},B_{|X_j| \times |X_i|},B_{ij,|X_i| \times |X_j|}.</script><p>è€Œå®šä¹‰$B$çŸ©é˜µä¸ºï¼š</p><script type="math/tex; mode=display">B_{|X_1|+...+|X_m| \times|X_1|+...+|X_m|  } = \begin{bmatrix}0&B_{12}&\cdots&B_{1d}\\B_{21}&0&\cdots&B_{2d}\\\vdots&\vdots&\ddots&\vdots\\B_{d1}&B_{d2}&\cdots&0\end{bmatrix}</script><script type="math/tex; mode=display">\Psi = \begin{bmatrix}\Psi_1^T,\Psi_2^T,...,\Psi_d^T\end{bmatrix}^T\\\Phi = \begin{bmatrix}\Phi_1^T,\Phi_2^T,...,\Phi_d^T\end{bmatrix}</script><p>$\mathbb{E}[\sum_{i \ne j} f_i(X_i)f_j(X_j)] = \Psi ^T B \Phi $<br>ç”±äºï¼š</p><script type="math/tex; mode=display">\mathbb{E}[f_i(X_i) f_j(X_j)] = \Psi_i^T B_{ij} \Phi_j= \Phi_j^T B_{ji} \Psi_i = \mathbb{E}[f_j(X_j) f_i(X_i)] = \Psi_j^T B_{ji} \Phi_i</script><p>æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š$\Phi_i = \Psi_i$ï¼Œä¹Ÿå°±æ˜¯å¯¹äºæ¯ä¸ªå˜é‡æˆ‘ä»¬åªéœ€è¦å­¦ä¹ ä¸€ä¸ªå‡½æ•°$f_i(X_i)$å³å¯ã€‚</p><p>ä¸‹é¢æ˜¯å¤šå˜é‡ACEç®—æ³•çš„è¿‡ç¨‹ï¼š</p><ol><li><p>é€‰æ‹©$f = {f_1,f_2,â€¦,f_g}$ï¼Œè¿™äº›å‡½æ•°ä¸ºnormalizeåçš„å‡½æ•°</p></li><li><p>$f_i(x_i) \leftarrow \mathbb{E}[ \sum_{j\ne i}f_j(X_j)|X_i = x_i]$ </p></li><li><p>normalize. $f_i(X_i) \leftarrow \frac{f_i(X_i)}{\mathbb{E}\left[ \sum_{i=1}^d f_i^2(X_i)\right]}$</p></li></ol><p>ç›´åˆ°æœ€åæ”¶æ•›ã€‚</p><p>ç°åœ¨æˆ‘æƒ³è¯´æ˜ï¼Œå®é™…ä¸Šå¦‚æœé™å®šfä¸ºçº¿æ€§æ˜ å°„ï¼Œé‚£ä¹ˆå¾—åˆ°çš„ç»“æœå®é™…ä¸Šå°±æ˜¯PCAç®—æ³•ã€‚</p><p><a href="https://wlsdzyzl.top/2018/11/19/Learning-From-Data%E2%80%94%E2%80%94PCA/" target="_blank" rel="noopener">PCA</a>æƒ³åšçš„æ˜¯ï¼š</p><script type="math/tex; mode=display">\max \frac{1}{n} \sum_{i=1}^n(X_i^Tu)^2</script><p>è€Œï¼š</p><script type="math/tex; mode=display">\begin{aligned}\frac{1}{n} \sum_{i=1}^n(X_i^Tu)^2 &= \frac{1}{n} \sum_{i=1}^m(\sum_{j=1}^d x_{ij} \mu_j)^2\\&= \frac{1}{n} \sum_{i=1}^n (\sum_{j=1}^d x_{ij}^2 \mu_j^2 + 2\sum_{j \ne q} \underbrace{(x_{ij}\mu_j)}_{f_j(x_ij)}\underbrace{(x_{iq}\mu_q)}_{f_q(x_iq)}) \end{aligned}</script><p>ç”±äºnormalize, æˆ‘ä»¬å¯ä»¥ä½¿å¾—ï¼š</p><script type="math/tex; mode=display">\frac{1}{n}\sum_{i=1}^n \sum_{j=1}^d x_{ij}^2 \mu_j^2 =\sum_{j=1}^d \frac{1}{n}\sum_{i=1}^n  x_{ij}^2 \mu_j^2 = \sum_{j=1}^d \mathbb{E}[f_j^2(X_j)]= d</script><p>è€Œï¼š</p><script type="math/tex; mode=display">\frac{1}{n} \sum_{i=1}^n \sum_{j \ne q} \underbrace{(x_{ij}\mu_j)}_{f_j(x_ij)}\underbrace{(x_{iq}\mu_q)}_{f_q(x_iq)}) = \mathbb{E}[\sum_{j \ne q}f_j(X_j)f_q(X_q)]</script><p>è€Œè¿™æ­£æ˜¯MACEåœ¨åšçš„äº‹æƒ…ã€‚ä¸è¿‡PCAåªèƒ½å‘ç°çº¿æ€§å…³ç³»ï¼Œå› æ­¤å®ƒè¦æ±‚fä¸ºçº¿æ€§æ˜ å°„ã€‚</p><p>æ›´å¤šç»†èŠ‚è¯·å‚è€ƒï¼š</p><p><a href="https://evolution-video.oss-cn-beijing.aliyuncs.com/wlsdzyzl_pdf/An%20information-theoretic%20approach%20to%20unsupervised%20feature%20selection%20for%20high-dimensional%20data_196602880.pdf" target="_blank" rel="noopener">An Information-theoretic Approach to Unsupervised<br>Feature Selection for High-Dimensional Data</a></p>]]></content>
      
      
      <categories>
          
          <category> æ•°æ®å­¦ä¹ è¯¾ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LFD class </tag>
            
            <tag> ACE </tag>
            
            <tag> PCA </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Learning From Dataâ€”â€”derive something from Softmax</title>
      <link href="/2018/12/05/Learning-From-Data%E2%80%94%E2%80%94derive-something-from-Softmax/"/>
      <url>/2018/12/05/Learning-From-Data%E2%80%94%E2%80%94derive-something-from-Softmax/</url>
      
        <content type="html"><![CDATA[<p>è¿™å‘¨çš„æ•°æ®å­¦ä¹ è¯¾æ›´ä¸çŸ¥é“è¯¥èµ·ä»€ä¹ˆé¢˜ç›®äº†ã€‚ä¸»è¦æ˜¯åŠ ä¸Šä¸€äº›å‡è®¾ï¼Œä»Softmaxå‡½æ•°å¼€å§‹æ¨å¯¼ï¼Œæœ€åå¾—åˆ°ä¸€ä¸ªéå¸¸ç®€å•çš„å½¢å¼ï¼Œä»è€Œå¤§å¤§ç®€åŒ–äº†ç®—æ³•ã€‚è¿™æ¬¡çš„derivationå’Œä¸Šç¯‡è®²å¾—ä¸œè¥¿è¿˜æ˜¯æœ‰ä¸€äº›ç›¸å…³çš„ã€‚<br><a id="more"></a></p><h2 id="Review"><a href="#Review" class="headerlink" title="Review"></a>Review</h2><p>é¦–å…ˆå›é¡¾ä¸€ä¸‹ä¸Šç¯‡åšå®¢ä»‹ç»çš„å†…å®¹ï¼Œä»HGR maximal correlationå¼€å§‹æ¨å¯¼ã€‚ä¾ç„¶æ˜¯ç¦»æ•£å˜é‡$X$ä¸$Y$ã€‚ä¸è¿‡ç¨å¾®åšç‚¹æ‹“å±•ï¼Œæˆ‘ä»¬åœ¨æå–ï½˜ï¼Œï½™çš„ä¿¡æ¯æ—¶ï¼ŒæŠŠä»–ä»¬æ˜ å°„åˆ°ä¸€ä¸ªï½‹ç»´åº¦çš„å‘é‡ï¼Œä¹Ÿå°±æ˜¯ï¼š</p><script type="math/tex; mode=display">f(x) \rightarrow \mathbb{R}^k,g(x) \rightarrow \mathbb{R}^k.</script><p>è¿™æ—¶å€™ï¼Œå’Œä¹‹å‰ä¸€æ ·ï¼Œåšä¸€äº›æ¨å¯¼å§ã€‚è¿™æ—¶å€™çš„ç›¸å…³ç³»æ•°å˜æˆäº†ç›¸å…³çŸ©é˜µï¼š</p><script type="math/tex; mode=display">\max Ï_{XY}=\max ğ”¼ p_{XY}[f(x) g(y)^T]</script><p>æˆ‘ä»¬çš„çº¦æŸå˜æˆï¼š</p><script type="math/tex; mode=display">\mathbb{E}[f (x)] = \mathbb{E}[ g(y)] = \mathbf{0}\\\mathbb{E}[ f^2(x)] = \mathbb{E}[ g^2(y)] = I_{k \times k}</script><p>é—®é¢˜æè¿°å˜ä¸ºï¼š</p><script type="math/tex; mode=display">\begin{aligned}\max \Psi^T B \Phi,s.t. &\langle\sqrt{P_X},\Phi\rangle = \langle\sqrt{P_Y},\Psi\rangle = \mathbf{0};\\&\Phi^T \Phi  =  \Psi^T \Psi  = I_{k \times k}.\end{aligned}</script><p>å…¶ä¸­ï¼š</p><script type="math/tex; mode=display">\Phi = \begin{bmatrix}\phi(x_1),\phi(x_2),...,\phi(x_{|X|})\end{bmatrix}^T_{|X|\times k},\\\Psi = \begin{bmatrix}\psi(y_1),\psi(y_2),...,\psi(y_{|Y|})\end{bmatrix}^T_{|Y|\times k},\\B_{y,x} =  \frac{p_{XY}(x,y)}{\sqrt{p_X(x)p_Y(y)}},B_{|Y| \times |X|}.</script><p>è€Œè¿™æ—¶å€™çš„$\Phi$ä¸$\Psi$å®é™…ä¸Šæ˜¯ç”±ï¼¢çš„ç¬¬$2ï¼Œâ€¦,k+1$å³å·¦ç‰¹å¾å‘é‡ç»„æˆ:</p><script type="math/tex; mode=display">\Phi = \begin{bmatrix}\upsilon_2,...,\upsilon_{k+1}\end{bmatrix}\\\Psi = \begin{bmatrix}\mu_2,...,\mu_{k+1}\end{bmatrix}</script><p>$f(x)=\frac{\phi(x)}{\sqrt{p_X(x)}} ,g(y) = \frac{\psi(y)}{\sqrt{p_Y(y)}}$.</p><h2 id="HGR-amp-Softmax"><a href="#HGR-amp-Softmax" class="headerlink" title="HGR &amp; Softmax"></a>HGR &amp; Softmax</h2><p>å‡è®¾$X,Y$æ˜¯ç¦»æ•£çš„ï¼Œå¹¶ä¸”å‡ ä¹ç‹¬ç«‹(weakly dependent),ä¹Ÿå°±æ˜¯$p_{XY}(xy) - p_{X}(x)p_{Y}(y)$éå¸¸å°ã€‚</p><p>è¿˜è®°å¾—softmax function:</p><script type="math/tex; mode=display">Q_{Y|X}(y|X) = \frac{e^{X^TW_y + b_y)}}{\sum_{y' \in \mathcal{Y}} e^{X^TW_{y'}+b_{y'}}}</script><p>åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æŠŠ$X,Y$å†æ¬¡è¿›è¡Œä¿¡æ¯æå–ï¼Œåˆ†å¸ƒä¸º$f(X),g(Y)$. ç”±äº$W_y$ä¸Yå€¼ç›¸å…³ï¼Œæˆ‘ä»¬å¯ä»¥å°†$W_y$çœ‹ä½œæ˜¯g(y)ã€‚å› æ­¤å†™æˆæ›´é€šç”¨çš„å½¢å¼ï¼š</p><script type="math/tex; mode=display">\begin{aligned}Q_{Y|X}(y|x) &= \frac{e^{f^T(x)g(y) + b(y)}}{\sum_{y' \in \mathcal{Y}}e^{f^T(x)g(y')+b(y')}}\\&= \frac{p_Y(y)e^{f^T(x)g(y) + b(y) - \log p_Y(y)}}{\sum_{y' \in \mathcal{Y}}p_Y(y')e^{f^T(x)g(y')+b(y') - \log p_Y(y)}}\end{aligned}</script><p>ç°åœ¨æˆ‘ä»¬å®šä¹‰ï¼š$d(y) \triangleq b(y)-\log p_Y(y)$ï¼Œåˆ™ï¼š</p><script type="math/tex; mode=display">Q_{Y|X}(y|x) = \frac{p_Y(y)e^{f^T(x)g(y) + d(y)}}{\sum_{y' \in \mathcal{Y}} p_Y(y')e^{f^T(x)g(y')+d(y')}}</script><p>å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœ$f = g = d =  0$ï¼Œ$Q_{Y|X}(y|x) = p_Y(y)$.</p><p>ç”±äºæˆ‘ä»¬çš„å‡è®¾å¯ä»¥çŸ¥é“,$p_Y(y) \approx Q_{Y|X}(y|x)$ï¼Œåˆ™$f^T(x)g(y)+ d(y) \approx 0$,æ ¹æ®æ³°å‹’å±•å¼€:</p><script type="math/tex; mode=display">\begin{align}e^{f^T(x)g(y) + d(y)} \approx 1 + f^T(x)g(y)+ d(y)\end{align}</script><p>è€Œ:</p><script type="math/tex; mode=display">\begin{aligned}\sum_{y' \in \mathcal{Y}} p_Y(y')e^{f^T(x)g(y')+d(y')} &\approx \sum_{y' \in \mathcal{Y}} p_Y(y')[ 1+f^T(x)g(y')+d(y')]\\&= 1 + f^T(x)\sum_{y' \in \mathcal Y} p_Y(y')g(y') + \sum_{y' \in \mathcal Y}p_Y(y')d(y')\\&= 1 +f^T(x)\mathbb{E}_Y[g(Y) ] + \mathbb{E}_Y[d(Y) ] \end{aligned}</script><p>è€Œç”±æ³°å‹’å±•å¼€$\frac{1}{1+x} \approx  1-x$å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">\begin{align}\frac{1}{\sum_{ y'\in \mathcal{Y}} p_Y(y' )e^{f^T(x)g(y' )+d(y' )}} \approx 1 -f^T(x)\mathbb{E}_Y[g(Y )] -\mathbb{E}_Y[d(Y )]\end{align}</script><p>ç»“åˆä¸Šé¢çš„(1),(2)ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">\begin{aligned}Q_{Y|X}(y|x) &= \frac{p_Y(y)e^{f^T(x)g(y) + d(y)}}{\sum_{y' \in \mathcal{Y}} p_Y(y')e^{f^T(x)g(y')+d(y')}}\\&\approx p_Y(y)(1 + f^T(x)g(y)+ d(y) )( 1 -f^T(x)\mathbb{E}_Y[g(Y )] -\mathbb{E}_Y[d(Y )])\\& \approx p_Y(y)[1 + f^T(x)g(y) +d(y) - f^T(x)\mathbb{E}_Y[g(Y) ] - \mathbb{E}_Y[d(Y) ]]\\&= p_Y(y)[1+f^T(x)(g(y)-\mathbb{E}_Y[g(Y) ]) + (d(y) - \mathbb{E}_Y[d(Y) ])]\end{aligned}</script><p>ç°åœ¨æˆ‘ä»¬ä»¤$\tilde{g}(y) = g(y) - \mathbb{E}_Y[g(Y)]ï¼Œs.t. \mathbb{E}_Y[\tilde {g}(Y)] = 0$.å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">Q_{Y|X}(y|x) = p_Y(y)[1+ f^T(x)\tilde g (y) + \tilde d (y)]</script><p>ç°åœ¨æˆ‘ä»¬åˆ©ç”¨è¿™ä¸ªå¼å­æ„å»º$empirical risk$ï¼Œå®é™…ä¸Šä¹Ÿå°±æ˜¯$-\frac{1}{n} \sum_{i=1}^n \log Q_{Y|X}(y_i|x_i)$.æœ€å°åŒ–ç»éªŒé£é™©(empirical risk)å®é™…ä¸Šä¹Ÿå°±æ˜¯æœ€å¤§åŒ–$\mathbb{E}_{p_{XY}} [ Q_{Y|X}(y|x)]$ï¼Œä¹Ÿæ˜¯æå¤§ä¼¼ç„¶ä¼°è®¡ã€‚</p><script type="math/tex; mode=display">\begin{aligned}\log Q_{Y|X}(y|x) &= \log p_Y(y) + \log (1+ f^T(x)\tilde{g}(y) + \tilde{d} (y))\\& \approx \log p_Y(y) + f^T(x)\tilde{g}(y) + \tilde{d}(y) -  \frac{1}{2} [(f^T(x)\tilde{g}(y))^2 + \tilde{d^2}(y) + 2 f^T(x)\tilde{g}(y)\tilde{d}(y)]\end{aligned}</script><p>ä¸Šè¿°è¿‡ç¨‹ç”¨åˆ°äº†æ³°å‹’å±•å¼€ï¼š$\log(1+x) \approx x - \frac{x^2}{2}$.</p><p>$\mathbb{E}[\log Q_{Y|X}(Y|X)] = \mathbb{E}[\log p_Y(Y)] + \mathbb{E}[f^T(X)\tilde{g}(Y) ] + \mathbb{E}[\tilde{d}(Y) ] - \mathbb{E}[\frac{1}{2} [(f^T(X)\tilde{g}(Y))^2 + \tilde{d^2}(Y) + 2 f^T(X)\tilde{g}(Y)\tilde{d}(Y)]] $</p><p>ç°åœ¨ï¼Œæˆ‘ä»¬æ¥è¯´æ˜ä¸€äº›å¿…è¦çš„ä¸œè¥¿ï¼šç”±å‡è®¾å¾—åˆ°$p_{XY}(x,y) - p_X(x)p_Y(y) = \epsilon \cdot \square = o(\epsilon)$ï¼Œ$o(\epsilon)$è¡¨ç¤º$\epsilon$çš„æ— ç©·å°é‡ï¼ˆè¿™ä¹ˆè¯´å…¶å®ä¸å‡†ç¡®ï¼Œå› ä¸ºæˆ‘ä»¬æœ€åè¦æœ€å¤§åŒ–è¿™ç§æ— ç©·å°é‡ï¼Œæ˜¾ç„¶ä¸åˆç†ï¼Œå¯ä»¥å½“ä½œä¸ºè¡¡é‡æœ‰å¤šå°çš„é‡çº§ï¼‰ã€‚</p><p>å› ä¸º<script type="math/tex">Q_{Y|X}(y|x) = P_Y(y)[1+f^T(x)\tilde{g}(y) + \tilde{d}(y)] \approx p_{Y},</script><br>åŒç†å¯ä»¥å¾—åˆ°$f^T(x)\tilde g (y)  = o(\epsilon),\tilde d (y) = o(\epsilon)$ï¼Œæˆ‘ä»¬å‡è®¾å¯¹æ‰€æœ‰çš„$f,\tilde g,\tilde d$éƒ½è¿›è¡Œäº†normalizeï¼Œä¹Ÿå°±æ˜¯$\mathbb{E}f = \mathbb{E}\tilde{g} = \mathbb{E}\tilde{d} = 0$,åˆ™ï¼š</p><ul><li><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}[f^T(X)\tilde g(Y) ] &= \sum_{x,y} p_{XY}(x,y)f^T(x)\tilde g(y)\\&= \sum_{x,y}(p_{X}(x)p_{Y}(y)f^T(x)\tilde g(y) + o(\epsilon)f^T(x)\tilde g(y))\\&= \sum_{x}p_{X}(x)f^T(x) \sum_{y}p_{Y}(y)\tilde g(y) + o(\epsilon^2)\\&= o(\epsilon^2).\end{aligned}</script></li><li><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}[(f^T(x)g(y) )^2] &= \sum_{x,y}p_{X}(x)p_{Y}(y)(f^T(x)\tilde g(y))^2 + \sum_{x,y} o(\epsilon)(f^T(x)\tilde g(y))^2\\&= \sum_{x,y}p_{XY}(x,y)(f^T(x)\tilde g(y))^2 + o(\epsilon^3)\end{aligned}</script></li><li><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}[f^T(x) \tilde{g}(y)\tilde{d}(y)] &= \sum_{x,y}p_{X}(x)p_Y(y) f^T(x)\tilde g (y) \tilde d (y) + o(\epsilon)\sum_{x,y}f^T(x)\tilde g (y) \tilde d (y)\\&= \sum_{x}p_{X}(x)f^T(x) \sum_{y}p_{Y}(y)\tilde g(y) \tilde d(y) + o(\epsilon^3)\\&=  o(\epsilon^3)\end{aligned}</script></li></ul><p>è€Œæˆ‘ä»¬çŸ¥é“$o(\epsilon^3)$åœ¨$o(\epsilon^2)$ä¹‹å‰æ˜¯å¯ä»¥è¢«å¿½ç•¥çš„ã€‚å› æ­¤æœ€ç»ˆï¼š</p><script type="math/tex; mode=display">\mathbb{E}[\log Q_{Y|X}] = \mathbb{E}[\log p_Y(Y)] + \mathbb{E}[f^T(X) \tilde g (Y)] - \frac 1 2 \mathbb{E}[(f^T(X) \tilde g (Y))^2] - \frac{1}{2} \mathbb{E}[\tilde{d^2} (y)]</script><p>ä¸Šå¼ä¸­ï¼Œç¬¬ä¸€é¡¹ä¸ºå¸¸æ•°ï¼Œæœ€åä¸€é¡¹ä¸ºéè´Ÿå€¼ï¼Œä¸”ä¸å‰é¢å‡ é¡¹æ²¡æœ‰çº¦æŸå…³ç³»ï¼Œå› æ­¤ä¸ºäº†æœ€å¤§åŒ–ä¸Šå¼åªéœ€ç®€å•ä»¤$\frac{1}{2} \mathbb{E}[\tilde{d^2} (y)]=0$ï¼Œå› æ­¤æœ€ç»ˆæˆ‘ä»¬è¦åšçš„æ˜¯ï¼š</p><script type="math/tex; mode=display">\max_{f,\tilde g} (\underbrace{\mathbb{E}[f^T(x)\tilde g(y)] - \frac{1}{2}\mathbb{E} [(f^T(x)\tilde g(y))^2]}_{\Delta})</script><p>å¦‚æœæˆ‘ä»¬å°†$ \mathbb{E}[f^T(x)\tilde g(y)] $å¯¹$f(x)$æ±‚å¯¼ï¼Œå¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">\frac{\partial \Delta}{\partial f(x)} = 0\\f(x) = \land ^{-1}_{\tilde g (Y)} \mathbb{E}[\tilde g(Y)|X = x]</script><p>å…¶ä¸­$\land ^{-1}_{\tilde g (Y)}=(\mathbb{E}_{p_Y}[\tilde g(Y){\tilde g}^T(Y)])^{-1}$ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å¾—åˆ°äº†æœ€ä½³çš„$f,\tilde g$.</p><p>åŒç†æˆ‘ä»¬ä¹Ÿå¾—åˆ°ï¼š</p><script type="math/tex; mode=display">\tilde {g ^*}(y) = \land ^{-1}_{f(X)} \mathbb{E}[f(X)|Y = y]</script><p>ä¹Ÿå°±æ˜¯å¦‚æœæˆ‘ä»¬å‘softmaxå‡½æ•°ä¸­å–‚å…¥$f(x)$ï¼ˆå½¢å¼å›ºå®šï¼‰,é‚£ä¹ˆsoftmaxå°½é‡åœ¨å­¦çš„ä¸œè¥¿ï¼Œä¹Ÿå°±æ˜¯$W$å®é™…ä¸Šæ˜¯$g^*$ï¼Œå½“ç„¶ä¸ä¸€å®šèƒ½æˆåŠŸå­¦åˆ°è¿™æ ·çš„å½¢å¼ã€‚</p><p>åŒæ ·çš„ï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œsoftmaxå¯ä»¥çœ‹ä½œæ˜¯åœ¨å¯»æ‰¾ï¼¸çš„ç‰¹å¾ï¼Œå®ƒæ‰¾åˆ°çš„æœ€ä½³å½¢å¼åº”è¯¥æ˜¯$f*$.</p><p>ç®€ç›´æ˜¯å¤´å¤§ã€‚å®é™…ä¸Šæˆ‘ä¸èƒ½ä¿è¯è¿™ç¯‡åšå®¢çš„æ­£ç¡®æ€§ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æ•°æ®å­¦ä¹ è¯¾ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LFD class </tag>
            
            <tag> softmax regression </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æ•°å­¦â€”â€”EVDä¸SVD</title>
      <link href="/2018/11/28/%E6%95%B0%E5%AD%A6%E2%80%94%E2%80%94EVD%E4%B8%8ESVD/"/>
      <url>/2018/11/28/%E6%95%B0%E5%AD%A6%E2%80%94%E2%80%94EVD%E4%B8%8ESVD/</url>
      
        <content type="html"><![CDATA[<p>è¿™ä¸ªåšå®¢ä»‹ç»ç‰¹å¾å€¼åˆ†è§£ï¼ˆEigen Value Decomposition,EVDï¼‰å’Œå¥‡å¼‚å€¼åˆ†è§£(Singular Value Decomposition),ã€€å¯ä»¥å½“ä½œæœºå™¨å­¦ä¹ çš„è¡¥å……ææ–™ã€‚<br><a id="more"></a><br>SVDåœ¨çº¿æ€§ä»£æ•°ä¸­æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„ä¸œè¥¿ã€‚Strangæ›¾ç»è¯´è¿‡ï¼šå®ƒè¿œè¿œæ²¡æœ‰å¾—åˆ°å®ƒåº”è¯¥æœ‰çš„åæ°”ã€‚åœ¨è€ƒç ”çš„çº¿æ€§ä»£æ•°ä¸­ä¹Ÿä»æ¥æ²¡æœ‰è§è¿‡SVDçš„èº«å½±ï¼Œä¸è¿‡åœ¨åŸæ¥åœ¨åšä¸€äº›å›¾åƒå¤„ç†ç›¸å…³çš„ç¨‹åºæ—¶å€™ï¼Œç»å¸¸ç”¨åˆ°OpenCVä¸­çš„SVDã€‚</p><h2 id="Theory"><a href="#Theory" class="headerlink" title="Theory"></a>Theory</h2><p>SVDå’Œå¯¹è§’åŒ–ä¸€ä¸ªçŸ©é˜µç´§å¯†è¿æ¥åœ¨ä¸€èµ·ï¼Œå›æƒ³ä¸€ä¸‹ï¼Œå¦‚æœæœ‰ä¸€ä¸ªå®å¯¹ç§°çŸ©é˜µ$A_{n \times n}$ï¼Œåˆ™æœ‰ä¸€ä¸ªæ­£äº¤çŸ©é˜µ$V$å’Œä¸€ä¸ªå¯¹è§’çŸ©é˜µ$D$ï¼Œä½¿å¾—$A = VDV^T$ã€‚è¿™é‡Œ$V$çš„æ¯ä¸€åˆ—å¯¹åº”$A$çš„ç‰¹å¾å€¼ï¼Œå½¢æˆä¸€ä¸ª$\mathbb{R}^n$ç©ºé—´çš„æ­£äº¤åŸºï¼Œè€Œ$D$çš„å¯¹è§’å…ƒç´ æ˜¯$D$çš„ç‰¹å¾å€¼ã€‚è¿™ä¸ªå°±æ˜¯EVDï¼Œeigenvalue decompositionã€‚</p><p>å¯¹äºSVDæ¥è¯´ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªä»»æ„çš„å®çŸ©é˜µ$A_{m \times n}$ï¼Œæœ‰ä¸¤ä¸ªæ­£äº¤çŸ©é˜µï¼š$U$å’Œ$V$ä»¥åŠä¸€ä¸ªå¯¹è§’çŸ©é˜µ$\Sigma$ï¼Œä½¿å¾—$A = U \Sigma V^T$ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œ$U$æ˜¯$m\times m$çŸ©é˜µï¼Œè€Œ$V$æ˜¯$n \times n$çŸ©é˜µï¼Œå› æ­¤$\Sigma$å’Œ$A$çš„å½¢çŠ¶ä¸€æ ·ï¼Œæ˜¯$m\times n$ï¼Œä¸è¿‡å®ƒåªæœ‰å¯¹è§’å…ƒç´ éé›¶ã€‚$\Sigma$çš„å¯¹è§’å…ƒç´ ï¼Œ$\Sigma_{ii} = \sigma_i$ï¼Œå¯ä»¥è¢«å®‰æ’æˆéè´Ÿä»¥åŠé€’å‡çš„é¡ºåºï¼Œå…¶ä¸­å¦‚æœ$\sigma_i&gt;0$ï¼Œå®ƒæ˜¯$A$çš„å¥‡å¼‚å€¼ï¼Œè€Œ$U,V$çš„åˆ—å‘é‡åˆ†åˆ«æ˜¯$A$çš„å·¦å³å¥‡å¼‚å‘é‡ã€‚</p><p>æˆ‘ä»¬å¯ä»¥æŠŠçŸ©é˜µçœ‹ä½œä¸€ä¸ªçº¿æ€§è½¬æ¢ï¼Œé€šè¿‡è¿™ä¸ªæƒ³æ³•æ¥è¿›ä¸€æ­¥å‘ç°EVDå’ŒSVDä¹‹é—´çš„ç›¸ä¼¼ä¹‹å¤„ã€‚</p><h3 id="EVD"><a href="#EVD" class="headerlink" title="EVD"></a>EVD</h3><p>å¯¹äºä¸€ä¸ªå®å¯¹ç§°çŸ©é˜µ$A$ï¼Œè¿™ä¸ªè½¬æ¢æŠŠä¸€ä¸ª$\mathbb{R}^n$å‘é‡ä¾ç„¶è½¬æ¢æˆ$\mathbb{R}^n$ä¸ºå‘é‡ï¼Œä¹Ÿå°±æ˜¯domainå’Œcodomainéƒ½æ˜¯$\mathbb{R}^n$ã€‚å¦å¤–æä¸€ä¸‹ï¼Œå‡å¦‚è½¬æ¢åå¯¹åº”çš„å…ƒç´ ä¸º$T(x)$ï¼Œåˆ™æˆ$T(x)$ä¸ºä¸€ä¸ªimageï¼Œè€Œæ‰€æœ‰imageçš„é›†åˆç§°ä¸º$range$ã€‚$V$é¢˜æä¾›äº†ä¸€ä¸ªéå¸¸å¥½çš„æ­£äº¤åŸºï¼Œå¦‚æœä¸€ä¸ª$\mathbb{R}^n$å‘é‡è¢«è¿™ä¸ªæ­£äº¤åŸºæ¥è¡¨ç¤ºï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹åˆ°ï¼Œè¿™ä¸ªè½¬æ¢æ‰©å¤§äº†ä¸€äº›è¿™ä¸ªå•ä½æ­£äº¤åŸºä¸­çš„æˆåˆ†ï¼Œå¯¹åº”çš„å°±æ˜¯è¾ƒå¤§çš„ç‰¹å¾å€¼$\sigma_i$ã€‚<br>æˆ‘å¸Œæœ›å¯ä»¥ä¸¾ä¸ªä¾‹å­æ¥å¸®åŠ©ç†è§£ï¼Œå‡å¦‚å‘é‡$x \in \mathbb{R}^n$:</p><script type="math/tex; mode=display">\begin{aligned}Ax &= V \Sigma V^Tx\\&= \begin{bmatrix}v_1 & \cdots & v^n\\\end{bmatrix}\begin{bmatrix}\sigma_1 &\cdots& 0\\\vdots & \ddots & \vdots\\0 & \cdots & \sigma_n\end{bmatrix}\begin{bmatrix}v_1^T\\\vdots\\v_n^T\end{bmatrix}x \\&= \begin{bmatrix}\sigma_1v_1&\cdots&\sigma_nv_n\end{bmatrix}\begin{bmatrix}v_1^Tx\\\vdots\\v_n^Tx\end{bmatrix}\\&=\sigma_1v_1v_1^Tx + ...+ \sigma_nv_nv_n^Tx\end{aligned}</script><p>è§‚å¯Ÿä¸Šé¢çš„å±•å¼€ï¼Œå®é™…ä¸Šæˆ‘ä»¬å¯ä»¥å‘ç°çš„æ˜¯ï¼Œ$V^TX$å¾—åˆ°çš„ï¼Œå®é™…ä¸Šæ˜¯åœ¨$V$è¿™ä¸ªæ­£äº¤åŸºä¸‹ï¼Œ$x$çš„â€œåæ ‡å€¼â€ï¼Œè€Œ$V\Sigma$å®é™…ä¸Šæ˜¯ç»è¿‡è½¬æ¢åçš„åæ ‡è½´ï¼Œæ”¾å¤§äº†å¯¹åº”ç‰¹å¾å€¼çš„å€æ•°ã€‚ä»è¿™é‡Œï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾ˆæ¸…æ¥šå¾—çœ‹åˆ°Aè¿™ä¸ªè½¬æ¢åœ¨åšä»€ä¹ˆã€‚</p><h3 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h3><p>ç°åœ¨æˆ‘ä»¬æ¥çœ‹çœ‹ï¼ŒSVDçš„è§£é‡Šã€‚åŒæ ·æˆ‘ä»¬æŠŠä»»æ„ä¸€ä¸ªå®çŸ©é˜µ$A$çœ‹ä½œè½¬æ¢,å®ƒæŠŠ$\mathbb{R}^n$å‘é‡ï¼Œè½¬åŒ–ä¸º$\mathbb{R}^m$ï¼Œè¿™æ„å‘³ç€è¿™ä¸ªè½¬æ¢$domain$æ˜¯$\mathbb{R}^n$ï¼Œè€Œ$codomain$æ˜¯$\mathbb{R}^m$ï¼Œè€Œimage âˆˆ range âˆˆ $\mathbb{R}^m$ã€‚å› æ­¤å¯¹äºdomainå’Œrangeéƒ½æä¸€ä¸ªå•ä½æ­£äº¤åŸºæ‰æ˜¯æ¯”è¾ƒåˆç†çš„ï¼Œè€Œ$U,V$æ°å¥½æä¾›äº†è¿™æ ·çš„åŸºï¼Œåˆ†åˆ«ç”¨æ¥è¡¨ç¤ºdomainçš„å‘é‡å’Œrangeçš„å‘é‡ã€‚é‚£ä¹ˆè¿™ä¸ªè½¬æ¢å°±å’Œä¸Šé¢ä¸€æ ·ï¼Œå˜å¾—æ¯”è¾ƒå®¹æ˜“ç†è§£äº†ï¼Œå®ƒåŒæ ·æ”¾å¤§äº†ä¸€äº›æˆåˆ†ï¼Œå¯¹åº”çš„æ˜¯singular valueçš„å¤§å°ï¼ŒåŒæ—¶æŠ›å¼ƒäº†ä¸€äº›æˆåˆ†ï¼Œå¯¹åº”çš„æ˜¯singular valueä¸º0çš„æ–¹å‘ã€‚SVDå‘Šè¯‰æˆ‘ä»¬æ€æ ·é€‰æ‹©æ­£äº¤åŸºï¼Œä½¿å¾—è½¬æ¢è¢«è¡¨ç¤ºæˆæœ€ç®€å•çš„æ–¹å¼â€”â€”â€”â€”å¯¹è§’çš„å½¢å¼ã€‚</p><p>é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•é€‰æ‹©å¾—åˆ°è¿™äº›åŸºï¼Ÿæƒ³è¦ä½¿ä¸­é—´çŸ©é˜µçš„å½¢å¼æ˜¯å¯¹è§’çš„ï¼Œå¾ˆå®¹æ˜“ï¼Œåªè¦è®©$Av_i = \sigma_iu_i$å³å¯ã€‚</p><p>ä¸ºäº†ç†è§£è¿™ä¸ªï¼Œæˆ‘ä»¬å‡è®¾$m \geq n$ï¼Œé‚£ä¹ˆå¦‚æœ$A v_i = \sigma_i u_i$ï¼Œåˆ™ï¼š</p><script type="math/tex; mode=display">\begin{aligned}AV &= A\begin{bmatrix}v_1&\cdots v_n \end{bmatrix}\\&= \begin{bmatrix}Av_1 & \cdots &Av_n\end{bmatrix}\\&= \begin{bmatrix}\sigma_1 u_1 & \cdots & \sigma_n u_n\end{bmatrix}\\&= U_{m\times n}\Sigma_{n\times_n} \end{aligned}</script><p>è¿™ä¿è¯äº†$\Sigma$çš„å¯¹è§’åŒ–ï¼Œä¸è¿‡æˆ‘ä»¬å¾ˆå®¹æ˜“å‘ç°çš„æ˜¯ä¸Šé¢çš„ç³»æ•°ä¸å¯¹ï¼Œ$\mu$å¹¶ä¸æ»¡è¶³åŸºçš„å®šä¹‰ï¼Œå®ƒæ²¡æœ‰åˆ°è¾¾$m$ä¸ªï¼Œè€Œ$\Sigma$ä¹Ÿéšä¹‹ä¸æ˜¯$m\times n$å½¢çŠ¶çš„çŸ©é˜µã€‚</p><p>å¦‚æœæˆ‘ä»¬å…ˆä¿è¯$V$æ˜¯å•ä½æ­£äº¤åŸºäº†ï¼Œé‚£ä¹ˆ$U_{m\times m}$ä¸­å¾ˆå¤šç»´åº¦æ˜¯æ²¡æœ‰ä»€ä¹ˆæ„ä¹‰çš„ï¼Œå› æ­¤å°†$U$æ‰©å……ä¸ºåŸºï¼Œå¹¶ä¸”å°†$\Sigma$çŸ©é˜µä¹Ÿå¯¹ä¸Šï¼Œå¯¹åº”çš„å…ƒç´ ç½®0å³å¯ã€‚</p><p>å¦‚æœ$m&lt; n$ï¼Œåˆ™æ˜¯ä¸€æ ·çš„é“ç†ï¼Œåªä¸è¿‡è¿™ä¸ªç»´åº¦è¢«$m$é™åˆ¶ä½äº†ã€‚è€Œä¸”å®é™…ä¸Šè¿™ä¸ª$\Sigma$è¿˜è¢«$A$çš„ç§©é™åˆ¶ä½ï¼Œæ¯•ç«Ÿ$\mathbb{R}(AB)\ge \min (\mathbb{R}(A),\mathbb{R}(B))$ï¼Œè€Œ$\mathbb{R}(U)=m,\mathbb{R}(V)=n$ï¼Œè¿™æ„å‘³ç€ï¼Œrå¦‚æœè¦æ±‚å„ä¸ª$\sigma_i$ä¸åŒä¸”$U$ä¸ºä¸€ç»„åŸºï¼Œé‚£ä¹ˆ$\sigma_i &gt; 0;i = 1,â€¦,k;k \ge \mathbb{R}(A)$.</p><p>é€šè¿‡ä¸Šé¢çš„æƒ³æ³•ï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“å°†Aè¡¨ç¤ºä¸ºå¯¹è§’å½¢å¼ã€‚ä¸è¿‡å®é™…ä¸Šï¼Œå³ä½¿ä¿è¯$V$æ˜¯æ­£äº¤åŸºï¼Œæˆ‘ä»¬ä¹Ÿå¾ˆéš¾ä¿è¯$U$æ˜¯æ­£äº¤çš„ã€‚å› æ­¤ä½¿å¾—Vçš„æ­£äº¤æ€§èƒ½åœ¨Aä¸‹ä¾ç„¶ä¿å­˜æ˜¯éå¸¸å…³é”®çš„ã€‚è€Œå®é™…ä¸Šï¼Œ$A^TA$çš„ç‰¹å¾çŸ©é˜µæ­£å¥½æ»¡è¶³è¿™ä¸ªæ¡ä»¶ã€‚</p><p>$A^TA = VDV^T$ï¼Œä¹Ÿå°±æ˜¯å¯¹$A^TA$è¿›è¡ŒEVDã€‚å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">Av_i \cdot Av_j = (Av_i)^T (Av_j) = v_i^TA^TA v_j = v_i^TA^T \lambda_j v_j = \lambda_j v_i v_j = 0</script><p>å¯ä»¥çœ‹åˆ°ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ$\{Av_1,Av_2,â€¦,Av_n\} = \{\sigma_1u_1,â€¦,\sigma_nu_n\}$æ˜¯äº’ç›¸æ˜¯æ­£äº¤çš„ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚è€Œè¿™ä¸ªé›†åˆä¸­çš„éé›¶å‘é‡ï¼Œå½¢æˆäº†ä¸€ä¸ª$A$çš„rangeçš„æ­£äº¤åŸºã€‚å› æ­¤ï¼Œ$A^TA$çš„ç‰¹å¾å‘é‡å’Œå®ƒä»¬ä¸Aå¾—åˆ°çš„imageï¼Œä½¿å¾—$A$å¯ä»¥è¢«è¡¨ç¤ºæˆå¯¹è§’å½¢å¼ã€‚</p><p>æˆ‘ä»¬ç»§ç»­æŠŠä¸Šé¢çš„åˆ†è§£è¡¥å…¨ã€‚æ³¨æ„ï¼Œå¦‚æœ$i = j$ï¼Œé‚£ä¹ˆ$Av_i \cdot Av_j \Vert Av_i \Vert^2= \lambda_i$. ä¸ºäº†è®©$u_i$æ˜¯å•ä½å‘é‡ï¼Œæˆ‘ä»¬å¯¹å…¶è¿›è¡Œæ ‡å‡†åŒ–ï¼š</p><script type="math/tex; mode=display">u_i = \frac{Av_i}{\Vert Av_i\Vert} = \frac{1}{ \sqrt{\lambda_i}} Av_i\\\sigma_i = \sqrt{\lambda_i}</script><p>æˆ‘ä»¬ä¹Ÿå¾ˆå®¹æ˜“æ¨å¯¼ï¼Œ$\lambda_i \ge 0$çš„ä¸ªæ•°æ˜¯$k$ä¸ªï¼Œå¯ä»¥ç”±ç§©å¾—åˆ°ã€‚è€Œ$D$ä¸­ç‰¹å¾å€¼çš„é¡ºåºå¦‚æœæ˜¯æŒ‰ç…§ä»å¤§åˆ°å°çš„é¡ºåºæ’åˆ—ï¼Œé‚£ä¹ˆ$\Sigma$ä¸­ä¹Ÿæ˜¯ä¸€æ ·çš„é€’å‡é¡ºåºã€‚</p><p>å¦‚æœ$k&lt; m$ï¼Œé‚£ä¹ˆå°†$U$æ‰©å±•åˆ°æ­£äº¤åŸºå³å¯ã€‚è¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†æƒ³è¦çš„SVDã€‚æ€»ç»“ä¸€ä¸‹ï¼ŒVæ˜¯$A^TA$çš„ç‰¹å¾å‘é‡ç»„æˆï¼Œè¢«ç§°ä¸ºå³ä¾§çš„å¥‡å¼‚å‘é‡ï¼Œ$\Sigma$ç”±ç‰¹å¾å€¼ç»„æˆï¼Œå…¶ä¸­$\sigma_i = \sqrt{\lambda_i}$ï¼Œè€Œ$U$æ˜¯æ­£äº¤åŒ–$Av_i$çš„ç»“æœï¼Œæœ‰å¿…è¦çš„è¯å†è¿›è¡Œæ‹“å±•ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæ­£äº¤åŸºã€‚</p><p>éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œè®¡ç®—SVDæ˜¯é€šè¿‡è®¡ç®—$A^TA$çš„ç‰¹å¾å€¼å’Œç‰¹å¾çŸ©é˜µï¼Œä½†æ˜¯å®é™…ä¸Šè¿˜æœ‰å…¶ä»–çš„åŠæ³•ï¼Œåœ¨å¾ˆå¤šåº”ç”¨ä¸­SVDçš„å®é™…ç”¨é€”æ˜¯è®¡ç®—å‡º$A^TA$çš„ç‰¹å¾å€¼å’Œç‰¹å¾çŸ©é˜µã€‚</p><p>åœ¨æˆ‘ä»¬çš„æ„é€ æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä»$A^TA$çš„EVDæ¥å¾—åˆ°SVDï¼Œè€Œå®é™…ä¸Šä»SVDçš„è§’åº¦å‡ºå‘ï¼Œæˆ‘ä»¬ä¹Ÿå¾ˆå®¹æ˜“å¾—åˆ°EVDï¼Œå¦‚æœ$A = U\Sigma V^T$ï¼š</p><script type="math/tex; mode=display">A^TA = V\Sigma^T U^T U \Sigma V^T = V \Sigma^T\Sigma V^T</script><p>å¯ä»¥å¾ˆå®¹æ˜“çœ‹åˆ°ä¸Šé¢æ­£æ˜¯$A^TA$çš„EVDï¼ŒåŒç†ä¹Ÿå¾ˆå®¹æ˜“å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">AA^T = U\Sigma V^TV \Sigma^T U^T = U \Sigma \Sigma^T U^T</script><p>è¿™æ„å‘³ç€å®é™…ä¸Š$U$æ­£æ˜¯ç”±$AA^T$çš„ç‰¹å¾å‘é‡ç»„æˆçš„ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œå¦‚æœ$A$æ˜¯æ˜¯å¯¹æˆçŸ©é˜µï¼Œé‚£ä¹ˆ$A^2=A^TA=AA^T$ï¼Œå®ƒä»¬çš„EVDä¹Ÿæ˜¯ç›¸åŒçš„ï¼Œç‰¹å¾å€¼ä¸º$\lambda^2$ï¼Œå…¶ä¸­$\lambda$ä¸ºAçš„ç‰¹å¾å€¼ï¼Œè€Œä¸”æ­¤æ—¶çš„SVDä¸EVDæ˜¯ç­‰ä»·çš„ã€‚</p><h2 id="SVDçš„å‡ ä½•æ„ä¹‰"><a href="#SVDçš„å‡ ä½•æ„ä¹‰" class="headerlink" title="SVDçš„å‡ ä½•æ„ä¹‰"></a>SVDçš„å‡ ä½•æ„ä¹‰</h2><p>æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹å•ä½åœ†ä¸Šçš„ç‚¹åˆ©ç”¨AçŸ©é˜µè¿›è¡Œè½¬æ¢ï¼Œæ¥æ˜ç™½$A$æ˜¯å¦‚ä½•æ‰­æ›²$\mathbb{R}^n$ç©ºé—´çš„ã€‚å‡å¦‚ç‚¹xåœ¨å•ä½åœ†(çƒ)ä¸Šï¼Œæ„å‘³ç€$x = v_1x_1 + v_2x_2+â€¦+v_nx_n$ï¼Œå…¶ä¸­$\sum_{i=1}^nx_i^2 = 1$ï¼Œåˆ™:</p><script type="math/tex; mode=display">Ax = U\Sigma Vx = x_1\sigma_1u_1 + ...+ x_k\sigma_ku_k.</script><p>å‡è®¾$y_i = \sigma_ix_i$ï¼Œåˆ™å•ä½çƒä½“çš„imageä¹Ÿç­‰äº$\sum_{i=1}^k y_iu_i$ï¼Œå…¶ä¸­ï¼š</p><script type="math/tex; mode=display">\sum_{i=1}^k \frac{y_i^2}{\sigma_i^2} = \sum_{i=1}^k x_i^2 \ge 1.</script><p>å¦‚æœ$\mathbb{R}(A)= k = n$ï¼Œé‚£ä¹ˆä¸Šè¿°ä¸ç­‰å¼æ˜¯ç›¸ç­‰çš„ã€‚å…¶ä»–æƒ…å†µä¸‹ï¼Œæ„å‘³ç€ä¸€äº›çº¬åº¦è¢«æŠ›å¼ƒäº†ã€‚ æ‰€ä»¥$A$çš„è½¬æ¢å®é™…ä¸Šæ˜¯å…ˆæŠ›å¼ƒ$n-k$ä¸ªç»´åº¦ï¼Œå°†å…¶å‹ç¼©åˆ°$k$ç»´ï¼Œå†é€šè¿‡$\Sigma$æ¥å¯¹ä¸åŒç»´åº¦çš„æƒå€¼è¿›è¡Œæ”¾ç¼©ï¼Œæœ€åæ‹“å±•çš„$m$ç»´ç©ºé—´ã€‚ä¸‹å›¾å±•ç¤ºäº†è¿™ä¸ªè¿‡ç¨‹(n=m=3,k=2)ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/G6T%7DO%5B7%5BIW70%5DP0VF2D%5DR89.png" alt=""></p><p>æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“å¾—åˆ°ï¼Œ$\Vert A Vert$ï¼Œç®—å­èŒƒæ•°ï¼Œå®šä¹‰ä¸º$\Vert Av \Vert$çš„æœ€å¤§å€¼ï¼Œä¹Ÿå°±æ˜¯$sigma_1$ï¼Œ$A$æœ€å¤§çš„å¥‡å¼‚å€¼ã€‚ä¹Ÿå°±æ˜¯ï¼š$\Vert Ax \Vert \leq \sigma\Vert x \Vert,x \in \mathbb{R}^n$ï¼Œå½“$x$ä¸º$v_1$çš„æ•´æ•°å€æ—¶ç­‰å·æˆç«‹ã€‚</p><h2 id="SVDçš„åˆ†å—çŸ©é˜µä»¥åŠå¤–ç§¯å½¢å¼"><a href="#SVDçš„åˆ†å—çŸ©é˜µä»¥åŠå¤–ç§¯å½¢å¼" class="headerlink" title="SVDçš„åˆ†å—çŸ©é˜µä»¥åŠå¤–ç§¯å½¢å¼"></a>SVDçš„åˆ†å—çŸ©é˜µä»¥åŠå¤–ç§¯å½¢å¼</h2><p>å®é™…ä¸Šï¼ŒSVDå¯ä»¥å†™æˆä¸‹é¢åˆ†å—çŸ©é˜µçš„å½¢å¼ï¼š</p><script type="math/tex; mode=display">A = \left [\begin{array}{ccc|ccc}u_1&\cdots&u_k&u_{k+1}&\cdots&u_n\end{array}\right ]\left [\begin{array}{ccc|c}\sigma_1&\cdots&0&0\\\vdots&\ddots&\vdots&\vdots\\0&\cdots&\sigma_k&0\\\hline0&\cdots&0&0\\\vdots&\vdots&\vdots&\vdots\end{array}\right]\begin{bmatrix}v_1^T\\\vdots\\v_k^T\\\hlinev_{k+1}^T\\\vdots\\v_{n}\end{bmatrix}</script><p>è¿™ä¸ªç»“æœå¯ä»¥å†™æˆï¼š</p><script type="math/tex; mode=display">\begin{aligned}A &= \begin{bmatrix}u_1&\cdots&u_k\end{bmatrix}\begin{bmatrix}\sigma_1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&\sigma_k\end{bmatrix}\begin{bmatrix}v_1^T\\\vdots\\v_k^T\end{bmatrix} + \begin{bmatrix}u_{k+1}&\cdots&u_n\end{bmatrix}\begin{bmatrix}0&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&0\end{bmatrix}\begin{bmatrix}v_{k+1}^T\\\vdots\\v_n^T\end{bmatrix}\\&=\begin{bmatrix}u_1&\cdots&u_k\end{bmatrix}\begin{bmatrix}\sigma_1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&\sigma_k\end{bmatrix}\begin{bmatrix}v_1^T\\\vdots\\v_k^T\end{bmatrix}\end{aligned}</script><p>ä¸Šè¿°å½¢å¼æ˜¯SVDçš„å¦ä¸€ç§è¡¨ç¤ºï¼š$A = U\Sigma V^T$ï¼Œå…¶ä¸­$U$ä¸º$m\times k,U^TU=I$ï¼Œ$\Sigma$ä¸º$k \times k$å¯¹è§’çŸ©é˜µï¼Œå¯¹è§’å…ƒç´ å¤§äº0ï¼Œ$V$ä¸º$n \times k,V^TV = I$.</p><p>æˆ‘ä»¬åœ¨è¿™é‡Œçš„åˆ†å—çŸ©é˜µçš„å…¬å¼å’Œä¸€èˆ¬çš„çŸ©é˜µä¹˜ç§¯æœ‰ç‚¹ä¸ä¸€æ ·ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œä¸¤ä¸ªçŸ©é˜µç›¸ä¹˜$XY$ï¼Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯$X$çš„è¡Œå’Œ$Y$çš„åˆ—ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ç”¨ç›¸åçš„æ–¹æ³•è¡¨ç¤ºã€‚å¦‚æœä¸¤ä¸ªçŸ©é˜µ$X_{m \times k},Y_{k \times n}$,æˆ‘ä»¬ç”¨$x_i$è¡¨ç¤ºXä¸­çš„ç¬¬iåˆ—ï¼Œç”¨$y_i^T$è¡¨ç¤ºYä¸­çš„ç¬¬iè¡Œï¼Œé‚£ä¹ˆï¼š</p><script type="math/tex; mode=display">XY = \sum_{i=1}^k x_iy_i^T</script><p>è€Œ$x_i y_i^T$æˆ‘ä»¬ç§°ä¸ºæ˜¯è¿™ä¸¤ä¸ªå‘é‡çš„å¤–ç§¯ï¼ˆOuter Productï¼‰ï¼Œä¹Ÿå°±æ˜¯çŸ©é˜µä¸­åˆ—ä¹˜ä¸Šè¡Œçš„æƒ…å†µã€‚</p><p>ç°åœ¨å›åˆ°SVDä¸­ï¼Œä»¤ï¼š</p><script type="math/tex; mode=display">X = \begin{bmatrix}u_1&\cdots&u_k\end{bmatrix}\begin{bmatrix}\sigma_1&\cdots&0\\\vdots&\ddots&\vdots\\0&\cdots&\sigma_k\end{bmatrix} = \begin{bmatrix}\sigma_1u_1&\cdots&\sigma_ku_k\end{bmatrix}</script><p>ä»¥åŠï¼š</p><script type="math/tex; mode=display">Y = \begin{bmatrix}v_1^T\\\vdots\\v_k^T\end{bmatrix}</script><p>å¯ä»¥å¾—åˆ°ï¼š<script type="math/tex">A = XY = \sum_{i=1}^k\sigma_iu_iv_i^T.</script></p><p>è¿™æ˜¯SVDçš„å¦ä¸€ç§å½¢å¼ï¼Œå®ƒæä¾›äº†Aå¦‚ä½•è½¬æ¢ä»»ä½•ä¸€ä¸ªå‘é‡$x$çš„å¦ä¸€ç§è§£é‡Šã€‚</p><script type="math/tex; mode=display">Ax = \sum_{i=1}^k\sigma_iu_iv_i^Tx = \sum_{i=1}^kv_i^Tx \sigma_iu_i</script><p>å› ä¸º$v_i^Tx$æ˜¯ä¸€ä¸ªæ ‡é‡ã€‚</p><p>è¿™ä¸ªæ—¶å€™$Ax$è¢«è¡¨è¾¾ä¸º$u_i$çš„çº¿æ€§ç»„åˆã€‚æ‰€ä»¥é€šè¿‡outer product expansionå¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡Aè½¬æ¢å°†xä¸­æ¯ä¸ª$v_i$æˆåˆ†è½¬æ¢æˆ$u_i$æˆåˆ†ï¼Œå¹¶ä¸”ä»¥$\sigma_i$çš„ç³»æ•°æ”¾ç¼©ã€‚</p><p>è¿™ç¯‡åšå®¢åŸºæœ¬ä¸Šæ˜¯ä¸‹é¢æ–‡çŒ®çš„éƒ¨åˆ†ç¿»è¯‘ï¼Œæ›´å¤šå†…å®¹è¯·çœ‹ï¼š<br><a href="https://evolution-video.oss-cn-beijing.aliyuncs.com/wlsdzyzl_pdf/SVD-%5BDan-Kalman%5D.pdf" target="_blank" rel="noopener">SVD</a></p>]]></content>
      
      
      <categories>
          
          <category> æ•°å­¦ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVD </tag>
            
            <tag> mathmatics </tag>
            
            <tag> algebra </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ä¿¡æ¯è®ºâ€”â€”ä¿¡é“åŠå…¶å®¹é‡</title>
      <link href="/2018/11/27/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94%E4%BF%A1%E9%81%93%E5%8F%8A%E5%85%B6%E5%AE%B9%E9%87%8F/"/>
      <url>/2018/11/27/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94%E4%BF%A1%E9%81%93%E5%8F%8A%E5%85%B6%E5%AE%B9%E9%87%8F/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> ä¿¡æ¯è®º </category>
          
      </categories>
      
      
        <tags>
            
            <tag> information theory </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Learning From Dataâ€”â€”Info between X and Y/SVD</title>
      <link href="/2018/11/27/Learning-From-Data%E2%80%94%E2%80%94Info-between-X-and-Y-SVD/"/>
      <url>/2018/11/27/Learning-From-Data%E2%80%94%E2%80%94Info-between-X-and-Y-SVD/</url>
      
        <content type="html"><![CDATA[<p>ä¸Šä¸€å‘¨çš„æ•°æ®å­¦ä¹ ï¼Œæ²¡æœ‰è¯¾ä»¶ï¼Œè€å¸ˆç›´æ¥å¼€å§‹ä»å¤´æ¨äº†ä¸€äº›ç¥å¥‡çš„ä¸œè¥¿å‡ºæ¥ã€‚æ‰€ä»¥æˆ‘ä¹Ÿå¾ˆéš¾ç»™è¿™ç¯‡çš„å†…å®¹èµ·ä¸€ä¸ªé¢˜ç›®ï¼Œå› ä¸ºæ˜¯ä»ï¼¸ä¸ï¼¹ä¹‹é—´çš„ä¿¡æ¯æ¨å¯¼åˆ°SVDçš„ï¼Œå› æ­¤å°±è¿™ä¹ˆå«å§ã€‚<br><a id="more"></a></p><h2 id="info-shared-Between-X-Y"><a href="#info-shared-Between-X-Y" class="headerlink" title="info shared Between X, Y"></a>info shared Between X, Y</h2><p>å‡å¦‚ç°åœ¨æœ‰ä¸¤ä¸ªç¦»æ•£å˜é‡ï¼Œï¼¸ä¸ï¼¹ä»–ä»¬æ˜¯ä¸€ä¸€å¯¹åº”çš„ã€‚åœ¨ä¹‹å‰çš„æœºå™¨å­¦ä¹ é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“ï¼¸æ˜¯è¾“å…¥æ•°æ®ï¼Œï¼¹æ˜¯æ ‡ç­¾ã€‚ç°åœ¨å¿˜æ‰ä¹‹å‰çš„å­¦ä¹ ç®—æ³•ï¼Œä»ä¿¡æ¯çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å¸Œæœ›åšçš„ï¼Œæ˜¯æå–ï¼¸ä¸ï¼¹ä¹‹é—´çš„å…¬å…±ä¿¡æ¯ï¼Œè€Œè¿™éƒ¨åˆ†æ‰æ˜¯å®ƒä»¬æœ‰æŸç§å…³ç³»çš„çœŸæ­£åŸå› ã€‚å› æ­¤ç°åœ¨æˆ‘ä»¬çš„ç›®æ ‡å˜æˆï¼Œä»ï¼¸ä¸ï¼¹çš„è”åˆåˆ†å¸ƒæå–ï¼¸ä¸ï¼¹ä¹‹å‰çš„å…¬å…±ä¿¡æ¯ï¼Œã€‚</p><p>é‚£ä¹ˆé¦–å…ˆé‡åˆ°çš„é—®é¢˜ï¼šæˆ‘ä»¬æ˜¯å¦‚ä½•æå–ï¼¸å’Œï¼¹çš„å„è‡ªçš„ä¿¡æ¯çš„ï¼Ÿæˆ‘ä»¬çš„åšæ³•æ˜¯åˆ©ç”¨ä¸€ä¸ªå‡½æ•°ï¼Œå°±åƒlinear regressionç­‰å­¦ä¹ ç®—æ³•ä¸€æ ·ï¼Œä¹Ÿå°±æ˜¯ï¼š</p><script type="math/tex; mode=display"> X \rightarrow f(X) \rightarrow \mathbb{R}\\ Y \rightarrow g(Y) \rightarrow \mathbb{R}\\</script><p>è¡¡é‡å®ƒä»¬ä¹‹é—´çš„å…±åŒä¿¡æ¯ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ¯”è¾ƒç†Ÿæ‚‰çš„åè¯ï¼šç›¸å…³ç³»æ•°ã€‚æˆ‘ä»¬è¦åšçš„å°±æ˜¯æ‰¾åˆ°åˆé€‚çš„$f,g$å‡½æ•°ï¼Œè®©å®ƒä»¬çš„å…±åŒä¿¡æ¯æœ€å¤§ï¼Œä¹Ÿå°±æ˜¯ç›¸å…³ç³»æ•°è¾¾åˆ°æœ€å¤§ï¼ˆHGR Maximal Correlationï¼‰ï¼š</p><script type="math/tex; mode=display">\max \rho_{XY} = \max \mathbb{E}_{p_{XY}} [f(x)g(y) ]</script><p>å½“ç„¶æˆ‘ä»¬è¿˜æœ‰ä¸€äº›é™åˆ¶æ¡ä»¶ï¼Œæ‰èƒ½è®©ç›¸å…³ç³»æ•°ä¸ºä¸Šå€¼ã€‚åœ¨æ¦‚ç‡è®ºä¸­æˆ‘ä»¬çŸ¥é“ï¼Œç›¸å…³ç³»æ•°çš„æ±‚æ³•ï¼š</p><script type="math/tex; mode=display">\begin{aligned}\rho_{XY} &= \frac{Cov(X,Y)}{\sqrt{Var (X) Var (Y)}}\\&= \frac{\mathbb{E}[XY ] - \mathbb{E}[X ] \mathbb{E}[Y ]}{ \sqrt{\mathbb{E}[X^2 ] - \mathbb{E}^2[X ]}}\end{aligned}</script><p>æ‰€ä»¥å¯ä»¥çœ‹åˆ°çš„æ˜¯ï¼Œæˆ‘ä»¬åº”è¯¥è®©</p><script type="math/tex; mode=display">\mathbb{E}[f(x) ] = \mathbb{E}[g(x) ] = 0,\\\mathbb{E}[f^2(x) ] = \mathbb{E}[g^2(x) ] = 1</script><p>è¿™æ—¶å€™ï¼Œ$0\leq\rho_{XY}\leq 1$.å½“$\rho_{XY} = 0$æ—¶ï¼Œè¯´æ˜äºŒè€…ç‹¬ç«‹ï¼ˆè¦æ³¨æ„ï¼Œè¿™å¹¶ä¸æ„å‘³ç€E(XY) = 0è¯´æ˜ï¼¸ï¼Œï¼¹ç‹¬ç«‹ï¼Œå®é™…ä¸Šç›¸å…³ç³»æ•°ä¸º0æ‰èƒ½è¯æ˜äºŒè€…ç‹¬ç«‹ï¼Œåªä¸è¿‡æˆ‘ä»¬è¿™é‡ŒåŠ ä¸Šäº†é™åˆ¶ï¼Œä½¿å¾—$\rho_{XY} = \mathbb{E}[f(x)g(x) ]$ï¼‰ã€‚</p><p>ç°åœ¨ï¼Œç”¨æ•°å­¦è¯­è¨€æ¥æè¿°è¿™ä¸ªé—®é¢˜ï¼š</p><script type="math/tex; mode=display">\begin{align}\max \mathbb{E}_{p_{XY}} [f(x)g(y) ]& = \sum_{x,y}p_{XY}(x,y)f(x)g(y)\\s.t. &\sum_{x}p_X(x)f(x) = \sum_y p_Y(y)g(y) = 0\\ & \sum_{x}p_X(x)f^2(x) = \sum_y p_Y(y)g^2(y) = 1\end{align}</script><p>ä¸ºäº†æ–¹ä¾¿åé¢çš„è®¡ç®—ï¼Œæˆ‘ä»¬è¦å¯¹åŸå¼è¿›è¡Œä¸€äº›è½¬æ¢ï¼š</p><script type="math/tex; mode=display">\begin{aligned}(1)&= \sum_{x,y} \frac{p_{XY}(x,y)}{\sqrt{p_X(x)p_Y(y)}} [\underbrace{\sqrt{p_X(x)}\cdot f(x)}_{\phi(x)} \underbrace{\sqrt{p_Y(y)}\cdot g(y)}_{\psi(y)}]\\&= \sum_{x,y}  \frac{p_{XY}(x,y)}{\sqrt{p_X(x)p_Y(y)}} \phi(x) \psi(y) = \Psi^TB\Phi \end{aligned}</script><p>ä¸Šå¼ä¸­ï¼š</p><script type="math/tex; mode=display">\Phi = \begin{bmatrix}\phi(x_1),\phi(x_2),...,\phi(x_{|X|})\end{bmatrix}^T_{|X|\times 1},\\\Psi = \begin{bmatrix}\psi(y_1),\psi(y_2),...,\psi(y_{|Y|})\end{bmatrix}^T_{|Y|\times 1},\\B_{y,x} =  \frac{p_{XY}(x,y)}{\sqrt{p_X(x)p_Y(y)}},B_{|Y| \times |X|}.</script><p>ç»è¿‡ä¸Šé¢çš„è½¬æ¢ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">\begin{aligned}(2): &\sum_{x} \sqrt{p_X{x}} \phi(x) = 0 \rightarrow \sqrt{P_X}^T \Phi = 0;\\&\sum_{y} \sqrt{p_Y{y}} \psi(y) = 0 \rightarrow \sqrt{P_Y}^T \Psi = 0;\end{aligned}</script><script type="math/tex; mode=display">(3): \Vert \Phi \Vert^2 = \Vert \Psi \Vert ^2 = 1</script><p>è¿™æ—¶å€™æˆ‘ä»¬çš„é—®é¢˜å˜æˆäº†ï¼š</p><script type="math/tex; mode=display">\begin{aligned}\max \Psi^T B \Phi,s.t. &\langle\sqrt{P_X},\Phi\rangle = \langle\sqrt{P_Y},\Psi\rangle = 0;\\&\Vert \Phi \Vert^2 = \Vert \Psi \Vert^2 = 1.\end{aligned}</script><p>è€Œå®é™…ä¸Šï¼Œä¸Šé¢é—®é¢˜çš„è§£æ­£æ˜¯ï¼¢çŸ©é˜µçš„æœ€å¤§çš„å¥‡å¼‚å‘é‡ã€‚</p><h2 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h2><p>æ¥ä¸‹æ¥ï¼Œæƒ³æä¸€ä¸‹ï¼Œä»€ä¹ˆæ˜¯å¥‡å¼‚å€¼åˆ†è§£(Singular Value Decomposition)ï¼š</p><script type="math/tex; mode=display">B = [uã€€][\Sigma][ã€€v^T],</script><p>å…¶ä¸­$u,v$çš„åˆ—å‘é‡ä¸ºï¼¢çš„å·¦å³å¥‡å¼‚å‘é‡ã€‚</p><p>è€Œå®é™…ä¸Šï¼Œ$\sqrt{P_X},\sqrt{P_Y}$æ­£æ˜¯ï¼¢çŸ©é˜µå¯¹åº”çš„æœ€å¤§çš„å¥‡å¼‚å‘é‡ã€‚ä½†æ˜¯æ˜¾ç„¶ï¼Œæˆ‘ä»¬æ˜¯ä¸èƒ½è®©å®ƒä»¬ä½œä¸º$\Phi,\Psi$çš„å€¼çš„ï¼Œå› ä¸ºè¿™ä¸ªä¸æ»¡è¶³çº¦æŸ(2):</p><script type="math/tex; mode=display">\langle\sqrt{P_X},\Phi\rangle = \langle\sqrt{P_X},\sqrt{P_X}\rangle = 1 \ne 0;\\\langle\sqrt{P_Y},\Psi\rangle = \langle\sqrt{P_Y},\sqrt{P_Y}\rangle =1 \ne 0.</script><p>ä¸è¿‡ï¼Œå¥½çš„æ¶ˆæ¯æ˜¯ï¼Œå®é™…ä¸Š$\Phi,\Psi$æ­£æ˜¯ç¬¬äºŒå¤§çš„å¥‡å¼‚å‘é‡ã€‚å¯ä»¥çœ‹å‡ºæ¥ï¼Œä¸åŒå¥‡å¼‚å‘é‡æ˜¯äº’ç›¸æ­£äº¤çš„ï¼Œè¿™ä¸ªå’Œä¸Šæ¬¡è¯´çš„PCAéå¸¸åƒã€‚å®é™…ä¸Šï¼ŒPCAå’ŒSVDæ˜¯æœ‰åƒä¸ä¸‡ç¼•çš„å…³ç³»çš„ã€‚</p><p>ä»ä¸Šé¢çš„åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">f(x) = \frac{1}{\sqrt{p_X(x)}} \phi^*(x);\\g(y) = \frac{1}{\sqrt{p_Y(y)}} \psi^*(y)</script><p>é‚£ä¹ˆæ¥ä¸‹æ¥ä¸€ä¸ªé—®é¢˜ï¼Œå¦‚ä½•ç›´æ¥ä»dataä¸­è®¡ç®—å‡ºæ¥f,gï¼Ÿ</p><p>æƒ³è¦è§£å†³ä¸Šé¢çš„é—®é¢˜ï¼Œé¦–å…ˆè€ƒè™‘ï¼Œå¦‚ä½•è®¡ç®—ï¼¢çš„å¥‡å¼‚å‘é‡ã€‚æœ‰äººè¯´ï¼Œmatlab, openCV. è¿™é‡Œä»‹ç»ä¸€ä¸ªç®€å•çš„ç®—æ³•ï¼šPower Iteration.</p><p>é¦–å…ˆï¼Œæˆ‘ä»¬è¦çŸ¥é“çš„æ˜¯ï¼Œå®é™…ä¸Šæ±‚å¥‡å¼‚å€¼åˆ†è§£ï¼Œä¹Ÿå°±æ˜¯åœ¨åšPCA(æ±‚ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼)ï¼š</p><script type="math/tex; mode=display">B^TB = v \Sigma^T u^T u \Sigma v^T = v \Sigma^2 v^T,\\BB^T = u \Sigma v^T v \Sigma^T u^T =  u \Sigma^2 u^T</script><p>è¿™é‡Œæä¸‹ç‰¹å¾å‘é‡åˆ†è§£(EVD)ï¼š$X = v D v^T $, å…¶ä¸­$v$ä¸º$X$çš„ç‰¹å¾å‘é‡ç»„æˆçš„çŸ©é˜µï¼Œè€Œ$D$ä¸ºå¯¹è§’çŸ©é˜µï¼Œå¯¹è§’å…ƒç´ ä¸º$X$çš„ç‰¹å¾å€¼ï¼Œ$X$ä¸ºå®å¯¹ç§°çŸ©é˜µã€‚</p><p>æ‰€ä»¥å¯ä»¥ä¸éš¾å¾—åˆ°ï¼Œï¼¢çš„å¥‡å¼‚å‘é‡ï¼Œä¹Ÿå°±æ˜¯$B^TB$ä¸$BB^T$çš„ç‰¹å¾å‘é‡ï¼Œåˆ†åˆ«å¯¹åº”å³ä¾§çš„å’Œå·¦ä¾§çš„å¥‡å¼‚å‘é‡ã€‚</p><p>å› æ­¤æˆ‘ä»¬çš„é—®é¢˜å˜æˆäº†ï¼Œå¦‚ä½•æ±‚eigen vectorï¼Ÿ</p><h3 id="Power-Iteration"><a href="#Power-Iteration" class="headerlink" title="Power Iteration"></a>Power Iteration</h3><p>ç»™å®šä¸€ä¸ªå®å¯¹ç§°çŸ©é˜µï¼­ï¼Œå‡è®¾$M = U \Sigma U^T$, åˆ™å‡è®¾å®ƒçš„ç‰¹å¾å‘é‡ä¸º$\upsilon_1,\upsilon_2,â€¦,\upsilon_n$ï¼Œå¯¹åº”çš„ç‰¹å¾å€¼$\lambda_1,â€¦,\lambda_n $.</p><ol><li>åˆå§‹åŒ–å¾—åˆ°ä¸€ä¸ªå‘é‡$\mu_0$ï¼Œåˆ™$\mu_0 = \alpha_1 \upsilon_1+â€¦+\alpha_n \upsilon_n$.</li><li>$\mu_1 = \frac{M\mu_0}{\Vert M\mu_0 \Vert}$</li><li>è¿­ä»£æ‰§è¡Œï¼“ï¼Œå¾—åˆ°$\mu_n$</li></ol><p>æœ€åè¿™ä¸ªç®—æ³•æ”¶æ•›äºæœ€å¤§çš„ç‰¹å¾å‘é‡ï¼ˆå‰ææ˜¯æœ€å¤§çš„ç‰¹å¾å€¼ä¸¥æ ¼å¤§äºå…¶ä»–çš„ç‰¹å¾å€¼ï¼Œå¹¶ä¸”åˆå§‹å‘é‡åœ¨æœ€å¤§ç‰¹å¾å‘é‡çš„æ–¹å‘ä¸Šåˆ†é‡ä¸ä¸º0ï¼Œè¿™ä¸¤ä¸ªæ¡ä»¶éƒ½æ¯”è¾ƒå¥½æ»¡è¶³ï¼‰ã€‚</p><p>è¿™ä¸ªç®—æ³•ä¸éš¾ç†è§£ï¼š</p><script type="math/tex; mode=display">\begin{aligned}\mu_k &=\frac{M \mu_{k-1}}{\Vert M \mu_{k-1} \Vert} \\&= \frac{M \frac{M\mu_{k-2}}{\Vert M \mu_{k-2}\Vert} }{ \Vert M \frac{M\mu_{k-2}}{\Vert M \mu_{k-2}\Vert}  \Vert  }\\&= \frac{M^2 \mu_{k-2}} {\Vert M^2 \mu_{k-2} \Vert}\\&= \frac{M^k \mu_0}{\Vert  M^k \mu_0\Vert}  \\&= \frac{M^k (\alpha_1 \upsilon_1+...+\alpha_n \upsilon_n)}{\Vert  M^k \mu_0\Vert}\\&= \frac{\alpha_1 \lambda_1^k \upsilon_1+...+\alpha_n \lambda_n^k \upsilon_n}{\Vert  M^k \mu_0\Vert}\\&= \frac{\alpha_1 \lambda_1^k (\upsilon_1+\frac{\alpha_2}{\alpha_1}\cdot \frac{\lambda_2^k}{\lambda_1^k} \upsilon_2+...+ \frac{\alpha_n}{\alpha_1}\cdot \frac{\lambda_n^k}{\lambda_1^k} \upsilon_n  )}{\Vert  M^k \mu_0\Vert}\end{aligned}</script><p>ä¸Šå¼ä¸­$k \rightarrow \infty ,\frac{\lambda_i^k}{\lambda_1^k} \rightarrow 0,i\ne 1$ï¼Œæ‰€ä»¥å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">M^k \mu_0 \rightarrow \alpha_1 \lambda_1^k \upsilon_1.</script><p>æ‰€ä»¥ï¼š</p><script type="math/tex; mode=display">\begin{aligned}\mu_k &= \frac{M^k \mu_0}{\Vert  M^k \mu_0\Vert} \\&\rightarrow  \frac{\alpha_1 \lambda_1^k \upsilon_1}{\Vert\alpha_1 \lambda_1^k \upsilon_1 \Vert}\\&= \frac{ \alpha_1 \lambda_1^k \upsilon_1}{\Vert \alpha_1 \lambda_1^k \Vert} \\&= \upsilon_1\end{aligned}</script><p>ä¸Šé¢é€šè¿‡power iteration,ã€€æˆ‘ä»¬å°±å¾—åˆ°äº†æœ€å¤§çš„ç‰¹å¾å‘é‡ã€‚ä½¿ç”¨Gram Schmidt procedureï¼Œå¯ä»¥æ±‚å¾—å…¶ä»–çš„ç‰¹å¾å‘é‡ã€‚æ¯”è¾ƒå®¹æ˜“ç†è§£ï¼Œæ‰¾åˆ°ä¸€ä¸ªå·²æ±‚å‘é‡å‚ç›´çš„å‘é‡è¿›è¡ŒPower Iterationï¼Œå°±å¯ä»¥æ±‚å¾—ç¬¬äºŒå¤§ç‰¹å¾å‘é‡ï¼Œç‰¹å¾å‘é‡ä»å¤§åˆ°å°ä¾æ¬¡è¢«æ±‚å‡ºã€‚</p><p>ç°åœ¨æˆ‘ä»¬è€ƒè™‘ï¼Œå¦‚ä½•å¾—åˆ°ï¼¢çŸ©é˜µçš„ç¬¬äºŒå¤§å¥‡å¼‚å‘é‡ã€‚</p><ol><li>é€‰æ‹©$\Phi^{(0)}$</li><li>$\Psi^{(0)} = B\Phi^{(0)}$, $\Phi^{(1)} = B^T \Psi^{(0)}$</li><li>è¿­ä»£ç¬¬äºŒæ­¥</li></ol><p>å®é™…ä¸Šç¬¬äºŒæ­¥åšçš„å°±æ˜¯:</p><script type="math/tex; mode=display">\Phi^{(1)} = B^TB \Phi^{(0)}, \Psi^{(1)} = BB^T \Psi^{(0)}</script><p>ä¹Ÿå°±æ˜¯ä¸€ç›´åœ¨åˆ©ç”¨Power Iterationè¿­ä»£æ±‚è§£$B^TB$ä¸$BB^T$çš„ç‰¹å¾å‘é‡.</p><p>ä½†æ˜¯è¿™ä¸ªå¦‚æœå¯¹åˆå§‹çš„å€¼ä¸åŠ ä»¥é™åˆ¶ï¼Œæ±‚å¾—çš„åº”è¯¥æ˜¯ç¬¬ä¸€å¤§ç‰¹å¾å‘é‡ã€‚å¦‚æœ$\Phi^{(0)} \perp \sqrt{P_X}$ï¼Œåˆ™æˆ‘ä»¬å¾—åˆ°äº†ç¬¬äºŒå¤§ç‰¹å¾å‘é‡ã€‚</p><p>é‚£ä¹ˆå¦‚ä½•ç›´æ¥ä»æ•°æ®æ±‚å¾—f,gå‘¢ï¼Ÿç»ˆäºåˆ°äº†ç»ˆæé—®é¢˜ã€‚</p><p>ä¸Šé¢æœ‰è¿™ä¹ˆä¸€ä¸ªè®¡ç®—:$B\Phi$ï¼Œå®é™…ä¸Šæ‹†å¼€çš„è¯ï¼Œæ¯ä¸€è¡Œå°±æ˜¯ï½™å›ºå®šä¹‹åçš„$p(x,y)$ä¸$\Phi$ç›¸ä¹˜ï¼Œæˆ‘ä»¬è®°åš$B \Phi (y)$:</p><script type="math/tex; mode=display">\begin{aligned}B \Phi(y) &= \sum_{x} B(y,x)\phi(x)\\&= \sum_x \frac{p_{XY}(x,y)}{\sqrt{p_X(x)p_Y(y)}} \cdot [\sqrt{p_X(x) f(x)}]\\&=\frac{1}{\sqrt{p_Y{y}}} \sum_x p_{XY}(x,y)f(x)\\&= \sqrt{p_Y(y)} \sum_x \frac{p_{XY}(x,y)}{p_{Y}(y)}f(x)\\&= \sqrt{p_Y(y)} \mathbb{E}[f(x)\vert Y = y] \end{aligned}</script><p>ä»ä¸Šé¢çš„æ¨å¯¼ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š$g(y) = \mathbb{E}[f(X)\vert Y=y ]$.</p><p>æ‰€ä»¥ä»åŸæ¥çš„$ \Psi^{(0)} = B\Phi^{(0)} \rightarrow g^{(0)} = \mathbb{E}[f^{(1)}(x) \vert Y=y]$</p><h2 id="ACEç®—æ³•"><a href="#ACEç®—æ³•" class="headerlink" title="ACEç®—æ³•"></a>ACEç®—æ³•</h2><ol><li>é€‰æ‹©$f^{(0)}(x), s.t. \mathbb{E}[f^{(0)}(X) ] = 0$(æ³¨æ„ï¼Œè¿™é‡Œ$ \mathbb{E}[f^{(0)}(X) ] = P_Xf_X(X) = \sqrt{P_X} \sqrt{P_X}f_X(X) = \sqrt{P_X}\Phi  = 0$ï¼Œä¹Ÿå°±æš—å«äº†$\Phi$ä¸$\sqrt{P_X}$å‚ç›´)</li><li><ul><li>$g^{(0)}(y) = \mathbb{E}[f^{(0)}(x)\vert Y=y]$</li><li>$f^{(1)}(x) = \mathbb{E}[g^{(0)}(y)\vert X = x]$</li></ul></li><li>è¿­ä»£ï¼’</li></ol><p>æœ€åï½†ï¼Œï½‡éƒ½ä¼šæ”¶æ•›åˆ°æœ€ä½³çš„ç»“æœã€‚</p><p>è¿™ä¸ªç®—æ³•è¢«ç§°ä¸ºACEï¼ˆAlternative Conditional Expectationï¼‰ç®—æ³•ï¼Œå½“ç„¶åœ¨å®é™…è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ¯ä¸€æ­¥éƒ½è¿›è¡Œnormalize. è€Œç¬¬äºŒæ­¥ä¸­ï¼Œæ¡ä»¶çš„ï½˜å’Œï½™çš„å€¼ï¼Œæœ‰æ—¶å€™æ˜¯éšæœºé€‰æ‹©ï¼Œåœ¨æ•°æ®é‡å°çš„æ—¶å€™å¯ä»¥å¯¹æ‰€æœ‰çš„ï½˜å’Œï½™éƒ½è¿›è¡Œè®¡ç®—ã€‚</p><p>è¿˜æœ‰ä¸€ç‚¹ï¼Œå¦‚ä½•è®¡ç®—$\mathbb{E}[f^{(i)}(x) \vert Y=y]$?</p><p>å…¶å®å¾ˆç®€å•ï¼šé¦–å…ˆä»$\{(x_1,y_1),â€¦,(x_n,y_n)\}$ä¸­æå–å‡ºæ¥$\{(x_1,y_1),â€¦,(x_k,y_k)\}$,ä½¿å¾—$y_1 = â€¦ = y_k = y$,ç„¶åï¼š</p><script type="math/tex; mode=display">\mathbb{E} = \frac{1}{k}\sum_{i=1}^kf^{(i)}(x_k)</script>]]></content>
      
      
      <categories>
          
          <category> æ•°æ®å­¦ä¹ è¯¾ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LFD class </tag>
            
            <tag> mathematics </tag>
            
            <tag> SVD </tag>
            
            <tag> ACE </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Learning From Dataâ€”â€”Kernel PCA</title>
      <link href="/2018/11/20/Learning-From-Data%E2%80%94%E2%80%94Kernel-PCA/"/>
      <url>/2018/11/20/Learning-From-Data%E2%80%94%E2%80%94Kernel-PCA/</url>
      
        <content type="html"><![CDATA[<p>ä¸Šæ¬¡è¯´äº†å‡ ä¸ªPCAçš„å±€é™æ€§ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªæ˜¯å®ƒåªæ‰¾å„ä¸ªç‰¹å¾ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚å¦‚ä½•æ‹“å±•çº¿æ€§å…³ç³»åˆ°éçº¿æ€§ï¼Œä¼¼ä¹æœ‰ç‚¹æ€è·¯ï¼Œå› ä¸ºä¹‹å‰SVMä¸­è¯´è¿‡ï¼Œå¯ä»¥é€šè¿‡kernelå°†SVMæ‹“å±•åˆ°éçº¿æ€§çš„åˆ†ç±»ã€‚åŒæ ·é€šè¿‡kernelï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ‰¾åˆ°ç‰¹å¾ä¹‹é—´çš„éçº¿æ€§å…³ç³»ï¼Œä»è€Œåˆ©ç”¨PCAè¿›è¡Œæ•°æ®å‹ç¼©ç­‰æ“ä½œã€‚<br><a id="more"></a></p><p>å¯¹äºçº¿æ€§çš„PCAçš„æ‰©å±•ï¼Œå°±æ˜¯å°†PCAæ˜ å°„åˆ°æ›´é«˜ç»´åº¦çš„ç©ºé—´,ä½¿å…¶åœ¨æ›´é«˜ç»´åº¦çš„ç©ºé—´æœ‰çº¿æ€§å…³ç³»ï¼š$\phi:\mathbb{R}^n \rightarrow \mathbb{R}^d,d\ge n$.è€Œå¦‚ä½•æ˜ å°„ï¼Œé€šè¿‡ä¸€ä¸ªkernelå‡½æ•°æ¥å®šä¹‰ï¼š$k(X_i,X_j) = \phi(X_i)^T\phi(X_j)$æˆ–è€…kernelçŸ©é˜µ$K \in \mathbb{R}^{m \times m}$. æ˜ å°„å®Œæˆåï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨æ›´é«˜ç»´åº¦çš„ç©ºé—´å†ä½¿ç”¨æ ‡å‡†çš„PCAç®—æ³•äº†ã€‚ä¸è¿‡è¿™ä¸ªæƒ³æ³•æ˜¯naiveçš„ï¼Œæ¯”å¦‚é«˜æ–¯æ ¸å‡½æ•°ï¼Œæ˜¯æ˜ å°„åˆ°æ— é™ç»´åº¦ï¼Œå¦‚æœç”¨åŸæ¥çš„PCAå»æ±‚ï¼Œæ˜¯ä¸ç°å®çš„ã€‚è€Œä¸”ï¼Œä»ä¸‹é¢çš„åˆ†æå¯ä»¥çœ‹åˆ°ï¼Œç›´æ¥æ±‚ä¹Ÿç”¨ä¸åˆ°kernelå‡½æ•°çš„æ–¹ä¾¿ä¹‹å¤„ã€‚</p><p>æ˜ å°„åçš„æ•°æ®çš„åæ–¹å·®çŸ©é˜µå¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\Sigma = \frac 1 m  \sum_{i=1}^m \phi(X_i) \phi(X_i)^T \in \mathbb{R}^{d\times d}</script><p>å‡å¦‚$(\lambda_k,v_k),k=1,â€¦,d$ä¸º$\sigma$çš„ç‰¹å¾åˆ†è§£ï¼Œåˆ™ï¼š</p><script type="math/tex; mode=display">\Sigma v_k = \lambda_k v_k</script><p>åˆ™åŸæ¥$X_l$çš„åœ¨ç¬¬kä¸ªä¸»è¦æˆåˆ†çš„æŠ•å½±ä¸ºï¼š$\phi(X_l)^Tv_k$. </p><p>åˆ°ç›®å‰ä½ç½®ï¼Œæˆ‘ä»¬éƒ½æ²¡æœ‰ç”¨åˆ°kernelã€‚$\phi(X_i)$æ˜¯éš¾ä»¥è®¡ç®—çš„ï¼Œè€Œkernelæ˜¯å®¹æ˜“è®¡ç®—çš„ã€‚å› æ­¤å¦‚ä½•é¿å…$\phi_(X_i)$çš„è®¡ç®—ï¼Ÿ</p><p>é¦–å…ˆæˆ‘ä»¬çŸ¥é“ï¼š</p><script type="math/tex; mode=display">\begin{align}\Sigma v_k = \left (\sum_{i=1}^m \phi(X_i) \phi(X_i)^T \right ) v_k = \lambda _k v_k\end{align}</script><p>å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å°†$v_k$å†™æˆ$\phi(X_i)^T$çš„ç»„åˆï¼ˆè¿™ä¸ªå¾ˆåƒæ˜¯ä¹‹å‰çš„kernel logistic regressionç”¨çš„trickï¼‰ï¼Œè¿™é‡Œå°±ä¸è¯æ˜ä¸ºä»€ä¹ˆä¸€å®šå¯ä»¥å†™æˆè¿™ä¸ªç»„åˆäº†ï¼Œæˆ‘è§‰å¾—åº”è¯¥ä¼šç”¨åˆ°çº¿æ€§ä»£æ•°çš„ç§©çš„çŸ¥è¯†ã€‚</p><script type="math/tex; mode=display">v_k = \sum_{i = 1} ^m \alpha_i^{(k)} \phi(X_i)</script><p>é‚£ä¹ˆï¼Œä¹‹å‰æåˆ°çš„æŠ•å½±å¯ä»¥å°†ä¸Šå¼å¸¦å…¥ï¼š</p><script type="math/tex; mode=display">\begin{aligned}\phi(X_l)^Tv_k &= \phi(X_l)^T \sum_{i = 1} ^m \alpha_i^{(k)} \phi(X_i)\\&= \sum_{i=1} ^m \alpha_i^{(k)} \phi(X_l)^T\phi(X_i)\\&= \sum_{i=1} ^m \alpha_i^{(k)} k(X_l,X_i)\end{aligned}</script><p>è¿™åˆå¼•å…¥äº†ä¸€ä¸ªé—®é¢˜ï¼šå¦‚ä½•å¾—åˆ°$\alpha_i^{(k)}$?</p><p>å°†$v_k = \sum_{i = 1} ^m \alpha_i^{(k)} \phi(X_i)$å¸¦å…¥(1)å¼,æˆ‘ä»¬å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">\left[\sum_{i=1}^m \phi(X_i) \phi(X_i)^T \right] \left(\sum_{i=1}^m \alpha_i^{(k)} \phi(X_i) \right) = \lambda_k m \left(\sum_{i=1}^m \alpha_i^{(k)} \phi(X_i) \right)\\\Phi(X)^T \Phi(X) \Phi(X)^T \alpha^{(k)} = \lambda_k m  \Phi(X)^T \alpha^{(k)}\\\Phi(X) \Phi(X)^T \alpha^{(k)} = \lambda_k m \alpha^{(k)}\\K \alpha^{(k)} = \lambda_k m \alpha_k,</script><p>ä¸Šå¼ä¸­ï¼š</p><script type="math/tex; mode=display">\alpha ^{(k)} = \begin{bmatrix}a_1^{(k)}\\\vdots\\a_m^{(k)}\end{bmatrix};\Phi(X) = \begin{bmatrix}\phi(X_1)^T\\\vdots\\\phi(X_m)^T\end{bmatrix} \\K = \begin{bmatrix}k(X_1,X_1)&k(X_1,X_2)&\cdots&k(X_1,X_m)\\k(X_2,X_1)&k(X_2,X_2)&\cdots&k(X_2,X_m)\\\vdots&\vdots\ &\ddots&\vdots\\k(X_m,X_1)&k(X_m,X_2)&\cdots&k(X_m,X_m)\end{bmatrix}</script><p>ä¸Šå¼ä¸­$\alpha^{(k)}$å¯ä»¥é€šè¿‡æ±‚è§£Kçš„ç‰¹å¾å‘é‡å¾—åˆ°ï¼Œè€ŒKæ˜¯kernelçŸ©é˜µã€‚</p><p>æˆ‘ä»¬å¸Œæœ›æ­£äº¤åŒ–$\alpha^{(k)}$ï¼Œä½¿å¾—$v_k^Tv_k = 1$ï¼ˆä¸ä¹‹å‰çš„æ ‡å‡†PCAä¸€è‡´ï¼‰.</p><script type="math/tex; mode=display">v_k^T v_k = (\alpha^{(k)})^T \Phi(X) \Phi(X)^T \alpha^{(k)} = \lambda_k m  ((\alpha^{(k)})^T \alpha^{(k)}).</script><p>æ‰€ä»¥$\Vert \alpha^{(k)} \Vert^2 = \frac{1}{\lambda_k m} $.</p><p>æ­¤å¤–ï¼Œå¦‚æœ$\mathbb{E}(\phi(X)) \ne 0$ï¼Œæˆ‘ä»¬è¿˜éœ€è¦ä¸­å¿ƒåŒ–$\phi(X)$:</p><script type="math/tex; mode=display">\tilde{\phi}(X_i) = \phi(X_i) - \frac 1 m   \sum_{l=1}^m \phi(X_i)</script><p>è€Œä¸­å¿ƒåŒ–ä»¥åçš„kernelçŸ©é˜µä¸ºï¼š</p><script type="math/tex; mode=display">\tilde{k}(i,j) = \tilde{\phi}(X_i)^T \tilde{\phi}(X_j)</script><p>å†™æˆçŸ©é˜µå°±æ˜¯ï¼š</p><script type="math/tex; mode=display">\tilde{K} = K -1_m K - K 1_m + 1_mK1_m</script><p>å…¶ä¸­ï¼š</p><script type="math/tex; mode=display">1_m = \begin{bmatrix}\frac 1 m & \cdots &\frac 1 m\\\vdots & \ddots & \vdots\\\frac 1 m & \cdots &\frac 1 m\end{bmatrix} \in \mathbb{R}^{m\times m}</script><p>æˆ‘ä»¬ä»å‰é¢å·²ç»æ¨å¯¼å‡ºæ¥ï¼Œè®¡ç®—kernel PCAçš„$v_k$æœ€ç»ˆéœ€è¦çš„æ˜¯kernelçŸ©é˜µï¼Œå› æ­¤ä½¿ç”¨$\tilde{K}$å»è®¡ç®—PCAå³å¯ã€‚</p><p>ï¼’ä¸ªé—®é¢˜ï¼š</p><ol><li>ä¹‹å‰çš„Xä¸å…‰æ˜¯ä¸­å¿ƒåŒ–çš„ï¼Œè€Œä¸”è¿˜æ˜¯Stdev(X) = 1,åœ¨kernel PCAä¸­æ²¡æœ‰è€ƒè™‘è¿™ä¸€ç‚¹å—ï¼Ÿ</li></ol><p>å¯¹äºkernel PCAçš„æ ‡å‡†åæ–¹å·®åˆ†ææ˜¯éå¸¸å›°éš¾çš„ã€‚ä¸è¿‡åæ–¹å·®çš„æ„ä¹‰åœ¨äºä½¿å¾—æ•°æ®èŒƒå›´ä¸ä¼šå˜åŠ¨è¿‡å¤§ï¼ˆå¦‚èº«é«˜ï½ä¸ºå•ä½ï¼Œå˜åŒ–ç¨‹åº¦å¯èƒ½æ˜¯ï¼‘ä»¥å†…ï¼Œè€Œä½“é‡ä»¥æ–¤ä¸ºå•ä½ï¼Œå˜åŒ–èŒƒå›´ä¸ºå‡ åï¼‰ï¼Œå› æ­¤å¦‚æœå¯¹åŸæ•°æ®è¿›è¡Œåæ–¹å·®å˜0çš„æ“ä½œåï¼Œå‡è®¾kernelæ˜¯å¯¹ç§°çš„ï¼Œå®ƒä»¬çš„å˜åŒ–èŒƒå›´ä¸ä¼šå·®å¾—è¿‡å¤šã€‚</p><ol><li>$ï½^{(k)}$ç®—å‡ºæ¥æœ‰ï½ä¸ªï¼Œä½†æ˜¯æˆ‘ä»¬ä¸ä¸€å®šæŠŠå®ƒæ˜ å°„åˆ°ï½ç»´åº¦çš„ç©ºé—´äº†ã€‚ä¸åƒåŸæ¥çš„PCAï¼Œï½ç»´æœ‰ï½ä¸ªç‰¹å¾å‘é‡ï¼Ÿ</li></ol><p>ä¸€èˆ¬æ¥è¯´æˆ‘ä»¬å‡ç»´æ—¶å€™éƒ½ä¼šå‡åˆ°æ— é™ç»´åº¦ï¼Œå¾ˆå°‘æœ‰æ¯”ï½å°çš„ã€‚è¿™æ—¶å€™å®ƒçš„ç‰¹å¾å‘é‡ä¸ªæ•°å°±å—åˆ°äº†æ•°æ®ä¸ªæ•°çš„é™åˆ¶ã€‚å¦‚æœç»´åº¦æ¯”ï½å°ï¼Œåˆ™ç‰¹å¾å‘é‡ä¸ä¼šçœŸæ­£è¾¾åˆ°ï½ä¸ªã€‚</p><p>æ€»ç»“ä»¥ä¸‹ï¼Œkernel PCAçš„æ­¥éª¤ï¼š</p><ol><li><p>æ±‚å‡ºï¼«</p></li><li><p>æ±‚å‡º$\tilde{K}$</p></li><li><p>æ±‚å‡ºä¸Šé¢çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œå¹¶ä¸”æ ¹æ®ä½¿å¾—ç‰¹å¾å‘é‡çš„é•¿åº¦ç­‰äº$\frac {1}{\sqrt{\lambda_k m}}$ï¼Œå…¶ä¸­$\lambda_km$ä¹Ÿå°±æ˜¯å¯¹åº”çš„ç‰¹å¾å€¼ã€‚</p></li><li><p>æ ¹æ®æ±‚å¾—çš„ç‰¹å¾å‘é‡ï¼Œæ±‚å¾—$v_k$.</p></li></ol><p>Kernel PCAçš„ä¾‹å­ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/KPCA1.png" alt=""></p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/KPCA2.png" alt=""></p><p>kernel PCAç»å¸¸ç”¨äºèšç±»ï¼Œå¼‚å¸¸æ£€æµ‹ç­‰ç­‰ã€‚å®ƒéœ€è¦æ‰¾åˆ°$m \times m$çš„ç‰¹å¾å‘é‡æ¥ä»£æ›¿$n \times n$ã€‚é€šè¿‡æŠ•å½±åˆ°kç»´ä¸»å­ç©ºé—´è¿›è¡Œé™ç»´é€šå¸¸æ˜¯ä¸å¯èƒ½çš„ï¼Œä¹Ÿå°±æ˜¯ä¸€èˆ¬ä¸ç”¨kernel PCAæ¥è¿›è¡Œæ•°æ®å‹ç¼©ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æ•°æ®å­¦ä¹ è¯¾ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LFD class </tag>
            
            <tag> unsupervised learning </tag>
            
            <tag> kernel </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Learning From Dataâ€”â€”PCA</title>
      <link href="/2018/11/19/Learning-From-Data%E2%80%94%E2%80%94PCA/"/>
      <url>/2018/11/19/Learning-From-Data%E2%80%94%E2%80%94PCA/</url>
      
        <content type="html"><![CDATA[<p>ä¸ŠèŠ‚è¯¾é™¤äº†ä»‹ç»äº†K-Meansï¼Œæ›´é‡ç‚¹ä»‹ç»äº†å¦å¤–ä¸€ä¸ªç®—æ³•ï¼ŒPCAï¼ˆPrincipal Component Analysisï¼‰ã€‚<br><a id="more"></a></p><p>PCAä¸­æ–‡åº”è¯¥ç¿»è¯‘ä¸ºä¸»è¦æˆåˆ†åˆ†æã€‚è¿™ä¸ªç¿»è¯‘æ˜¯ç›´ç™½çš„ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½å¾ˆå®¹æ˜“çŸ¥é“çŒœå¾—åˆ°è¿™ä¸ªç®—æ³•åœ¨åšä»€ä¹ˆã€‚</p><p>é¦–å…ˆä¸¾ä¸ªä¾‹å­ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/PCA1.png" alt=""></p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/PCA2.png" alt=""></p><p>ä»ä¸Šé¢çœ‹å‡ºæ¥ï¼Œæœ‰æ—¶å€™å¾ˆå¤šç‰¹å¾åŒ…å«äº†å¾ˆå¤šçš„å†—ä½™ä¿¡æ¯ã€‚å¦‚æ‹¥æŒ¤ç¨‹åº¦å’Œäººæµå¯†åº¦ï¼Œå°±æœ‰å¾ˆå¤§çš„ç›¸å…³æ€§ã€‚</p><p>æˆ‘ä»¬éœ€è¦æ¶ˆé™¤è¿™æ ·çš„ç›¸å…³æ€§ï¼Œå¹¶ä¸”å‡å°‘å™ªå£°ã€‚</p><h2 id="PCAæè¿°"><a href="#PCAæè¿°" class="headerlink" title="PCAæè¿°"></a>PCAæè¿°</h2><p>PCAç®—æ³•çš„æè¿°å¦‚ä¸‹ï¼š</p><p>ç»™å®šè¾“å…¥${X_1,X_2,â€¦,X_m}ï¼ŒX_i \in \mathbb{R}$.æ‰¾åˆ°è¾“å…¥çš„ä¸€ä¸ªçº¿æ€§ï¼Œæ­£äº¤è½¬æ¢Wï¼š$\mathbb{R}^n, \mathbb{R}^k$ã€‚Wå°†æœ€å¤§æ–¹å·®æ–¹å‘ä¸æ–°ç©ºé—´çš„åæ ‡è½´å¯¹å…¶ã€‚</p><p>å¦‚ä¸‹å›¾ï¼šå·¦ä¾§å›¾ç‰‡ä¸­ï¼Œ$x_1$ä¸$x_2$æ˜¯é«˜åº¦ç›¸å…³çš„ï¼Œå³ä¾§å›¾ä¸ºè½¬æ¢è¿‡åçš„zï¼Œå®ƒå‡ ä¹å’Œæ°´å¹³åæ ‡è½´å¹³è¡Œã€‚<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/PCA3.png" alt=""></p><h2 id="æ¨å¯¼PCA"><a href="#æ¨å¯¼PCA" class="headerlink" title="æ¨å¯¼PCA"></a>æ¨å¯¼PCA</h2><p>ä¸ºäº†æ–¹ä¾¿PCAçš„æ¨å¯¼ï¼Œæˆ‘ä»¬é¦–å…ˆä¼šå¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä¹Ÿå°±æ˜¯å¯¹å…¶è¿›è¡Œnormalize,ä½¿å¾—Mean(X) =0,Stdev(X) = 1:</p><script type="math/tex; mode=display">X_i := X_i - Mean(X) \leftarrow recenter\\X_i := X_i / Stdev(X) \leftarrow scale</script><p>Stdev(X)ä¸ºæ ‡å‡†åå·®å‡½æ•°ã€‚</p><p>æˆ‘ä»¬å¸Œæœ›åœ¨è¾“å…¥ä¸­æ‰¾åˆ°è®©å„ä¸ªæ ·æœ¬å˜åŒ–æœ€å¤§çš„æ–¹å‘çš„ä¸»è½´uï¼ˆæ‰¾åˆ°å˜åˆ†å•ä½å‘é‡çš„ä¸»è½´ï¼‰ï¼Œå¦‚ä¸‹å›¾ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/PCA7.png" alt=""></p><p>PCAçš„ç›®æ ‡ï¼š</p><ol><li>æ‰¾åˆ°äº’ç›¸æ­£äº¤çš„ä¸»è¦æˆåˆ†$u_1,u_2,â€¦,u_n$ï¼Œå®ƒä»¬ä¹‹é—´äº’ä¸ç›¸å…³ã€‚</li><li>å¤§å¤šæ•°Xä¸­çš„å˜åŒ–ä¼šè¢«kä¸ªä¸»è¦æˆåˆ†ä»£è¡¨äº†ï¼Œè¿™é‡Œçš„$k &lt;&lt; n$.</li></ol><p>æ ¹æ®PCAçš„ç›®æ ‡ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†æPCAçš„ä¸»è¦æ­¥éª¤ï¼š</p><ol><li>æ‰¾åˆ°Xåœ¨æŸä¸ªå‘é‡ä¸Šçš„æŠ•å½±ï¼Œä½¿å¾—$u_1^TX$æœ‰æœ€å¤§çš„æ–¹å·®ã€‚</li><li>å¯¹äºj=2,â€¦nï¼Œç»§ç»­ä¸Šé¢çš„æ­¥éª¤ï¼Œæ‰¾åˆ°Xåœ¨æŸä¸ªå‘é‡ä¸Šçš„æŠ•å½±ï¼ˆä¸ä¹‹å‰çš„å‘é‡æ­£äº¤ï¼‰ï¼Œä½¿å¾—$u_j^TX$æœ‰æœ€å¤§çš„æ–¹å·®ï¼Œå†æ¬¡å¼ºè°ƒï¼š$u_j$ä¸$u_1,â€¦,u_{j-1}$æ­£äº¤ã€‚</li></ol><p>å› ä¸º$\Vert u \Vert = 1$,åˆ™$X_i$åœ¨$u$ä¸Šçš„æŠ•å½±é•¿åº¦ä¸ºï¼š$X_i^Tu$.è€Œè¿™äº›æŠ•å½±çš„æ–¹å·®è®¡ç®—ç»“æœå¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\begin{aligned}\frac 1 m \sum_{i=1}^m (X_i^T u)^2 &= \frac 1 m \sum{i=1}^m u^TX_iX_i^T u \\&= u^T (\sum_{i=1}^m X_i X_i^T)u\\&= u^T \Sigma u \end{aligned}</script><p>è¿™é‡Œçš„$\Sigma$æ˜¯$X$çš„åæ–¹å·®çŸ©é˜µã€‚</p><p>æ‰¾åˆ°å•ä½å‘é‡$u_1$ä½¿å¾—æŠ•å½±çš„æ–¹å·®æœ€å¤§ï¼Œå¯ä»¥ç”¨æ•°å­¦è¯­è¨€æè¿°å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">u_1 =  argmax_{u:\Vert u \Vert = 1} u^T\Sigma u</script><p>$u_i$æ˜¯Xçš„ç¬¬iä¸ªä¸»è¦æˆåˆ†ã€‚</p><p>å¦‚ä½•æ±‚è§£$u_i$å‘¢ï¼Ÿé¦–å…ˆï¼Œæ—¢ç„¶$u_i$è¦ä¸ä¹‹å‰çš„æ­£äº¤ï¼Œå› æ­¤è¿™ä¸ªæ±‚è§£é¡ºåºæ˜¯ä»1åˆ°nã€‚ç°åœ¨æˆ‘ä»¬æ¥åˆ†æ$u_1$ã€‚</p><h4 id="å‘½é¢˜1"><a href="#å‘½é¢˜1" class="headerlink" title="å‘½é¢˜1"></a>å‘½é¢˜1</h4><p>$u_1$æ˜¯åæ–¹å·®çŸ©é˜µæœ€å¤§çš„ç‰¹å¾å‘é‡(eigen vector)ã€‚</p><p>è¯æ˜å¦‚ä¸‹ï¼š</p><p>é¦–å…ˆï¼Œæ ¹æ®æ•°å­¦æè¿°æ„å»ºLagrange functionï¼š</p><script type="math/tex; mode=display">L(u) = -u^T \Sigma u + \beta (u^Tu - 1)</script><p>to minimize $L(u)$:</p><script type="math/tex; mode=display">\frac{\partial L(u)} {\partial u} = -\Sigma u + \beta u = 0</script><p>å› æ­¤æˆ‘ä»¬å¯ä»¥ç¡®å®šäº†uæ˜¯ä¸€ä¸ª$\Sigma$çš„ç‰¹å¾å‘é‡ã€‚</p><p>åŒæ—¶,æŠ•å½±æ–¹å·®ç­‰äº$u^T \Sigma u = u^T \beta u = \beta $.</p><p>ä¸ºäº†è®©æ–¹å·®æœ€å¤§ï¼Œä¹Ÿå°±æ˜¯$\beta$æœ€å¤§ã€‚è€Œæœ€å¤§çš„ç‰¹å¾å€¼å¯¹åº”è¿™æœ€å¤§çš„ç‰¹å¾å‘é‡ã€‚</p><h4 id="å‘½é¢˜2"><a href="#å‘½é¢˜2" class="headerlink" title="å‘½é¢˜2"></a>å‘½é¢˜2</h4><p>ç¬¬jä¸ªXçš„ä¸»è¦æˆåˆ†ï¼Œä¹Ÿå°±æ˜¯$u_j$ä¸º$\Sigma$çš„ç¬¬jä¸ªæœ€å¤§çš„ç‰¹å¾å‘é‡ã€‚</p><p>ä¸ºäº†ç®€åŒ–é—®é¢˜ï¼Œå…ˆå†™å‡ºç¬¬äºŒä¸ªä¸»è¦æˆåˆ†çš„æ•°å­¦æè¿°ï¼š</p><script type="math/tex; mode=display">u = argmax_{u:\Vert u\Vert = 1;u_1^T u = 0} u^T \Sigma u</script><p>åŒæ ·åœ°æ„å»ºLagrange Functionï¼š</p><script type="math/tex; mode=display">L(u) = -u^T\Sigma u + \beta_1 (u^Tu-1 ) + \beta_2 (u_1^Tu)</script><script type="math/tex; mode=display">\frac{\partial L(u)} {\partial u} = -\Sigma u + \beta_1 u + \beta_2 u_1 = 0</script><p>ä¸Šå¼ä¸­ï¼Œæˆ‘ä»¬çŸ¥é“ä¸¤ä¸ªäº’ç›¸æ­£äº¤çš„éé›¶å‘é‡åŠ èµ·æ¥ä¸å¯èƒ½ä¸º0.æ‰€ä»¥å¾—åˆ°ï¼š</p><p>$\beta_2 = 0, \Sigma u = \beta_1 u $</p><p>æ‰€ä»¥æŒ‰ç…§è¯æ˜å‘½é¢˜1åŒæ ·çš„æ­¥éª¤ï¼Œæˆ‘ä»¬å°±è¯æ˜äº†å‘½é¢˜2ä¸­çš„ç¬¬äºŒä¸ªä¸»è¦æˆåˆ†æ˜¯æˆç«‹çš„.</p><p>åŒæ ·çš„è¯æ˜æ–¹æ³•å¯ä»¥ç»§ç»­æ‹“å±•ï¼Œ$j=3,â€¦,n$ï¼Œéƒ½æ˜¯æˆç«‹çš„ã€‚</p><h2 id="PCAçš„æ€§è´¨"><a href="#PCAçš„æ€§è´¨" class="headerlink" title="PCAçš„æ€§è´¨"></a>PCAçš„æ€§è´¨</h2><p>ä»ä¸Šé¢çš„æ¨å¯¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°çš„PCAçš„ä¸‹é¢å‡ ä¸ªæ€§è´¨ï¼š</p><ul><li>ä¸»è¦æˆåˆ†æŠ•å½±çš„æ–¹å·®åˆ†åˆ«ä¸ºï¼š</li></ul><p>$Var(X^Tu_j) = u_j^T \Sigma u_j = \lambda_j,j=1,2,â€¦,n$</p><ul><li>ä¸åŒæ–¹å·®çš„ç™¾åˆ†æ¯”$\frac {\lambda_j}{\sum_{i=1} ^n \lambda _i}$ä¹Ÿå°±æ˜¯ä¸»è¦æˆåˆ†çš„æ‰€å æ¯”é‡ï¼Œä¹Ÿè¯´æ˜äº†å„ä¸ªä¸»è¦æˆåˆ†ä¹‹é—´æ˜¯ä¸ç›¸å…³çš„ã€‚</li></ul><h2 id="PCAæŠ•å½±"><a href="#PCAæŠ•å½±" class="headerlink" title="PCAæŠ•å½±"></a>PCAæŠ•å½±</h2><p>ç¡®å®šä¸»è¦æˆåˆ†åï¼Œæˆ‘ä»¬é€šè¿‡å°†åŸæ•°æ®å¯¹ä¸»è¦æˆåˆ†æŠ•å½±æ¥å¾—åˆ°æ•°æ®çš„å‹ç¼©ç­‰æ•ˆæœã€‚ä¹Ÿå°±æ˜¯ï¼š</p><p>$Z_i = [X_{i}^T u_1,X_i^T u_2,â€¦,X_i^T u_n]$</p><p>ä½¿ç”¨çŸ©é˜µå½¢å¼ï¼š</p><script type="math/tex; mode=display">\begin{aligned}Z &= \begin{bmatrix}x_{1}^T&X_{2}^T&...&X_{m}^T\end{bmatrix}\begin{bmatrix} |&|&...&|\\u_1 & u_2& ... & u_n\\  |&|&...&|\end{bmatrix}&=XU\end{aligned}</script><p>æˆ–è€…$Z_i = U^TX_i$ã€‚</p><p>æˆ‘ä»¬å¯ä»¥çœ‹å‡ºæ¥ï¼Œ$Z_i$åŒæ ·æ˜¯nç»´åº¦çš„ã€‚è€Œæˆªæ–­è½¬æ¢$Z_k = XU_k$åªä¿ç•™å‰kä¸ªä¸»è¦æˆåˆ†ï¼Œç”¨æ¥åšç»´åº¦çš„å‹ç¼©ï¼Œå› ä¸ºå‰kä¸ªä¸»è¦æˆåˆ†å¾€å¾€å äº†å†…å®¹çš„å¤§éƒ¨åˆ†ã€‚</p><h2 id="PCAåœ¨åšä»€ä¹ˆï¼Ÿ"><a href="#PCAåœ¨åšä»€ä¹ˆï¼Ÿ" class="headerlink" title="PCAåœ¨åšä»€ä¹ˆï¼Ÿ"></a>PCAåœ¨åšä»€ä¹ˆï¼Ÿ</h2><p>PCAåˆ é™¤äº†è¾“å…¥Xä¸­çš„å†—ä½™æ•°æ®ï¼š</p><p>å¦‚æœç»è¿‡è½¬æ¢åä¸ºZï¼Œåˆ™ $cov(Z) = \frac 1 n Z^T Z = \frac 1 n (XW)^T (XW) = \frac 1 n W^T(X^TX)W  = \frac 1 n W^T\Sigma W$.</p><p>ç”±äº$\Sigma$æ˜¯å¯¹ç§°çŸ©é˜µï¼Œå› æ­¤å®ƒæœ‰å®ç‰¹å¾å€¼ã€‚å®ƒçš„ç‰¹å¾åˆ†è§£(eigen decomposition)ä¸ºï¼š</p><script type="math/tex; mode=display">\Sigma = W Î› W^Tï¼Œ\\where~W = \begin{bmatrix} |&|&...&|\\u_1 & u_2& ... & u_n\\  |&|&...&|\end{bmatrix},Î› = \begin{bmatrix}\lambda _1 &0&\cdots&0\\0&\lambda_2&\cdots&0\\\vdots & \vdots& \ddots &\vdots\\0&0&\cdots&\lambda_n  \end{bmatrix}</script><p>å› æ­¤ï¼š $cov(Z) = W^TW Î› W^TW = Î› $.ä¸»æˆåˆ†å˜æ¢XWå¯¹è§’åŒ–äº†Xçš„æ ·æœ¬åæ–¹å·®çŸ©é˜µ.</p><p>PCAçš„è‘—åä¾‹å­ï¼šIris Datasetï¼ŒEigenFacesã€‚</p><h2 id="PCAçš„é™åˆ¶"><a href="#PCAçš„é™åˆ¶" class="headerlink" title="PCAçš„é™åˆ¶"></a>PCAçš„é™åˆ¶</h2><p>PCAå¾ˆæœ‰ç”¨ï¼Œä½†æ˜¯å®ƒä¹Ÿæœ‰ä¸€äº›æ˜æ˜¾çš„ç¼ºé™·ï¼š</p><ul><li>åªè€ƒè™‘çº¿æ€§å…³ç³»</li><li>å‡è®¾æ•°æ®æ˜¯çœŸå®å¹¶ä¸”è¿ç»­çš„</li><li>å‡è®¾è¾“å…¥ç©ºé—´è¿‘ä¼¼æ­£æ€åˆ†å¸ƒï¼ˆä¸è¿‡åœ¨éæ­£æ€åˆ†å¸ƒä¸­ä¹Ÿå¯èƒ½å·¥ä½œå¾—å¾ˆå¥½ï¼‰<br>ä¸‹é¢æ˜¯ä¸€ä¸ªéæ­£æ€åˆ†å¸ƒçš„ä¾‹å­ï¼š</li></ul><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/PCA6.png" alt=""></p><p>è¾“å…¥ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/PCA4.png" alt=""></p><p>PCAæŠ•å½±ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/PCA5.png" alt=""></p><p>ä¸‹æ¬¡æˆ‘ä»¬å°†ä¸»è¦è¯´æ˜ä¸€ä¸‹ç¬¬ä¸€ä¸ªç¼ºé™·çš„è§£å†³åŠæ³•ï¼škernel PCAã€‚</p><h2 id="k-meansä¸PCAçš„å¯¹æ¯”"><a href="#k-meansä¸PCAçš„å¯¹æ¯”" class="headerlink" title="k-meansä¸PCAçš„å¯¹æ¯”"></a>k-meansä¸PCAçš„å¯¹æ¯”</h2><p>Unsupervised learning algorithm:</p><div class="table-container"><table><thead><tr><th>algorithm</th><th style="text-align:center">low dimension</th><th style="text-align:center">sparse</th><th style="text-align:center">disentangle variations</th></tr></thead><tbody><tr><td>k-means</td><td style="text-align:center">no</td><td style="text-align:center">yes</td><td style="text-align:center">no</td></tr><tr><td>PCA</td><td style="text-align:center">yes</td><td style="text-align:center">no</td><td style="text-align:center">yes</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> æ•°æ®å­¦ä¹ è¯¾ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LFD class </tag>
            
            <tag> unsupervised learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Learning From Dataâ€”â€”K-Means Clustering</title>
      <link href="/2018/11/19/Learning-From-Data%E2%80%94%E2%80%94K-Means-Clustering/"/>
      <url>/2018/11/19/Learning-From-Data%E2%80%94%E2%80%94K-Means-Clustering/</url>
      
        <content type="html"><![CDATA[<p>è½¬çœ¼é—´è¿™ä¸€ä¸ªå­¦æœŸå·²ç»è¿‡äº†ä¸€åŠäº†ã€‚å¼€å§‹å­¦ä¹ éç›‘ç£å­¦ä¹ ç®—æ³•äº†ã€‚ç¬¬ä¸€ä¸ªä»‹ç»çš„ç®—æ³•ï¼Œæ˜¯K-Meansèšç±»ç®—æ³•ã€‚</p><a id="more"></a><p>è¿™æ˜¯ç¬¬ä¸€ç¯‡è®²unsupervised learningçš„ç®—æ³•ï¼Œå…ˆè¯´ä¸€ä¸‹ï¼Œunsupervised learningè¯•å›¾åœ¨åšäº›ä»€ä¹ˆã€‚</p><p>éç›‘ç£å­¦ä¹ å’Œç›‘ç£å­¦ä¹ å¾ˆåƒï¼Œéƒ½æ˜¯å¸Œæœ›å­¦ä¹ å‡ºä¸€ä¸ªæ¨¡å‹ï¼Œ$x \rightarrow f(x)$ã€‚ä¸è¿‡éç›‘ç£æ²¡æœ‰æ ‡ç­¾äº†ã€‚æ‰€ä»¥ä¸€èˆ¬æ¥è¯´éç›‘ç£å­¦ä¹ æ›´éš¾ã€‚</p><p>éç›‘ç£å­¦ä¹ çš„ç›®æ ‡ï¼Œæ˜¯æƒ³æ‰¾åˆ°è¾“å…¥ç‰¹å¾Xçš„ä»£è¡¨ï¼ˆRepresentationï¼‰ã€‚Representation learning problemå¯ä»¥æè¿°ä¸ºï¼šç»™å®šäº†è¾“å…¥Xï¼Œæ‰¾åˆ°æ›´ç®€å•çš„ç‰¹å¾Zæ¥ä¿å­˜å’ŒXä¸€æ ·çš„ä¿¡æ¯ã€‚</p><p>è¯´äº†è¿™ä¹ˆå¤šï¼Œè¿™å“ªåƒæ˜¯ä¸€ä¸ªå­¦ä¹ ç®—æ³•ï¼Œè¿™æ›´åƒæ˜¯å‹ç¼©ç®—æ³•ã€‚å®é™…ä¸Šï¼Œéç›‘ç£å­¦ä¹ å°±å¹¿æ³›åº”ç”¨äºå‹ç¼©ã€‚</p><p>ä¸€èˆ¬ï¼Œå¥½çš„representationæœ‰ä»¥ä¸‹å‡ ä¸ªç‰¹ç‚¹ï¼š</p><ol><li><p>ä½ç»´åº¦ï¼šæŠŠä¿¡æ¯å‹ç¼©å¾—æ›´å°</p></li><li><p>ç¨€ç–ä»£è¡¨ï¼šæ¯”å¦‚ä¸€ä¸ªçŸ©é˜µï¼Œå¤§éƒ¨åˆ†é¡¹éƒ½æ˜¯0,å¯ä»¥å¤§å¤§ç®€åŒ–è®¡ç®—ï¼Œç§°ä¸ºç¨€ç–çŸ©é˜µã€‚è€Œç¨€ç–ä»£è¡¨ä¹Ÿå³å¤§éƒ¨åˆ†æ•°æ®çš„ç‰¹å¾çš„å¤§éƒ¨åˆ†é¡¹éƒ½æ˜¯0.</p></li><li><p>ç‹¬ç«‹ä»£è¡¨ï¼šdisentangle the source of variations.è¿™ä¸ªç¿»è¯‘æ˜¯è§£å¼€å˜å¼‚ä¹‹æºâ€¦â€¦å¥½ä¸­äºŒçš„æ„Ÿè§‰ã€‚è¿™ä¸ªæ˜¯ä»€ä¹ˆæ„æ€æˆ‘ä¹Ÿä¸æ˜¯å¾ˆç†è§£ã€‚</p></li></ol><p>éç›‘ç£å­¦ä¹ å¹¿æ³›ç”¨äºæ•°æ®å‹ç¼©ï¼Œå¼‚å¸¸æ£€æµ‹ï¼Œåˆ†ç±»èšç±»ç­‰ç­‰ã€‚</p><p>è€Œè¿™æ¬¡è¦è¯´çš„ç®—æ³•ï¼šK-Meansç®—æ³•ï¼Œæ˜¯ä¸€ä¸ªèšç±»ç®—æ³•ã€‚</p><p>èšç±»çš„ç›®æ ‡æ˜¯ç»™å®šä¸€ç»„è¾“å…¥ç‰¹å¾ï¼Œå°†æ•°æ®åˆ†æˆå‡ ç»„ç»“åˆåœ¨ä¸€èµ·çš„ç°‡ã€‚</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/k_means_1.png" alt="k-means-1"></p><p>èšç±»çš„ç†æƒ³ç»“æœåº”æ»¡è¶³ä¸‹é¢çš„æ¡ä»¶ï¼š</p><ul><li>åœ¨åŒä¸€ä¸ªç°‡ä¸­çš„å¯¹è±¡ç›¸æ¯”äºä¸åŒçš„ç°‡çš„å¯¹è±¡æ¥è¯´æ›´ä¸ºç›¸ä¼¼ã€‚</li></ul><h2 id="K-Meansé—®é¢˜æè¿°"><a href="#K-Meansé—®é¢˜æè¿°" class="headerlink" title="K-Meansé—®é¢˜æè¿°"></a>K-Meansé—®é¢˜æè¿°</h2><p>ç»™å®šnä¸ªæ ·æœ¬ï¼š${X_1,X_2,â€¦,X_n}$ï¼Œå°†å®ƒä»¬åˆ†ä¸ºkä¸ªç±»ï¼ˆ$k\leq n$ï¼‰$C_1,C_2,â€¦,C_k$ï¼Œä½¿å¾—ç°‡å†…å¹³æ–¹å’Œï¼ˆwithin-cluster sum of squaresï¼ŒWCSSï¼‰è¾¾åˆ°æœ€å°ï¼š</p><script type="math/tex; mode=display">argmin_C \sum_{j=1}^k \sum{x \in C_j} \Vert x - \mu_j \Vert ^2</script><p>$\mu_j$æ˜¯ä¸€ä¸ªç°‡çš„ä¸­å¿ƒï¼š$\mu_j =\frac{1}{ \vert C_j \vert } \sum_{X \in C_j} X_j$ã€‚</p><p>è¿™ä¸ªé—®é¢˜è¿˜æœ‰å…¶ä»–å‡ ç§ç­‰ä»·çš„æè¿°ï¼š</p><ul><li><p>æœ€å°åŒ–ç°‡å†…åæ–¹å·®ï¼š$\sum_{j=1}^k \vert C_j\vert Var(C_j)$</p></li><li><p>æœ€å°åŒ–ç›¸åŒçš„ç°‡å†…ç‚¹çš„ä¸¤ä¸¤å¹³æ–¹åå·®ï¼š$\sum_{i=1}^k \frac 1 {2\vert C_i \vert} \sum_{x,xâ€™\in C_i} \Vert x - xâ€™ \Vert$</p></li><li><p>æœ€å¤§åŒ–ç°‡ä¸ç°‡ä¹‹é—´çš„å¹³æ–¹å’Œï¼ˆBCSSï¼‰</p></li></ul><p>åœ¨æ¬§å‡ é‡Œå¾—ç©ºé—´æ‰¾åˆ°æœ€å¥½çš„èšç±»æ•ˆæœï¼ˆå…¨å±€æœ€ä¼˜è§£ï¼‰æ˜¯ä¸€ä¸ªNP-hardé—®é¢˜ã€‚å› æ­¤ç»å¸¸ç”¨å¯å‘å¼ï¼Œè¿­ä»£å¼çš„ç®—æ³•æ¥å¾—åˆ°èšç±»æ•ˆæœï¼Œä¸€èˆ¬å¾—åˆ°çš„æ˜¯å±€éƒ¨æœ€ä¼˜è§£ã€‚</p><h2 id="Llyodâ€™s-Algorithm"><a href="#Llyodâ€™s-Algorithm" class="headerlink" title="Llyodâ€™s Algorithm"></a>Llyodâ€™s Algorithm</h2><p>è™½ç„¶å…¨å±€æœ€ä¼˜è§£æ˜¯ä¸€ä¸ªNP-hardé—®é¢˜ï¼Œä½†æ˜¯å¾—åˆ°å±€éƒ¨æœ€ä¼˜è§£ç¡®å®éå¸¸å®¹æ˜“çš„ã€‚åœ¨è¿™é‡Œä»‹ç»Llyodâ€™s Algorithm, å®é™…ä¸Šå®ƒçš„è¿‡ç¨‹æ˜¯å¼‚å¸¸ç®€å•çš„ã€‚æ•´ä¸ªç®—æ³•åˆ†ä¸ºä¸‹é¢å‡ æ­¥ï¼š</p><ol><li><p>éšæœºåˆå§‹åŒ–kä¸ªä¸­å¿ƒï¼š$u_1,u_2,â€¦,u_k$</p></li><li><p>å¯¹äºæ¯ä¸ªæ ·æœ¬iï¼Œ$C^{(i)} = argmin_j \Vert X_i - \mu_j \Vert^2$</p></li><li><p>æ ¹æ®èšç±»ç»“æœé‡æ–°è®¡ç®—$\mu_j$</p></li></ol><p>é‡å¤ä¸Šè¿°è¿‡ç¨‹ï¼Œç›´åˆ°$\mu$ä¸å†æ”¹å˜ã€‚</p><p>ç°åœ¨ï¼Œä»éç›‘ç£å­¦ä¹ çš„ç›®æ ‡æ¥é‡æ–°ç†è§£è¿™ä¸ªèšç±»ç®—æ³•ï¼Œå®ƒå®é™…ä¸Šæ˜¯å­¦ä¹ åˆ°äº†ä¸€ä¸ªk-dimentionalçš„ç¨€ç–ä»£è¡¨ã€‚ä¹Ÿå°±æ˜¯ï¼š$X_i$è½¬æ¢åˆ°$Z_i$äº†ï¼Œè€Œ</p><p>$z_{i,j} =\left \{ \begin{matrix}<br>1&amp; if C^{i} = j;\\<br>0&amp; otherwise<br>\end{matrix}<br>\right .<br>$</p><p>å®ƒå°†åŸæ¥çš„Xç‰¹å¾å‘é‡è½¬æ¢æˆZç»´å‘é‡ã€‚è€Œè¿™ä¸ªZçŸ©é˜µæ˜¯ç¨€ç–çš„ï¼Œå› ä¸ºæ¯ä¸ªå‘é‡åªæœ‰ä¸€é¡¹å€¼ä¸º0.</p><p>å› ä¸ºè¿™ä¸ªç®—æ³•åªèƒ½å¾—åˆ°ä¸€ä¸ªå±€éƒ¨æœ€ä¼˜è§£ï¼Œå› æ­¤åˆå§‹åŒ–æ˜¯å¾ˆé‡è¦çš„ï¼Œå¯èƒ½ä¼šå½±å“ç»“æœã€‚</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/k_means_2.png" alt="k-means-2"></p><p>è¿™ä¸ªç®—æ³•è¿˜ç•™ä¸‹å‡ ä¸ªç–‘é—®ï¼š</p><ol><li>å¦‚ä½•åˆå§‹åŒ–ï¼Ÿ</li></ol><p>Uniformly random sampling, </p><p>kmeans++ [Arthur &amp; Vassilvitskii SODA 2007]: distance-based sampling</p><ol><li>å¦‚ä½•è‡ªåŠ¨é€‰æ‹©åˆ†æˆå‡ ç±»ï¼ˆkçš„å–å€¼ï¼‰ï¼Ÿ</li></ol><p>Cross validationï¼ˆäº¤å‰éªŒè¯ï¼‰</p><p>G-Means [Hamerly &amp; Elkan, NIPS 2003]</p><p>è¯´åˆ°æœ€åï¼Œè¯´å‡ å¥é¢˜å¤–è¯ã€‚æ•°æ®å­¦ä¹ è¿›è¡Œäº†æœŸä¸­è€ƒè¯•ï¼Œæˆ‘çš„åˆ†æ•°æ˜¯å30%çš„æ°´å¹³ã€‚æˆ‘çš„å¿ƒé‡Œè¿˜æ˜¯æŒºéš¾è¿‡çš„ã€‚è™½ç„¶æˆ‘çš„æœ¬ç§‘ä¹Ÿæ˜¯å¾ˆå·®çš„æ’åï¼Œä½†æ˜¯é‚£æ˜¯å› ä¸ºæˆ‘ä¸å­¦ä¹ ã€‚ä¸è¿‡æ•°æ®å­¦ä¹ è¿™é—¨è¯¾æˆ‘å­¦å¾—è¿˜æ˜¯æŒºè®¤çœŸçš„ã€‚</p><p>æˆ‘å¯ä»¥ç»™è‡ªå·±æ‰¾å¾ˆå¤šå€Ÿå£ï¼šæ²¡æœ‰å¤ä¹ ;æ—¶é—´æ²¡å®‰æ’å¥½(ç¬¬ä¸€é¢˜èŠ±äº†å¤ªé•¿æ—¶é—´)ç­‰ç­‰ï¼Œä¸è¿‡ä¸»è¦åŸå› è¿˜æ˜¯å®åŠ›ä¸å¤Ÿã€‚å°±ç®—è¿™äº›æˆ‘éƒ½åšåˆ°äº†ï¼Œæˆ‘ä¾ç„¶åŠä¸äº†æ ¼ã€‚æ—¢ç„¶æœ‰80åˆ†çš„å¤§ä½¬ï¼Œé‚£åœ¨ä¹è¿™å‡ åˆ†ä¹Ÿæ²¡ä»€ä¹ˆæ„æ€ã€‚è¯´æ˜è‡ªå·±è¿˜æ˜¯å¤ªèœäº†ã€‚</p><p>å¸Œæœ›æœŸæœ«è€ƒè¯•å¯ä»¥å–å¾—ä¸€ä¸ªå¥½æˆç»©ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æ•°æ®å­¦ä¹ è¯¾ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LFD class </tag>
            
            <tag> unsupervised learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ä¿¡æ¯è®ºâ€”â€”Huffmanç¼–ç å®ç°åŠå…¶æœ€ä¼˜æ€§</title>
      <link href="/2018/11/17/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94Huffman%E7%BC%96%E7%A0%81%E5%AE%9E%E7%8E%B0%E5%8F%8A%E5%85%B6%E6%9C%80%E4%BC%98%E6%80%A7/"/>
      <url>/2018/11/17/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94Huffman%E7%BC%96%E7%A0%81%E5%AE%9E%E7%8E%B0%E5%8F%8A%E5%85%B6%E6%9C%80%E4%BC%98%E6%80%A7/</url>
      
        <content type="html"><![CDATA[<p>é¦™å†œè™½ç„¶æå‡ºäº†é¦™å†œç ï¼Œä½†æ˜¯é¦™å†œç å¾ˆå¤šæƒ…å†µä¸‹ç¦»æœ€ä¼˜ç è¿˜å·®å¾—ä¸å°‘ã€‚æ¯”å¦‚ï¼šK=2,$p(a_1) = 0.9999,p(a_2) = 0.0001$ï¼Œè¿™ç§åå·®éå¸¸å¤§çš„æƒ…å†µä¸‹ï¼Œé¦™å†œç ç»™$a_1,a_2$çš„ç¼–ç é•¿åº¦åˆ†åˆ«ä¸ºï¼š1,14.è€Œå®é™…ä¸Šä¸¤ä¸ªå€¼ï¼Œæˆ‘ä»¬å¯ä»¥ä»…ç”¨ä¸€ä¸ªbitå°±èƒ½åŒºåˆ†å¼€æ¥ã€‚åœ¨è¿™é‡Œä»‹ç»ä¸€ä¸ªå¤§åé¼é¼çš„æœ€ä¼˜å‰ç¼€ç¼–ç ï¼šHuffmanç¼–ç ã€‚</p><a id="more"></a><p>éœå¤«æ›¼ç¼–ç å°±ä¸ä»‹ç»äº†ã€‚å› ä¸ºè¿™ä¸ªä¸œè¥¿æˆ‘ä¹Ÿå®ç°è¿‡ï¼ŒçŸ¥é“å®ƒæ˜¯æ€ä¹ˆåšçš„ã€‚ä¸è¿‡å¦‚ä½•æå‡ºæ¥è¿™ä¸ªç¼–ç æ˜¯éå¸¸æœ‰æ„æ€çš„ã€‚</p><p>éœå¤«æ›¼ç æå‡ºä¹‹å‰ï¼Œäººä»¬ä¸€ç›´åœ¨è¿½æ±‚æœ€ä¼˜å‰ç¼€ç¼–ç ã€‚éœå¤«æ›¼åœ¨MITè¯»åšå£«çš„æ—¶å€™ï¼Œè€å¸ˆç»™äº†ä¸€ä¸ªä½œä¸šé¢˜ç›®ï¼šæ‰¾åˆ°æœ€ä¼˜çš„äºŒè¿›åˆ¶ç¼–ç ã€‚éœå¤«æ›¼æƒ³ï¼Œæƒ³è¦è¯æ˜å·²æœ‰ç¼–ç æ˜¯å¦æ˜¯æœ€ä¼˜çš„å¤ªéš¾äº†ï¼Œæ‰€ä»¥ä»–å°±æƒ³ç€è‡ªå·±æ‰¾ä¸ªç¼–ç æ–¹å¼ã€‚ç„¶åï¼Œè¯ç”Ÿäº†éœå¤«æ›¼ç¼–ç ã€‚</p><p>å”‰ï¼Œè¿™å°±æ˜¯å¤§ä½¬å•Šã€‚</p><p>éœ€è¦æ³¨æ„çš„æ˜¯kå‰éœå¤«æ›¼æ ‘æ¯ä¸ªç»“ç‚¹è¦ä¹ˆæœ‰kä¸ªå­©å­ï¼Œè¦ä¹ˆæ²¡æœ‰å­©å­ã€‚ç¡®å®šäº†ç¬¬ä¸€ä¸ªï¼Œæ¯æ¬¡ç¼–ç éœ€è¦å‡å°‘(k-1)ä¸ªå¶å­ç»“ç‚¹ã€‚è¿™æ„å‘³ç€ï¼Œä¿¡æºçš„ç§ç±»ä¸ªæ•°å¿…é¡»æ»¡è¶³ï¼š1+n(k-1)ã€‚å› æ­¤æœ‰æ—¶å€™éœ€è¦å¡«å……0æ¦‚ç‡çš„å­—ç¬¦æ¥ä¿è¯ç¼–ç è¿‡ç¨‹é¡ºåˆ©ã€‚</p><p>æœ€ä¼˜æ€§å¦‚ä½•è¯æ˜ï¼Ÿ</p><p>ç•™ä¸ªå‘å§ã€‚çœ‹ä¹¦ä¸Šè¿™ä¸ªè¯æ˜ä¹ŸæŒºé•¿çš„ã€‚æœ‰æ—¶é—´äº†çœ‹å®Œäº†å†æ¥å†™ï¼ˆä¹Ÿå¯èƒ½ä¸€ç›´æ²¡æœ‰å†™ï¼‰ã€‚</p><p>æœ€åå‘ä¸€ä¸‹å¤šå¹´å‰å®ç°çš„huffmanäºŒè¿›åˆ¶ç¼–ç ï¼š<br>Node.java:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> hfm_compress;</span><br><span class="line"><span class="keyword">import</span> edu.princeton.cs.algs4.*;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.PriorityQueue;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Node  </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"> <span class="class"><span class="keyword">class</span> <span class="title">Code</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    <span class="keyword">boolean</span> used;</span><br><span class="line">    <span class="keyword">short</span> code;</span><br><span class="line">    <span class="keyword">int</span> size;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span> <span class="keyword">implements</span> <span class="title">Comparable</span>&lt;<span class="title">Node</span>&gt;</span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">   <span class="keyword">private</span> Node left;</span><br><span class="line">    <span class="keyword">private</span> Node right;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> frep;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">char</span> symbol;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Node <span class="title">left</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> left;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> Node <span class="title">right</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> right;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">(Node l,Node r,<span class="keyword">int</span> f,<span class="keyword">char</span> s)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        left = l;</span><br><span class="line">        right = r;</span><br><span class="line">        frep = f;</span><br><span class="line">        symbol = s;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(Node n)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.frep - n.frep;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">frep</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> frep;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">char</span> <span class="title">symbol</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> symbol;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasLeft</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> !(left == <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hasRight</span><span class="params">()</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> !(right == <span class="keyword">null</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String []args)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"wocao"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>HuffmanCompress.java<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> hfm_compress;</span><br><span class="line"><span class="keyword">import</span> java.io.*;</span><br><span class="line"><span class="keyword">import</span> edu.princeton.cs.algs4.*;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.PriorityQueue;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HuffmanCompress</span></span></span><br><span class="line"><span class="class"></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Node <span class="title">BuildTree</span><span class="params">(<span class="keyword">int</span> []frep)</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        Node n = <span class="keyword">null</span>;</span><br><span class="line">        Node left = <span class="keyword">null</span>,right = <span class="keyword">null</span>;</span><br><span class="line">        PriorityQueue&lt;Node&gt; pq = <span class="keyword">new</span> PriorityQueue&lt;Node&gt; ();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i!=<span class="number">256</span>;++i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(frep[i]!=<span class="number">0</span>)</span><br><span class="line">            &#123;</span><br><span class="line">            n = <span class="keyword">new</span> Node(<span class="keyword">null</span>,<span class="keyword">null</span>,frep[i],(<span class="keyword">char</span>)i);</span><br><span class="line">            pq.add(n);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(pq.size()&gt;<span class="number">1</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            left = pq.poll();</span><br><span class="line">            right = pq.poll();</span><br><span class="line">            n = <span class="keyword">new</span> Node(left,right,left.frep()+right.frep(),(<span class="keyword">char</span>)<span class="number">0</span>);</span><br><span class="line">            pq.add(n);</span><br><span class="line">        &#125;</span><br><span class="line">        n = pq.poll();</span><br><span class="line">       <span class="keyword">return</span> n;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Code [] BuildTable(Node tree)</span><br><span class="line">    &#123;</span><br><span class="line">        Code []table = <span class="keyword">new</span> Code[<span class="number">256</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i!=<span class="number">256</span>;++i)</span><br><span class="line">            table[i] = <span class="keyword">new</span> Code();</span><br><span class="line">         BuildTable(tree,table,(<span class="keyword">short</span>)<span class="number">0</span>,<span class="number">0</span>);</span><br><span class="line">         <span class="keyword">return</span> table;</span><br><span class="line">    &#125;</span><br><span class="line">   <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">BuildTable</span><span class="params">(Node tree,Code []table,<span class="keyword">short</span> code,<span class="keyword">int</span> size)</span></span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">      <span class="keyword">if</span>(tree.hasLeft())</span><br><span class="line">          BuildTable(tree.left(),table,(<span class="keyword">short</span>)(code&lt;&lt;<span class="number">1</span>),size+<span class="number">1</span>);</span><br><span class="line">      <span class="keyword">if</span>(tree.hasRight())</span><br><span class="line">          BuildTable(tree.right(),table,(<span class="keyword">short</span>)((code&lt;&lt;<span class="number">1</span>)|<span class="number">1</span>),size+<span class="number">1</span>);</span><br><span class="line">      <span class="keyword">if</span>(!tree.hasRight()&amp;&amp;!tree.hasLeft())</span><br><span class="line">      &#123;</span><br><span class="line">          table[tree.symbol()].size = size;</span><br><span class="line">          table[tree.symbol()].code = code;</span><br><span class="line">          table[tree.symbol()].used = <span class="keyword">true</span>;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">compress</span><span class="params">(BinaryIn in,BinaryOut out)</span></span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> frep[] = <span class="keyword">new</span> <span class="keyword">int</span> [<span class="number">256</span>];</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i!=<span class="number">256</span>;++i)</span><br><span class="line">       &#123;</span><br><span class="line">           frep[i] = <span class="number">0</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">int</span> max = <span class="number">255</span>;</span><br><span class="line">       ArrayList&lt;Character&gt; data = <span class="keyword">new</span> ArrayList&lt;Character&gt;();</span><br><span class="line">       <span class="keyword">while</span>(!in.isEmpty())</span><br><span class="line">       &#123;</span><br><span class="line">          <span class="keyword">char</span> sym = in.readChar();</span><br><span class="line">          ++frep[sym];</span><br><span class="line">          <span class="keyword">if</span>(frep[sym]&gt;max)</span><br><span class="line">             max = frep[sym];</span><br><span class="line">          data.add(sym);</span><br><span class="line">              </span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i!=<span class="number">256</span>;++i)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">int</span> a = frep[i]/(max/<span class="number">255</span>);</span><br><span class="line">           <span class="keyword">if</span>(a == <span class="number">0</span>&amp;&amp;frep[i]&gt;<span class="number">0</span>)</span><br><span class="line">               frep[i] = <span class="number">1</span>;</span><br><span class="line">           <span class="keyword">else</span></span><br><span class="line">               frep[i] = a;</span><br><span class="line">       &#125;</span><br><span class="line">       Node tree = BuildTree(frep);</span><br><span class="line">     </span><br><span class="line">       Code []table = BuildTable(tree);</span><br><span class="line">       Iterator&lt;Character&gt; ii = data.iterator();</span><br><span class="line">       out.write(data.size());</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i!=<span class="number">256</span>;++i)</span><br><span class="line">           out.write(frep[i],<span class="number">8</span>);</span><br><span class="line">     </span><br><span class="line">       <span class="keyword">while</span>(ii.hasNext())</span><br><span class="line">       &#123;</span><br><span class="line">          <span class="keyword">char</span> b = ii.next();</span><br><span class="line">          <span class="keyword">if</span>(table[b].used)</span><br><span class="line">          out.write(table[b].code,table[b].size);</span><br><span class="line">       &#125;</span><br><span class="line">       out.close();</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">expand</span><span class="params">(BinaryIn in,BinaryOut out)</span></span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">       <span class="keyword">int</span> size = in.readInt();</span><br><span class="line">     <span class="comment">//  StdOut.println(size);</span></span><br><span class="line">       <span class="keyword">int</span> []frep = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">256</span>];</span><br><span class="line">       <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i!=<span class="number">256</span>;++i)</span><br><span class="line">           frep[i] =(<span class="keyword">int</span>)in.readChar();</span><br><span class="line">       Node tree = BuildTree(frep);</span><br><span class="line">       Node node = tree;</span><br><span class="line">       <span class="keyword">int</span> ipos = <span class="number">0</span>;</span><br><span class="line">       <span class="keyword">while</span>(size&gt;<span class="number">0</span>)</span><br><span class="line">       &#123;</span><br><span class="line">           <span class="keyword">boolean</span> t = in.readBoolean();</span><br><span class="line">          <span class="comment">// if(node == null)</span></span><br><span class="line">            <span class="comment">//   throw new Exception ("The Tree is not right!");</span></span><br><span class="line">             <span class="keyword">if</span>(t)</span><br><span class="line">             node = node.right();</span><br><span class="line">             <span class="keyword">else</span> node = node.left();</span><br><span class="line">             <span class="keyword">if</span>(node == <span class="keyword">null</span>)</span><br><span class="line">                 <span class="keyword">return</span>;</span><br><span class="line">             <span class="keyword">if</span>(!node.hasLeft()&amp;&amp;!node.hasRight())</span><br><span class="line">             &#123;</span><br><span class="line">                 --size;</span><br><span class="line">                 out.write(node.symbol());</span><br><span class="line">                 node = tree;</span><br><span class="line">             &#125;</span><br><span class="line">                 </span><br><span class="line">       &#125;</span><br><span class="line">     out.close();</span><br><span class="line"></span><br><span class="line">   &#125;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String []args)</span> <span class="keyword">throws</span> IOException</span></span><br><span class="line"><span class="function">   </span>&#123;</span><br><span class="line">       BufferedReader reader = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(System.in));</span><br><span class="line">       String s1 = reader.readLine();</span><br><span class="line">       String s2 = reader.readLine();</span><br><span class="line">       String s3 = reader.readLine();</span><br><span class="line">       <span class="keyword">if</span>(s1.equals(<span class="string">"compress"</span>))</span><br><span class="line">       &#123;</span><br><span class="line">       compress(<span class="keyword">new</span> BinaryIn(s2),<span class="keyword">new</span> BinaryOut(s3));</span><br><span class="line">       System.out.println(<span class="string">"the file has been compressed ! "</span>);</span><br><span class="line">       File old = <span class="keyword">new</span> File(s2);</span><br><span class="line">       File now = <span class="keyword">new</span> File(s3);</span><br><span class="line">       System.out.println(<span class="string">"Old : "</span>+old.length()+<span class="string">" new : "</span>+now.length()+<span class="string">"\nthe compression ratio is "</span>+(<span class="keyword">double</span>)now.length()/old.length());</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">else</span></span><br><span class="line">       &#123;</span><br><span class="line">       expand(<span class="keyword">new</span> BinaryIn(s2),<span class="keyword">new</span> BinaryOut(s3));</span><br><span class="line">       System.out.println(<span class="string">"the file has been expanded ! "</span>);</span><br><span class="line">       File old = <span class="keyword">new</span> File(s2);</span><br><span class="line">       File now = <span class="keyword">new</span> File(s3);</span><br><span class="line">       System.out.println(<span class="string">"Old : "</span>+old.length()+<span class="string">" new : "</span>+now.length());</span><br><span class="line">       &#125;</span><br><span class="line">       </span><br><span class="line">   &#125;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>è¿™ä¸¤ä¸ªæ–‡ä»¶æ”¾åœ¨ä¸€ä¸ªåä¸ºhfm_compressçš„packageé‡Œã€‚</p>]]></content>
      
      
      <categories>
          
          <category> ä¿¡æ¯è®º </category>
          
      </categories>
      
      
        <tags>
            
            <tag> code </tag>
            
            <tag> information theory </tag>
            
            <tag> lossless coding </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Learning From Dataâ€”â€”æ‹Ÿåˆsinæ›²çº¿</title>
      <link href="/2018/11/16/Learning-From-Data%E2%80%94%E2%80%94%E6%8B%9F%E5%90%88sin%E6%9B%B2%E7%BA%BF/"/>
      <url>/2018/11/16/Learning-From-Data%E2%80%94%E2%80%94%E6%8B%9F%E5%90%88sin%E6%9B%B2%E7%BA%BF/</url>
      
        <content type="html"><![CDATA[<p>è¿™æ¬¡çš„ä½œä¸šæ˜¯ç”¨ç¥ç»ç½‘ç»œæ¥æ‹Ÿåˆsinæ›²çº¿ã€‚é€šè¿‡å®è·µæ›´èƒ½æ„Ÿå—åˆ°ReLUä»¥åŠsigmoidï¼Œtanhæ¿€æ´»å‡½æ•°çš„åŒºåˆ«ã€‚<br><a id="more"></a></p><p>ä½œä¸šä¸­å·²ç»å°†æ•´ä½“æ¡†æ¶å†™å¥½ï¼Œå¥½è®©æˆ‘ä»¬èƒ½ä¸“æ³¨äºç®—æ³•éƒ¨åˆ†ã€‚æ¯”è¾ƒå¤æ‚çš„éƒ¨åˆ†æ˜¯å‘é‡åŒ–ï¼Œå› ä¸ºè‡ªå·±çš„åšå®¢å®šä¹‰çš„è¿™ä¸ªçŸ©é˜µçš„å½¢å¼å¯èƒ½æ˜¯å¤šæ ·çš„ï¼Œä½†æ˜¯æœ€åçš„ç»“æœè‚¯å®šæ˜¯ä¸€è‡´çš„ï¼Œä»¥åŠéœ€è¦ä¼ å…¥çš„å‚æ•°å¦‚ä½•åˆ†é…ã€‚</p><p>æˆ‘é€‰æ‹©ä¼ å…¥çš„inputä¸º$a^{(l)}$ä»¥åŠ$a^{(l-1)}$ï¼Œä¼ å…¥çš„gran_outä¸º$\sigma^{(l)}$ä¸­é™¤å»ä¹˜$gâ€™(z)$çš„éƒ¨åˆ†ã€‚å› ä¸ºå¯¼æ•°éƒ¨åˆ†éœ€è¦ä¸Šä¸€å±‚çš„æ¿€æ´»å‡½æ•°æ¥å†³å®šã€‚</p><p>åœ¨ä½œä¸šä¸­å®šä¹‰W,açš„æ–¹å¼å’Œæˆ‘å®šä¹‰$\Sigma,a$çš„æ–¹å¼æ­£å¥½ç›¸åï¼Œè¿™æ˜¯éœ€è¦æ³¨æ„çš„åœ°æ–¹ã€‚</p><p>è¿™ä¸ªç¥ç»ç½‘ç»œåŒ…å«4å±‚ï¼šè¾“å…¥å±‚ï¼Œå…¨è¿æ¥å±‚ï¼ˆ1Ã—80ï¼‰ï¼ŒReLUå±‚ï¼ˆ80Ã—80ï¼‰ï¼Œè¾“å‡ºå±‚ï¼ˆ80Ã—1ï¼‰ã€‚</p><p>æœ€åå¾—åˆ°çš„æ‹Ÿåˆç»“æœå’Œlossæ›²çº¿å¦‚ä¸‹ï¼š</p><p>purelin:</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/1.png" alt=""></p><p>loss history:</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/loss.png" alt=""></p><p>tanh:</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/2.png" alt=""></p><p>loss history:</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/loss_t.png" alt=""></p><p>ä»£ç ä¼šåœ¨ä½œä¸šæˆªæ­¢åä¸Šä¼ ã€‚<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>) </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">x = np.linspace(-np.pi,np.pi,<span class="number">140</span>).reshape(<span class="number">140</span>,<span class="number">-1</span>)</span><br><span class="line">y = np.sin(x)</span><br><span class="line">lr = <span class="number">0.02</span>     <span class="comment">#set learning rate</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(np.ones_like(x)+np.exp(-x))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tanh</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_square_loss</span><span class="params">(y_pre,y_true)</span>:</span>         <span class="comment">#define loss </span></span><br><span class="line">    loss = np.power(y_pre - y_true, <span class="number">2</span>).mean()*<span class="number">0.5</span></span><br><span class="line">    loss_grad = (y_pre-y_true)/y_pre.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> loss , loss_grad           <span class="comment"># return loss and loss_grad</span></span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReLU</span><span class="params">()</span>:</span>                     <span class="comment"># ReLu layer</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input)</span>:</span></span><br><span class="line">        unit_num = input.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># check if the ReLU is initialized.</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> hasattr(self, <span class="string">'W'</span>):</span><br><span class="line">            self.W = np.random.randn(unit_num,unit_num)*<span class="number">1e-2</span> </span><br><span class="line">            self.b = np.zeros((<span class="number">1</span>,unit_num))</span><br><span class="line">        temp = input.dot(self.W) + self.b.repeat(input.shape[<span class="number">0</span>]).reshape(self.W.shape[<span class="number">1</span>],input.shape[<span class="number">0</span>]).T</span><br><span class="line">        <span class="keyword">return</span> np.where(temp&gt;<span class="number">0</span>,temp,<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self,input,grad_out)</span>:</span></span><br><span class="line">        a_lm1 = input[<span class="number">0</span>]</span><br><span class="line">        a_l = input[<span class="number">1</span>]</span><br><span class="line">        derivative = np.where(a_l&gt;<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>)</span><br><span class="line">        sample_num = a_lm1.shape[<span class="number">0</span>]</span><br><span class="line">        delt_W =  a_lm1.T.dot(grad_out*derivative)/sample_num</span><br><span class="line">        delt_b = np.ones((<span class="number">1</span>,sample_num)).dot(grad_out*derivative)/sample_num</span><br><span class="line">        to_back = (grad_out*derivative).dot(self.W.T)</span><br><span class="line">        self.W -= lr * delt_W</span><br><span class="line">        self.b -= lr * delt_b</span><br><span class="line">        <span class="keyword">return</span> to_back</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FC</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_dim,output_dim)</span>:</span>    <span class="comment"># initilize weights</span></span><br><span class="line">        self.W = np.random.randn(input_dim,output_dim)*<span class="number">1e-2</span> </span><br><span class="line">        self.b = np.zeros((<span class="number">1</span>,output_dim))</span><br><span class="line">                       </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input)</span>:</span>          </span><br><span class="line"></span><br><span class="line">        <span class="comment">#purelin</span></span><br><span class="line">        <span class="comment">#return input.dot(self.W) + self.b.repeat(input.shape[0]).reshape(self.W.shape[1],input.shape[0]).T</span></span><br><span class="line">        <span class="comment">#tanh</span></span><br><span class="line">        <span class="keyword">return</span> tanh(input.dot(self.W) + self.b.repeat(input.shape[<span class="number">0</span>]).reshape(self.W.shape[<span class="number">1</span>],input.shape[<span class="number">0</span>]).T)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="comment"># backpropagation,update weights in this step</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self,input,grad_out)</span>:</span></span><br><span class="line">        a_lm1 = input[<span class="number">0</span>]</span><br><span class="line">        a_l = input[<span class="number">1</span>]</span><br><span class="line">        sample_num = a_lm1.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="comment">#purelin</span></span><br><span class="line">        <span class="string">'''delt_W =  a_lm1.T.dot(grad_out)/sample_num</span></span><br><span class="line"><span class="string">        delt_b = np.ones((1,sample_num)).dot(grad_out)/sample_num</span></span><br><span class="line"><span class="string">        to_back = grad_out.dot(self.W.T)'''</span></span><br><span class="line">        <span class="comment">#tanh</span></span><br><span class="line">        delt_W =  a_lm1.T.dot(grad_out*(<span class="number">1</span>-np.power(a_l,<span class="number">2</span>)))/sample_num</span><br><span class="line">        delt_b = np.ones((<span class="number">1</span>,sample_num)).dot(grad_out*(<span class="number">1</span>-np.power(a_l,<span class="number">2</span>)))/sample_num</span><br><span class="line">        to_back = (grad_out*(<span class="number">1</span>-np.power(a_l,<span class="number">2</span>))).dot(self.W.T)</span><br><span class="line">        self.W -= lr * delt_W</span><br><span class="line">        self.b -= lr * delt_b</span><br><span class="line">        <span class="keyword">return</span> to_back</span><br><span class="line"></span><br><span class="line"><span class="comment">#  bulid the network      </span></span><br><span class="line">layer1 = FC(<span class="number">1</span>,<span class="number">80</span>)</span><br><span class="line">ac1 = ReLU()</span><br><span class="line">out_layer = FC(<span class="number">80</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># count steps and save loss history</span></span><br><span class="line">loss = <span class="number">1</span></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line">l= []</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> loss &gt;= <span class="number">1e-4</span> <span class="keyword">and</span> step &lt; <span class="number">15000</span>: <span class="comment"># training </span></span><br><span class="line">            </span><br><span class="line">    <span class="comment"># forward     input x , through the network and get y_pre and loss_grad   </span></span><br><span class="line">    <span class="comment"># To get a[l]</span></span><br><span class="line">    a = [x]</span><br><span class="line">    a.append(layer1.forward(a[<span class="number">0</span>]))</span><br><span class="line">    a.append(ac1.forward(a[<span class="number">1</span>]))</span><br><span class="line">    a.append(out_layer.forward(a[<span class="number">2</span>]))</span><br><span class="line">    <span class="comment">#backward   # backpropagation , update weights through loss_grad</span></span><br><span class="line">    <span class="comment">#sigma and a[l-1] is what the backpropagation needs. If you want get the derivative, the a[l] is also needed.  </span></span><br><span class="line">    sigma = out_layer.backward([a[<span class="number">2</span>],a[<span class="number">3</span>]],a[<span class="number">3</span>] - y)</span><br><span class="line">    sigma = ac1.backward([a[<span class="number">1</span>],a[<span class="number">2</span>]],sigma)</span><br><span class="line">    sigma = layer1.backward([a[<span class="number">0</span>],a[<span class="number">1</span>]],sigma)    </span><br><span class="line">    <span class="comment">#This step is for plotting the initial line.</span></span><br><span class="line">    <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">        y_start = a[<span class="number">3</span>]</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line">    loss = mean_square_loss(a[<span class="number">3</span>],y)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    l.append(loss)</span><br><span class="line">    <span class="comment">#print("step:",step,loss)</span></span><br><span class="line">y_pre = a[<span class="number">3</span>]</span><br><span class="line">    </span><br><span class="line"><span class="comment"># after training , plot the results</span></span><br><span class="line"></span><br><span class="line">plt.plot(x,y,c=<span class="string">'r'</span>,label=<span class="string">'true_value'</span>)</span><br><span class="line">plt.plot(x,y_pre,c=<span class="string">'b'</span>,label=<span class="string">'predict_value'</span>)</span><br><span class="line">plt.plot(x,y_start,c=<span class="string">'black'</span>,label=<span class="string">'begin_value'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.savefig(<span class="string">'1.png'</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(np.arange(<span class="number">0</span>,len(l)), l )</span><br><span class="line">plt.title(<span class="string">'loss history'</span>) </span><br><span class="line"><span class="comment"># save the loss history.</span></span><br><span class="line">plt.savefig(<span class="string">'loss_t.png'</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> æ•°æ®å­¦ä¹ è¯¾ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> code </tag>
            
            <tag> LFD class </tag>
            
            <tag> neural network </tag>
            
            <tag> homework </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Learning From Dataâ€”â€”Activation Function</title>
      <link href="/2018/11/14/Learning-From-Data%E2%80%94%E2%80%94Activation-Function/"/>
      <url>/2018/11/14/Learning-From-Data%E2%80%94%E2%80%94Activation-Function/</url>
      
        <content type="html"><![CDATA[<p>ä¸Šæ¬¡lfdçš„åšå®¢è®²äº†ç¥ç»ç½‘ç»œçš„ä¸€äº›åŸºæœ¬å†…å®¹ï¼ŒåŒ…æ‹¬å®ƒçš„èµ·æºï¼Œå‰å‘ä¼ æ’­ä»¥åŠåå‘ä¼ æ’­ã€‚å®é™…ä¸Šï¼Œå¯¹äºä¸€ä¸ªå¾ˆé‡è¦çš„éƒ¨åˆ†ï¼šactivation functionï¼Œåªæ˜¯ç®€å•æåˆ°ã€‚æ‰€ä»¥è¿™æ¬¡ç€é‡è¯´ä¸€ä¸‹ä¸åŒçš„æ¿€æ´»å‡½æ•°ä¹‹é—´çš„åŒºåˆ«ã€‚<br><a id="more"></a></p><p>ä½ åº”è¯¥è¿˜è®°å¾—æœ‰è¿™ä¹ˆä¸€æ®µè¯ï¼š</p><blockquote><blockquote><p>ä¸Šé¢çš„å‡½æ•°ä¸­ï¼Œgä¸ºlogisticå‡½æ•°ï¼Œåˆå«sigmoidå‡½æ•°ã€‚å½“ç„¶è¿™ä¸ªå‡½æ•°ä¸ä»…ä»…å±€é™äºsigmoidå‡½æ•°ï¼Œä¹Ÿæœ‰reluå‡½æ•°ï¼Œtanhå‡½æ•°ï¼š</p><script type="math/tex; mode=display">\begin{matrix}g(z) = \frac 1 {1+e^{-z}} &(sigmoid)\\g(z) = \max(z,0) &(ReLU)\\g(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}& (tanh)\end{matrix}</script></blockquote></blockquote><p>å®é™…ä¸Šï¼Œå®é™…ä¸Šä½¿ç”¨çš„ä¹Ÿå¤šæ˜¯è¿™ä¸‰ä¸ªå‡½æ•°ï¼Œæˆ–è€…å®ƒä»¬å…¶ä¸­æŸä¸ªçš„å˜ç§ã€‚</p><p>åŒæ ·ï¼Œä¹‹å‰åšå®¢ä¹Ÿè¯´æ˜äº†ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸€å®šè¦åœ¨ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨éçº¿æ€§å‡½æ•°ã€‚æ‰€ä»¥åœ¨è¿™é‡Œå°±ä¸å¤šæäº†ã€‚è¿™ç¯‡åšå®¢ï¼Œä¸»è¦å°±ä»‹ç»è¿™3ä¸ªå‡½æ•°çš„åŒºåˆ«ä»¥åŠä»–ä»¬çš„ä½¿ç”¨åœºæ™¯ã€‚</p><h2 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h2><p>Logistic Regressionæ˜¯ä¸€ä¸ªéå¸¸åŸºæœ¬çš„ç®—æ³•ã€‚åœ¨äºŒå…ƒåˆ†ç±»æ—¶å€™ï¼Œå®ƒç”¨çš„éå¸¸å¤šã€‚ä¸è¿‡å¾ˆé—æ†¾çš„æ˜¯ï¼Œåœ¨neural networkä¸­ï¼Œæˆ‘ä»¬é™¤äº†è¾“å‡ºå±‚å‡ ä¹ä¸ä½¿ç”¨è¿™ä¸ªå‡½æ•°ã€‚</p><p>Logistic Function(Sigmoid Function)çš„å›¾åƒå¦‚ä¸‹ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/sigmoid.png" alt=""></p><p>å®ƒçš„ç¼ºç‚¹å¾ˆæ˜æ˜¾ï¼š</p><ol><li>ä¸æ˜¯ä»¥0ä¸ºä¸­å¿ƒçš„</li><li>å½“|x|æ¯”è¾ƒå¤§çš„æ—¶å€™ï¼Œè¿™ä¸ªå‡½æ•°çš„æ¢¯åº¦éå¸¸å°ï¼Œç§°ä¸ºé¥±å’ŒåŒºæ¢¯åº¦æ‰¼æ€ã€‚</li><li>æŒ‡æ•°è¿ç®—è¾ƒä¸ºå¤æ‚</li></ol><p>å› ä¸º1çš„å­˜åœ¨ï¼Œä½¿å¾—ä¸‹ä¸€å±‚çš„è¾“å…¥éƒ½æ˜¯æ­£çš„ï¼Œé‚£ä¹ˆä¸‹ä¸€å±‚çš„æ¢¯åº¦å°±ä¼šå—é™ã€‚æ­¤å¤–ï¼Œé¥±å’ŒåŒºæ¢¯åº¦å¤ªå°ï¼Œå†åŠ ä¸ŠæŒ‡æ•°è¿ç®—æ¯”è¾ƒå¤æ‚ï¼Œè¿™äº›ä½¿å¾—sigmoidçš„æ¢¯åº¦ä¸‹é™éå¸¸ç¼“æ…¢ã€‚</p><p>ä½†æ˜¯sigmoidå‡½æ•°ä¹Ÿæœ‰éå¸¸å¤§çš„ä¼˜åŠ¿ï¼Œå®ƒä¸€èˆ¬ä½œä¸ºè¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•°ï¼Œå› ä¸ºå®ƒå°†å‡½æ•°è¾“å‡ºæ§åˆ¶åœ¨0,1ä¹‹é—´ã€‚å®é™…è¿ç”¨ä¸­ï¼Œé™¤äº†è¾“å‡ºå±‚ï¼Œå‡ ä¹ä¸ç”¨sigmoidå‡½æ•°ã€‚</p><h2 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h2><p>tanhä¸ºåŒæ›²æ­£åˆ‡å‡½æ•°ã€‚å®ƒçš„å›¾åƒå¦‚ä¸‹ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/tanh.png" alt=""></p><p>å¯ä»¥çœ‹åˆ°å®ƒå’Œsigmoidå‡½æ•°éå¸¸ç›¸ä¼¼ï¼Œä¸è¿‡å®ƒçš„ä¼˜ç‚¹æ˜¯ä»¥0ä¸ºä¸­å¿ƒã€‚ä¸è¿‡å‘¢å®ƒçš„ç¼ºç‚¹ä¹Ÿæ¯”è¾ƒæ˜æ˜¾ï¼Œé™¤äº†ä»¥0ä¸ºä¸­å¿ƒï¼Œsigmoidæœ‰çš„ç¼ºç‚¹å®ƒéƒ½æœ‰ã€‚ä¸è¿‡ä¹Ÿå› ä¸ºè¿™ç‚¹ï¼Œä¸€èˆ¬æ¥è¯´å®ƒè¡¨ç°çš„æ€»æ˜¯ä¼šæ¯”sigmoidå‡½æ•°æ›´å¥½ã€‚æ‰€ä»¥é™¤äº†è¾“å‡ºå±‚ï¼Œæƒ³è¦ä½¿ç”¨sigmoidçš„åœ°æ–¹ä¸å¦‚æ¢æˆtanhå‡½æ•°ã€‚</p><h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p>ReLUï¼ˆRectified Linear Unitï¼‰ä¸ºçº¿æ€§æ•´æµå‡½æ•°ï¼Œåˆç§°ä¸ºä¿®æ­£çº¿æ€§å•å…ƒã€‚å®é™…ä¸Šå®ƒæ˜¯ç›®å‰æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ã€‚<br>å®ƒçš„æ­£åŠè½´æ²¡æœ‰é¥±å’Œæ‰¼æ€æ¢¯åº¦çš„å½±å“ï¼Œè€Œä¸”è¿ç®—ä¹Ÿéå¸¸ç®€å•ï¼Œä½¿å¾—å®ƒåœ¨ç¥ç»ç½‘ç»œä¸­çš„æ”¶æ•›é€Ÿåº¦æ¯”å…¶ä»–çš„æ¿€æ´»å‡½æ•°è¦å¿«å¾ˆå¤šã€‚å®ƒçš„å›¾åƒå¦‚ä¸‹ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/3.png" alt=""></p><p>ä¸è¿‡å®ƒä¹Ÿæœ‰ç¼ºç‚¹ï¼šè¾“å‡ºä¸æ˜¯ä»¥0ä¸ºä¸­å¿ƒã€‚è€Œä¸”å½“xå°äº0æ—¶å€™æ¢¯åº¦å°†ä¼šè¢«æ‰¼æ€ã€‚</p><p>é’ˆå¯¹ReLUçš„ä¸è¶³æœ‰å¾ˆå¤šReLUçš„æ”¹è‰¯ç‰ˆï¼Œå¦‚Leaky ReLU:g(x) = max(0.01z,z)ç­‰ã€‚è¿™äº›åœ¨å®é™…ä¸­æ¯”ReLUè¡¨ç°æ›´å¥½ï¼Œä½†æ˜¯ä½¿ç”¨ReLUä¾ç„¶æ˜¯æœ€å¤šçš„é€‰æ‹©ï¼Œå®é™…ä¸ŠReLUæ˜¯ç›®å‰çš„ç¥ç»ç½‘ç»œçš„é»˜è®¤æ¿€æ´»å‡½æ•°ã€‚</p><h2 id="derivative-of-activation-function"><a href="#derivative-of-activation-function" class="headerlink" title="derivative of activation function"></a>derivative of activation function</h2><div class="table-container"><table><thead><tr><th>function</th><th>derivative</th></tr></thead><tbody><tr><td>sigmoid</td><td>$gâ€™(x) = g(x)(1 - g(x))$</td></tr><tr><td>tanh</td><td>$gâ€™(x) = 1-g^2(x)$</td></tr><tr><td>ReLU</td><td>$gâ€™(x) =\left \{ \begin{matrix}0&amp;x<0\\1&x>0\end{matrix}\right .$</0\\1&x></td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> æ•°æ®å­¦ä¹ è¯¾ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LFD class </tag>
            
            <tag> deep learning </tag>
            
            <tag> neural network </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ä¿¡æ¯è®ºâ€”â€”Kraftä¸ç­‰å¼ä»¥åŠå˜é•¿ç¼–ç å®šç†</title>
      <link href="/2018/11/14/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94Kraft%E4%B8%8D%E7%AD%89%E5%BC%8F%E4%BB%A5%E5%8F%8A%E5%8F%98%E9%95%BF%E7%BC%96%E7%A0%81%E5%AE%9A%E7%90%86/"/>
      <url>/2018/11/14/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94Kraft%E4%B8%8D%E7%AD%89%E5%BC%8F%E4%BB%A5%E5%8F%8A%E5%8F%98%E9%95%BF%E7%BC%96%E7%A0%81%E5%AE%9A%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>ä¸Šæ¬¡ä»‹ç»äº†é¦™å†œæ— æŸç¼–ç å®šç†ä»¥åŠä¸€äº›ä¸åŒç±»åˆ«çš„ç¼–ç ã€‚è¿™æ¬¡ä»‹ç»kraftä¸ç­‰å¼ä»¥åŠhuffmanç¼–ç ï¼Œå¹¶ä¸”è¯´æ˜éœå¤«æ›¼ç¼–ç çš„æœ€ä¼˜æ€§ã€‚<br><a id="more"></a></p><p>Kraftä¸ç­‰å¼ä¸ºå‰ç¼€ç çº¦æŸæ¡ä»¶ã€‚åœ¨å‰ç¼€ç ä¸­ï¼Œæ˜¾ç„¶ä¸èƒ½ä½¿ç”¨æ‰€æœ‰çš„æœ€çŸ­çš„ç å­—ï¼Œè¿™æ ·çš„è¯å‰ç¼€ç çš„æ¡ä»¶å°±æ— æ³•æ»¡è¶³ã€‚å°±ç”¨äºŒå‰æ ‘æ¥ç¼–ç ï¼ˆäºŒè¿›åˆ¶ç¼–ç ï¼‰çš„æ—¶å€™ï¼Œå¦‚æœä¸€ä¸ªèŠ‚ç‚¹è¢«ä½œä¸ºç å­—ï¼Œåˆ™å®ƒçš„å­æ ‘ç»“ç‚¹éƒ½æ— æ³•ä½œä¸ºç å­—ã€‚</p><h2 id="Kraftä¸ç­‰å¼"><a href="#Kraftä¸ç­‰å¼" class="headerlink" title="Kraftä¸ç­‰å¼"></a>Kraftä¸ç­‰å¼</h2><p>kraftä¸ç­‰å¼å®šä¹‰å¦‚ä¸‹ï¼š</p><p>ä»»æ„D-è¿›åˆ¶ç å‰ç¼€ç å…¶ç é•¿$l_1,l_2,â€¦,l_m$ï¼Œæ»¡è¶³</p><script type="math/tex; mode=display">\sum_{i} D^{-l_i} \leq 1</script><p>åä¹‹ï¼Œå¦‚æœç é•¿çº¦æŸæ»¡è¶³ä¸Šè¿°ä¸ç­‰å¼ï¼Œåˆ™å¿…ç„¶å¯ä»¥æ„é€ å‡ºå…·æœ‰æ­¤ç é•¿çš„å‰ç¼€ç ã€‚ </p><p>Kraftä¸ç­‰å¼çš„è¯æ˜æ˜¯éå¸¸ç›´æ¥çš„ï¼š</p><ul><li>å¿…è¦æ€§ï¼š</li></ul><p>æˆ‘ä»¬ä¾ç„¶ä»Då‰æ ‘æ¥è€ƒè™‘è¿™ä¸ªé—®é¢˜ã€‚å‡å¦‚ç å­—æœ€é•¿ä¸º$l_{max}$.å½“ä¸€ä¸ªç»“ç‚¹è¢«é€‰åšç å­—çš„æ—¶å€™ï¼Œå‡è®¾è¿™ä¸ªç»“ç‚¹çš„æ·±åº¦ä¸º$l_i$ï¼Œä¹Ÿå°±æ˜¯ç é•¿ä¸º$l_i$.é‚£ä¹ˆå› ä¸ºå®ƒçš„å­˜åœ¨ï¼Œå®ƒå­æ ‘çš„ç»“ç‚¹éƒ½ä¸èƒ½å†æ¬¡ä½œä¸ºç å­—ã€‚å› æ­¤æˆ‘ä»¬å°±æŸå¤±äº†D^{l_{max} - l_i}ä¸ªå¶å­ç»“ç‚¹ï¼ˆæ³¨æ„ï¼Œè¿™é‡Œè¯´çš„æ˜¯å¶å­ç»“ç‚¹ï¼‰ã€‚</p><p>è¿™æ£µæ ‘çš„æ‰€æœ‰å¶å­ç»“ç‚¹æ•°ç›®ä¸ºï¼š$D^{l_max}$.æˆ‘ä»¬æœ€å¤šæŠŠæ‰€æœ‰çš„å¶å­ç»“ç‚¹éƒ½ç»™å‰ªæ‰ã€‚</p><p>ç°åœ¨å‡è®¾ä¸€å…±æœ‰mä¸ªç å­—ï¼Œåˆ™ï¼š</p><script type="math/tex; mode=display">\sum_{i=1}^m D^{l_{max} - l_i} \leq D^{l_{max}}</script><p>ä¸¤ä¾§åŒæ—¶é™¤ä»¥$D^{l_{max}}$,å°±å¾—åˆ°äº†kraftä¸ç­‰å¼ã€‚</p><ul><li>å……åˆ†æ€§</li></ul><p>å……åˆ†æ€§æ›´å¥½è¯æ˜ã€‚æˆ‘ä»¬åªè¦åœ¨æ ‘ä¸Šå°±å¯ä»¥å¾ˆå®¹æ˜“æ„é€ å‡ºæ¥è¿™æ ·çš„ç¼–ç ã€‚</p><p>Kraftä¸ç­‰å¼ç»™å‡ºäº†å³æ—¶ç çš„å……è¦æ¡ä»¶ï¼Œä½†æ˜¯å’Œæœ€ä¼˜ç é•¿æ— å…³ã€‚</p><h2 id="ä»»æ„å‰ç¼€ç ç é•¿çº¦æŸ"><a href="#ä»»æ„å‰ç¼€ç ç é•¿çº¦æŸ" class="headerlink" title="ä»»æ„å‰ç¼€ç ç é•¿çº¦æŸ"></a>ä»»æ„å‰ç¼€ç ç é•¿çº¦æŸ</h2><p>å¯¹éšæœºå˜é‡Xè¿›è¡ŒDè¿›åˆ¶å‰ç¼€ç ç¼–ç ï¼Œå¾—åˆ°çš„ç é•¿æ»¡è¶³ï¼š</p><script type="math/tex; mode=display">L \ge H_D(X)</script><p>ç­‰å·å½“ä¸”ä»…å½“$D^{-l_i} = p_i$æ—¶å€™æˆç«‹ã€‚è¿™ä¸ªLä¸ºå¹³å‡ç é•¿ã€‚</p><p>è¯æ˜å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\begin{aligned}L - H_D(X) &= \sum P_i l_i - \sum P_i \log_D \frac 1 {P_i}\\&= \sum P_i \log_D D^{l_i} +\sum P_i \log_D P_i\\&= \sum P_i \log_D P_i - \sum P_i \log _D D^{-l_i}\\&= \sum P_i \log_D \frac{P_i}{D^{-l_i}} ----(1)\\\end{aligned}</script><p>ä¹Ÿè®¸å¤§å®¶ä¼šè§‰å¾—ä»ï¼ˆ1ï¼‰å¯ä»¥ç›´æ¥å¾—åˆ°:D(P\Vert D^{-l})ï¼Œç„¶åç”±äº’ä¿¡æ¯å¤§äº0ä»è€Œå®Œæˆè¯æ˜ã€‚ä½†è¿™æ ·æ˜¯ä¸ä¸¥è°¨çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ— æ³•ä¿è¯$\sum D^{-l_i} = 1$.</p><p>å› æ­¤è¿™é‡Œéœ€è¦åšä¸€ä¸ªå½’ä¸€åŒ–å¤„ç†ï¼š<br>ä»¤ï¼š$r_i = \frac{D^{-l_i}}{\Delta}ï¼Œ\Delta = \sum D^{-l_i}$<br>(è¿™ä¸ªåœ°æ–¹çœŸæ˜¯ä¸å®¹æ˜“æƒ³åˆ°å•Šï¼Œå¾ˆå®¹æ˜“å°±ä¸ä¸¥è°¨äº†)</p><p>åˆ™ï¼ˆ1ï¼‰å¯ä»¥å†™ä¸ºï¼š</p><script type="math/tex; mode=display">\begin{aligned}\sum P_i \log_D \frac{P_i}{D^{-l_i}} \\&=\sum P_i \log _D \frac{P_i}{r_i}\frac 1 {\Delta}\\&=\sum P_i \log_D \frac{P_i}{r_i} - \log_D \Delta \sum P_i\\&=D(P\Vert r) + \log \frac 1 \Delta ç é•¿ç é•¿ç é•¿è€Œ$D(P\Vert r) + \log \frac 1 \Delta \ge 0$ï¼ˆç”±å¯¹æ•°æ€§è´¨ï¼Œé‰´åˆ«ä¿¡æ¯æ€§è´¨ä»¥åŠKraftä¸ç­‰å¼å†³å®šï¼‰.æ‰€ä»¥ï¼Œæˆ‘ä»¬å®Œæˆäº†å¯¹ä¸Šè¿°å®šç†çš„è¯æ˜ã€‚## æœ€ä¼˜å‰ç¼€ç å®šç†ï¼ˆé¦™å†œç¬¬ä¸€å®šç†ï¼‰ ##è¯¥å®šç†æè¿°å¦‚ä¸‹ï¼šå¯¹éšæœºå˜é‡Xè¿›è¡ŒDè¿›åˆ¶å‰ç¼€ç¼–ç ï¼Œå¾—åˆ°çš„æœ€ä¼˜ç é•¿æ»¡è¶³ä¸‹åˆ—ä¸ç­‰å¼ï¼š</script><p>H_D(X) \leq L^* &lt; H_D(X)+1</p><script type="math/tex; mode=display">å·¦ä¾§æ˜¯ä¸ç”¨è¯æ˜çš„ï¼Œå› ä¸ºä¸Šä¸ªå®šç†å·²ç»è¯æ˜äº†ã€‚æˆ‘ä»¬ä¸»è¦éœ€è¦è¯æ˜çš„æ˜¯å³ä¾§ã€‚é¦™å†œé€šè¿‡æ„é€ é¦™å†œç æ¥è¯æ˜å³ä¾§ï¼šå–$\lceil \log_D \frac 1{P_i} \rceil$ä¸ºç å­—é•¿åº¦ï¼Œè¿™æ ·çš„ç¼–ç æ˜¯æ»¡è¶³kraftä¸ç­‰å¼çš„ï¼š</script><p>\sum D^{-\rceil \log_D \frac 1 {P_i}} \leq \sum D^{-\log_D \frac 1 {P_i}} = \sum P_i = 1</p><script type="math/tex; mode=display">å› æ­¤æˆ‘ä»¬å¯ä»¥æ ¹æ®è¿™ä¸ªç é•¿æ¥æ„é€ å‡ºç›¸åº”çš„å‰ç¼€ç ã€‚æˆ‘ä»¬çŸ¥é“ï¼š</script><p>\log _D \frac 1 {P_i} \leq l_i \leq \log_D \frac 1 {P_i}+1</p><script type="math/tex; mode=display">å¦‚æœå¯¹ä¸Šè¿°ä¸ç­‰å¼çš„æ‰€æœ‰sideæ±‚æœŸæœ›ï¼Œå¾—åˆ°ï¼š</script><p>H_D(X) \leq \sum p_il_i \leq H_D(X) + 1</p><script type="math/tex; mode=display">è¿™å°±æ˜¯é¦™å†œç ã€‚å®ƒä¸ä¸€å®šæ˜¯æœ€ä¼˜ï¼Œå› æ­¤æˆ‘ä»¬å°±å®Œæˆäº†é¦™å†œç¬¬ä¸€å®šç†çš„è¯æ˜ã€‚## åˆ†ç»„å‰ç¼€ç  ##å®šé•¿ç¼–ç å®šç†å‘Šè¯‰æˆ‘ä»¬ï¼Œ$\epsilon$å¯ä»¥ä»»æ„å°ï¼Œè€Œå˜é•¿ç¼–ç å‘Šè¯‰æˆ‘ä»¬ï¼Œæˆ‘ä»¬ä»˜å‡ºçš„ä»£ä»·å°äº1ã€‚èƒ½ä¸èƒ½è®©è¿™ä¸ªä»£ä»·èƒ½ä¿è¯æ¯”1æ›´å°å‘¢ï¼Ÿè¿™å°±æ˜¯åˆ†ç»„ç¼–ç ã€‚å¯¹äºä¿¡æºXè¿›è¡Œåˆ†ç»„å‰ç¼€ç¼–ç ï¼Œå¾—åˆ°çš„æ¯æ¶ˆæ¯ç¬¦å·æ•°$L_n^*$æ»¡è¶³ä¸ç­‰å¼ï¼š</script><p>\frac{H(X_1,X_2,â€¦,X_n)}{n} \leq L_n^* \leq \frac{H(X_1,X_2,â€¦,X_n)}{n}  + \frac 1 n</p><script type="math/tex; mode=display">å¦‚æœä¿¡æºç¨³æ’ï¼Œåˆ™$L_n^* \rightarrow H(X)$.è¿™è¦æ±‚æˆ‘ä»¬å¯¹Xè¿›è¡Œåˆ†ç»„ï¼Œå› æ­¤ä¼šæœ‰è§£ç å»¶è¿Ÿï¼Œä¹Ÿéœ€è¦ç¼“å†²åŒºã€‚ä½†æ˜¯é€šè¿‡è¿™ä¸ªï¼Œå¯ä»¥ä½¿å¾—å¹³å‡ç é•¿ä»£ä»·æ›´å°ã€‚å¤©ä¸‹æ²¡æœ‰å…è´¹çš„åˆé¤ã€‚## ç¼–ç æ•ˆç‡ä¸äº’ä¿¡æ¯ ##æœ€ä¼˜å‰ç¼€ç çš„ç¼–ç ä¸ä¿¡æºçš„åˆ†å¸ƒå¯†åˆ‡ç›¸å…³ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸ä¸€å®šèƒ½å‡†ç¡®çŸ¥é“ä¿¡æºçš„åˆ†å¸ƒã€‚å¦‚æœä¿¡æºåˆ†å¸ƒä¼°è®¡å‡ºç°åå·®ï¼Œåˆ™å¹³å‡ç é•¿å°±ä¼šå—åˆ°æƒ©ç½šã€‚Penaltyåˆ†æï¼šå¯ä»¥è¯æ˜ï¼Œå¯¹äºæœä»p(x)ä¿¡æºXè¿›è¡Œå‰ç¼€ç¼–ç ï¼Œå¦‚æœç å­—é•¿åº¦å–$l(x) = \lceil \log \frac 1 {q(x)}\rceil$ï¼Œåˆ™å¹³å‡ç é•¿æ»¡è¶³ï¼š</script><p>H(p) + D(p\Vert q) \leq E_{p}l(X) &lt; H(p) + D(p\Vert q) +1</p><p>$$</p><p>å—åˆ°çš„æƒ©ç½šä¸­ï¼Œäº’ä¿¡æ¯å‡ºæ¥äº†ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> ä¿¡æ¯è®º </category>
          
      </categories>
      
      
        <tags>
            
            <tag> information theory </tag>
            
            <tag> lossless encoding </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SLAMâ€”â€”éçº¿æ€§ä¼˜åŒ–</title>
      <link href="/2018/11/14/SLAM%E2%80%94%E2%80%94%E9%9D%9E%E7%BA%BF%E6%80%A7%E4%BC%98%E5%8C%96/"/>
      <url>/2018/11/14/SLAM%E2%80%94%E2%80%94%E9%9D%9E%E7%BA%BF%E6%80%A7%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p>å®é™…ä¸Šä»ä¸Šæ¬¡ä»‹ç»çš„ä¸œè¥¿ï¼Œæˆ‘ä»¬ç†è®ºä¸Šå·²ç»çŸ¥é“äº†SLAMæ˜¯æ€ä¹ˆè¿ä½œçš„äº†ã€‚ä»æ·±åº¦å›¾å’Œé¢œè‰²å›¾ï¼Œä¼°è®¡ç›¸æœºä½å§¿ï¼Œé€šè¿‡ç›¸æœºä½å§¿ï¼Œä»¥åŠæ·±åº¦å›¾å’Œé¢œè‰²å›¾ï¼Œæˆ‘ä»¬å®é™…ä¸Šå°±å¯ä»¥å»æ‹¼æ¥ç‚¹äº‘æˆ–è€…ä¸‰ç»´å»ºæ¨¡äº†ã€‚ä¸è¿‡ç°å®å¾€å¾€æ²¡æœ‰é‚£ä¹ˆå®¹æ˜“ï¼Œå¦‚æœè¿™ä¹ˆç®€å•SLAMä¹Ÿæ²¡ä»€ä¹ˆå¥½ç ”ç©¶çš„äº†ã€‚<br><a id="more"></a><br>å®é™…ä¸Šï¼Œç”Ÿæ´»ä¸­å¤„å¤„å……æ»¡äº†å™ªå£°ã€‚æˆ‘ä»¬é‡‡é›†çš„æ•°æ®ä¹Ÿæ˜¯ä¸€æ ·ã€‚æˆ‘ä»¬æ— æ³•æ¶ˆé™¤å™ªå£°ã€‚æ‰€ä»¥æˆ‘ä»¬å¾—åˆ°çš„è¿åŠ¨æ–¹ç¨‹è¿˜æœ‰è§‚æµ‹æ–¹ç¨‹ï¼Œéƒ½ä¸ä¸€å®šï¼ˆå®é™…ä¸Šæ˜¯ä¸€å®šä¸ï¼‰æ˜¯ä¸¥æ ¼æˆç«‹çš„ï¼Œåªèƒ½è¿‘ä¼¼æˆç«‹ã€‚ä¸ºäº†ä½¿å¾—çŠ¶æ€ä¼°è®¡åœ¨å™ªå£°ä¸­æœ‰ä¸é”™çš„æ•ˆæœï¼Œæˆ‘ä»¬å¿…é¡»å¾—è¿›è¡Œä¼˜åŒ–ã€‚ç°å®ä¸­çš„ä¼˜åŒ–é—®é¢˜å¾€å¾€æ˜¯éçº¿æ€§é—®é¢˜ï¼Œæ‰€ä»¥è¿™æ¬¡ä¸»è¦è®²çš„å†…å®¹æ˜¯éçº¿æ€§ä¼˜åŒ–ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mathematics </tag>
            
            <tag> SLAM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>å›¾å½¢å­¦â€”â€”Viewing</title>
      <link href="/2018/11/14/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E2%80%94%E2%80%94Viewing/"/>
      <url>/2018/11/14/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E2%80%94%E2%80%94Viewing/</url>
      
        <content type="html"><![CDATA[<p>ä¸Šæ¬¡å›¾å½¢å­¦çš„åšå®¢ä¸­ä»‹ç»äº†è½¬æ¢ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä»ä¸–ç•Œåæ ‡è½¬æ¢åˆ°ç›¸æœºåæ ‡äº†ã€‚ä¸è¿‡è™½ç„¶æˆ‘ä»¬å­¦çš„æ˜¯ä¸‰ç»´æ¨¡å‹ï¼Œä¸è¿‡æˆ‘ä»¬çœ‹åˆ°çš„éƒ½æ˜¯äºŒç»´çš„ã€‚è¿‘å¤§è¿œå°æ˜¯å°å­¦ç”Ÿéƒ½æ˜ç™½çš„é“ç†ï¼Œè€Œä¸€ä¸ªç‰©å“çš„è·ç¦»ç­‰ç­‰éƒ½ä¼šå½±å“å®ƒåœ¨æˆ‘ä»¬çœ¼ä¸­,ä»¥åŠæ‹æ‘„å‡ºæ¥ç…§ç‰‡çš„æ ·å­ã€‚å› æ­¤è¿™æ¬¡è®²å¾—å†…å®¹æ˜¯è§‚å¯Ÿï¼ˆViewingï¼‰ã€‚<br><a id="more"></a></p><h2 id="æ­£äº¤æŠ•å½±ï¼ˆOrthographic-Projectionï¼‰"><a href="#æ­£äº¤æŠ•å½±ï¼ˆOrthographic-Projectionï¼‰" class="headerlink" title="æ­£äº¤æŠ•å½±ï¼ˆOrthographic Projectionï¼‰"></a>æ­£äº¤æŠ•å½±ï¼ˆOrthographic Projectionï¼‰</h2><p>æ­£äº¤æŠ•å½±æ˜¯æœ€ç®€å•çš„ä¸€ä¸ªæŠ•å½±æ–¹å¼ã€‚å®ƒå®é™…ä¸Šå°±æ˜¯ä¸‰ç»´åæ ‡ä¸­çš„ç‚¹ä¸¢å¼ƒä¸€ä¸ªåæ ‡è½´ï¼Œå¦‚æˆ‘ä»¬éœ€è¦å°†ç‰©ä½“æŠ•å½±åˆ°xyå¹³é¢ä¸Šï¼Œæˆ‘ä»¬å°±éœ€è¦ä¸¢å¼ƒæ‰zè½´ã€‚</p><p>å®ƒçš„ç‰¹ç‚¹ï¼šåŸæ¥å¹³è¡Œçš„çº¿ä¿æŒå¹³è¡Œã€‚è¿™ä¸ªç‰¹ç‚¹ä½¿å¾—å®ƒåœ¨å¾ˆå¤šå·¥ç¨‹åˆ¶å›¾ä¸­éå¸¸æœ‰ç”¨ã€‚</p><p>è¿™ä¸ªåšå®¢ä¼šä»‹ç»OpenGLä¸­çš„æ­£äº¤æŠ•å½±ï¼ˆgluOrthoï¼‰å®ç°ã€‚</p><p>åœ¨OpenGLä¸­ï¼ŒgluOrthoåšçš„å®é™…ä¸Šæ˜¯å°†ç‰©ä½“è½¬æ¢åˆ°ä¸€ä¸ªä¸­å¿ƒä½äºåæ ‡è½´ä¸­å¿ƒçš„æ­£æ–¹ä½“ä¸Šã€‚ç‰©ä½“åŸæ¥æ˜¯ä¸ªé•¿æ–¹ä½“ï¼Œæ‰€ä»¥gluOrthoéœ€è¦æä¾›çš„æ˜¯left,right;up,bottom;near,far.</p><p>ä¸ºä»€ä¹ˆè¦è¿™ä¹ˆåšï¼Ÿè¿™ä¸ªæ˜¯ä¸‰ç»´pipelineçš„ä¸€æ­¥ï¼Œå…ˆæ˜ å°„åˆ°æ­£æ–¹ä½“ä¸Šï¼Œæœ€åæ–¹ä¾¿æŠ•å½±åˆ°çœŸæ­£çš„å±å¹•ä¸Šï¼Œä¹Ÿå°±æ˜¯æ˜ å°„åˆ°å¹³é¢åƒç´ ä¸Šã€‚</p><p>è€Œæ˜ å°„åˆ°ä¸­å¿ƒæ­£æ–¹ä½“çš„è¾¹é•¿æ˜¯2,å·¦å³ï¼ˆä¸Šä¸‹å‰åï¼‰åæ ‡åˆ†åˆ«ä¸º-1,1. å› æ­¤å¦‚ä½•æ˜ å°„ï¼Ÿ</p><p>å‡å¦‚æä¾›çš„leftï¼Œright;upï¼Œbottom;near,faråˆ†åˆ«å€¼ä¸ºl,r;u,b;n,f;æ—¢ç„¶è¦æ˜ å°„åˆ°æ­£æ–¹ä½“ä¸Šï¼Œé‚£ä¹ˆéœ€è¦ä¸¤éƒ¨ï¼šä¸€ä¸ªå¹³ç§»ï¼Œä¸€ä¸ªç¼©æ”¾ã€‚</p><p>é¦–å…ˆæ˜¯å¹³ç§»ï¼Œå¹³ç§»å‘é‡å¾ˆå®¹æ˜“ï¼š</p><script type="math/tex; mode=display">t = \begin{bmatrix}-\frac{l+r}{2}\\-\frac{u+b}{2}\\-\frac{n+f}{2}\end{bmatrix}</script><p>å†ä¸€ä¸ªæ˜¯ç¼©æ”¾ã€‚æ—¢ç„¶è¦ç¼©æ”¾ï¼Œæ¯”å¦‚å·¦å³è·ç¦»çš„ç¼©æ”¾ï¼Œæ˜¯ä»$r - l$ç¼©æ”¾åˆ°2.å› æ­¤ç¼©æ”¾æ¯”ä¾‹ä¸ºï¼š$\frac{2}{r-l}$.</p><p>åŒæ ·çš„é“ç†ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ç¼©æ”¾çŸ©é˜µï¼š</p><script type="math/tex; mode=display">S = \begin{bmatrix}\frac{2}{r - l}&0&0\\0&\frac{2}{u-b}&0\\0&0&\frac{2}{f - n}\end{bmatrix}</script><p>éœ€è¦æ³¨æ„çš„æ˜¯ç¼©æ”¾çš„è¿™äº›å€¼éƒ½æ˜¯æ­£å€¼ã€‚</p><p>ç„¶åé€šè¿‡é½æ¬¡åæ ‡å°†ä¸Šé¢ä¸¤ä¸ªç»“åˆèµ·æ¥å¾—åˆ°è½¬æ¢çŸ©é˜µï¼š</p><script type="math/tex; mode=display">M = \begin{bmatrix}\frac{2}{r - l}&0&0&-\frac{r+l}{r-l}\\0&\frac{2}{u-b}&0&-\frac{u+b}{u-b}\\0&0&\frac{2}{f - n}&-\frac{f+n}{f-n}\\0&0&0&1\end{bmatrix}</script><p>ä¸è¿‡äº‹æƒ…è¿˜æ²¡å®Œã€‚è¦çŸ¥é“ï¼Œåœ¨OpenGLä¸­ï¼Œè§„å®šæˆ‘ä»¬è§‚å¯Ÿçš„æ–¹å‘æ˜¯Zè½´çš„è´Ÿå‘ï¼ˆä¹Ÿå°±æ˜¯åœ¨è§†ç‚¹åæ ‡ä¸­ï¼Œx,yçš„åæ ‡éƒ½æ˜¯æœ‰æ­£æœ‰è´Ÿçš„ï¼Œä½†æ˜¯æˆ‘ä»¬å¾€å‰çœ‹åˆ°çš„ä¸œè¥¿çš„zåæ ‡éƒ½ä¸€å®šæ˜¯è´Ÿçš„ï¼‰ã€‚æ‰€ä»¥ä¸Šé¢çš„å¼å­å°±è¦æœ‰ç‚¹å˜åŒ–äº†ï¼Œæˆ‘ä»¬ä»ç„¶å¸Œæœ›è¿œçš„æŠ•å½±åˆ°+1,è€Œè¿‘çš„æŠ•å½±åˆ°-1,è¿™å°±è¦æ±‚å®é™…ä¸Šä¸å…‰è¦å¹³ç§»åˆ°åŸç‚¹ï¼Œåœ¨ç¼©æ”¾æ—¶å€™è¿˜è¦å°†è¿œè¿‘ä¸¤ä¸ªé¢é¢ å€’ã€‚è¿™æ—¶å€™å¹³ç§»å¤§å°å˜ä¸ºï¼š$\frac{f+n}{2}$(å› ä¸ºå®é™…åæ ‡æ˜¯-f,-n),è€Œä¸ºäº†è®©è¿œçš„æŠ•å½±åˆ°1,è€Œè¿‘çš„æŠ•å½±åˆ°-1,è¿™ä¸ªç¼©æ”¾å°ºåº¦å°±è¦å˜æˆè´Ÿæ•°ï¼Œä½¿å¾—ä½ç½®é¢ å€’ï¼Œå› æ­¤ç¼©æ”¾å°ºåº¦å˜ä¸ºï¼š$-\frac{2}{f-n}$ï¼Œæœ€åä¹˜è¿›å»åï¼Œå˜åŒ–çš„åªæœ‰ä¸€å°éƒ¨åˆ†ï¼š</p><script type="math/tex; mode=display">M = \begin{bmatrix}\frac{2}{r - l}&0&0&-\frac{r+l}{r-l}\\0&\frac{2}{u-b}&0&-\frac{u+b}{u-b}\\0&0&-\frac{2}{f - n}&-\frac{f+n}{f-n}\\0&0&0&1\end{bmatrix}</script><p>ä¹Ÿå°±æ˜¯ï¼Œå®é™…ä¸Šï¼Œåªæœ‰ä¸€é¡¹å˜åŒ–äº†ã€‚éœ€è¦æ³¨æ„çš„æ˜¯è¿™é‡Œçš„få’Œnéƒ½æ˜¯æ­£å€¼ã€‚</p><h2 id="é€å°„æŠ•å½±ï¼ˆPerspective-Projectionï¼‰"><a href="#é€å°„æŠ•å½±ï¼ˆPerspective-Projectionï¼‰" class="headerlink" title="é€å°„æŠ•å½±ï¼ˆPerspective Projectionï¼‰"></a>é€å°„æŠ•å½±ï¼ˆPerspective Projectionï¼‰</h2><p>é€å°„æŠ•å½±ä¸­ï¼Œè¿œå¤„çš„æ™¯è‰²æ€»æ˜¯æ›´è¿‘ä¸€ç‚¹ã€‚å®é™…ä¸Šè¿™å°±æ˜¯é€å°„æŠ•å½±ã€‚</p><p>ä¸‹é¢è¯´çš„è¿™ä¸ªä¸œè¥¿å’ŒSLAMä¸­è¯´çš„é’ˆå­”æ¨¡å‹å¾ˆç›¸ä¼¼ï¼šå‡å¦‚æœ‰ä¸€ä¸ªç‚¹åæ ‡ä¸º$X,Y,Z$ï¼Œè€Œé¢å‰æœ‰ä¸€ä¸ªå±å¹•ï¼Œåˆ°é’ˆå­”çš„è·ç¦»ä¸ºdï¼ˆd&gt;0ï¼‰ï¼Œé‚£ä¹ˆåœ¨å±å¹•ä¸Šè¿™ä¸€ç‚¹çš„æŠ•å½±ä¸ºï¼š</p><script type="math/tex; mode=display">X' = -d\frac X Z\\Y' = -d\frac Y Z</script><p>è¿™é‡Œè´Ÿå·çš„å­˜åœ¨ï¼Œè¿˜æ˜¯å› ä¸ºzåæ ‡éƒ½æ˜¯è´Ÿçš„ã€‚</p><p>è€Œæˆ‘å®é™…ä¸Šæˆ‘ä»¬å¯ä»¥å°†é€å°„æŠ•å½±è½¬æ¢å†™æˆè¿™æ ·ï¼š</p><script type="math/tex; mode=display">P = \begin{bmatrix}1&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&-\frac {1}{d}&0\end{bmatrix}</script><p>è¿™ä¸ªçŸ©é˜µä¹˜èµ·æ¥ä¹‹åä¹‹å‰çš„åæ ‡éƒ½æ²¡æœ‰æ”¹å˜ï¼Œé™¤äº†æœ€åä¸€é¡¹1å˜æˆäº†$-\frac Z d$. è€Œé½æ¬¡åæ ‡å¦‚æœå°†æœ€åä¸€ä¸ªè½¬åŒ–ä¸º1,åˆ™ä¹‹å‰çš„Xï¼ŒYï¼ŒZå˜æˆäº†ï¼š$-d\frac X Z, -d\frac Y Z,-d$.è¿™æ˜¯ä¸ªå¾ˆå·§å¦™çš„è½¬æ¢ã€‚</p><p>è€ŒOpenGLä¸­çš„é€æŠ•å½±å‡½æ•°ä¼šæ›´å¤æ‚ä¸€ç‚¹ã€‚æˆ‘ä»¬è¿˜æ˜¯é€šè¿‡è¯´æ˜gluPerspectiveï¼Œæ¥ç†è§£é€å°„æ˜ å°„ã€‚</p><p>é¦–å…ˆæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªæ–°çš„åè¯ï¼Œå«åšViewing Frustum(è§†é”¥ä½“)ã€‚ä¸€ä¸ªè§†é”¥ä½“å¦‚ä¸‹å›¾ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/IMG_0579.PNG" alt=""></p><p>ä»»ä½•è¿‘è£é¢è¿‘çš„ç‚¹æˆ–è€…æ¯”è¿œè£é¢è¿œçš„ç‚¹éƒ½ä¼šè¢«é®æŒ¡ã€‚</p><p>gluPerspectiveçš„å‚æ•°éœ€è¦ï¼šfovyï¼Œaspect,zNear,zFar(zNear,zFar&gt;0,åæ–‡ç®€å†™ä¸º$Z_n,Z_f$). fovyä¸ºè§†é‡ï¼Œå¯ä»¥ç†è§£ä¸ºçœ¼ç›çå¾—å¤§å°ç¨‹åº¦ï¼Œè€Œaspectå®šä¹‰äº†è§†é”¥çš„é«˜å®½æ¯”ã€‚</p><p>gluPerspectiveä¾ç„¶æ˜¯å°†è¿™ä¸ªè§†é”¥ä½“çš„æŠ•å½±ç»“æœè½¬æ¢åˆ°åæ ‡è½´çš„ä¸­å¿ƒæ­£æ–¹ä½“ï¼ˆè¾¹é•¿ä¸º2ï¼‰ï¼Œä½¿å¾—è¿‘æˆªé¢çš„zåæ ‡ä¸º1,è¿œæˆªé¢çš„zåæ ‡ä¸º-1.</p><p>è€ŒzNearå’ŒzFarä»£è¡¨äº†æˆ‘ä»¬éœ€è¦é€å°„æŠ•å½±çš„æœ€è¿‘è·ç¦»å’Œæœ€è¿œè·ç¦»ã€‚<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/IMG_0581.PNG" alt=""><br>æŠ•å½±åˆ°çš„â€å±å¹•â€ç”±ä¸‹å›¾ç¡®å®šï¼Œï¼ˆå…¶ä¸­æŠ•å½±å±å¹•é«˜ä¸ºä¸¤ä¸ªå•ä½ï¼‰ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/IMG_0580.PNG" alt=""><br>å› ä¸ºè¦æ˜ å°„åˆ°æœ€åçš„ä¸­å¿ƒæ­£æ–¹ä½“ï¼ˆå˜é•¿ä¸º2ï¼‰ï¼Œæ‰€ä»¥è¿™ä¸ªâ€œå±å¹•â€çš„é«˜å·²ç»å·²ç»ç¡®å®šäº†ï¼Œæ‰€ä»¥dçš„è·ç¦»ç”±$\theta$ç¡®å®šï¼Œè€Œ$\theta = \frac {fovy}2ï¼Œd = \cot \theta$.å¦ä¸€æ–¹é¢ï¼Œé«˜ç¡®å®šä¸º2, å› æ­¤aspectå®é™…ä¸Šæ”¹å˜æœ€ç»ˆæŠ•å½±çš„å®½çª„ï¼Œç”±ä¹‹å‰çš„åŸºç¡€ï¼Œæˆ‘ä»¬å…ˆè¿™æ ·å†™ä¸‹è¿™ä¸ªå¼å­ï¼š</p><script type="math/tex; mode=display">P = \begin{bmatrix}\frac 1 {aspect}&0&0&0\\0&1&0&0\\0&0&1&0\\0&0&-\frac {1}{d}&0\end{bmatrix}</script><p>æ—¢ç„¶é½æ¬¡åæ ‡æœ€ç»ˆæœ€åä¸€é¡¹è¦è½¬åŒ–ä¸º1ï¼Œä¹Ÿå°±æ˜¯åŒæ—¶ä¹˜ä»¥æŸä¸ªæ•°ä¸ä¼šå½±å“é½æ¬¡åæ ‡çš„å¤§å°ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸Šé¢ä¸ªçš„çŸ©é˜µå†™æˆï¼š</p><script type="math/tex; mode=display">P = \begin{bmatrix}\frac d {aspect}&0&0&0\\0&1&0&0\\0&0&A&B\\0&0&-1&0\end{bmatrix}</script><p>å› ä¸ºæˆ‘ä»¬æœ€åè¦å½±å“Zåæ ‡ï¼Œæ‰€ä»¥éœ€è¦æ”¹å˜çš„å€¼æ˜¯Aå’ŒBçš„ä½ç½®ï¼Œè€Œä¸èƒ½è®©ä»–ä»¬ä¸º0.ä»ä¸Šå¼æ±‚å¾—åæ ‡ï¼š</p><script type="math/tex; mode=display">p' = \begin{bmatrix}\frac d {aspect}&0&0&0\\0&d&0&0\\0&0&A&B\\0&0&-1&0\end{bmatrix} \begin{bmatrix}x\\y\\z\\1\end{bmatrix} = \begin{bmatrix}\frac {dx}{aspect} \\dy\\Az+B\\-z\end{bmatrix} =  \begin{bmatrix}-\frac {xd}{aspect*z} \\-\frac{yd}{z}\\-A-\frac B z\\1\end{bmatrix}</script><p>å› ä¸ºæˆ‘ä»¬è¦è®©è¿œè£å‰ªé¢åœ¨-1ï¼Œè¿‘è£å‰ªé¢åœ¨+1ï¼Œå› æ­¤ï¼š</p><script type="math/tex; mode=display">\left \{ \begin{matrix}-A-\frac B {-Z_f} = 1\\-A - \frac B {-Z_n} = =-1\end{matrix}\right .</script><p>å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">A =-\frac{Z_f+Z_n}{Z_f-Z_n} \\B = -\frac{2 Z_n Z_f}{Z_f - Z_n}</script><p>å› æ­¤å°†Aï¼ŒBå¸¦å…¥åå°±æ˜¯æœ€ågluPerspectiveå¾—åˆ°çš„çŸ©é˜µã€‚</p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><ul><li>åœ¨è¿™é‡Œæˆ‘ä»¬ä¸èƒ½å°†$Znear$è®¾ç½®ä¸º0,å¦‚æœé‚£æ ·çš„è¯ï¼Œä¼šå¯¼è‡´æ·±åº¦ä¿¡æ¯æ— æ³•è§£æã€‚</li><li>fovyè§†é‡è¶Šå¤§ï¼Œæˆ‘ä»¬çœ‹åˆ°çš„å¯¹è±¡å˜å¾—è¶Šå°ï¼Œè¿™æ˜¯å› ä¸ºå±å¹•å¤§å°æ˜¯å›ºå®šçš„ã€‚</li><li>æˆ‘ä¸æ˜ç™½ä¸ºä»€ä¹ˆopenGLè¦å°†è¿™ä¸ªæ˜ å°„åˆ°ç«‹æ–¹ä½“ä¸Šåšçš„è¿™ä¹ˆå¤æ‚ï¼Œæ›´è¿œçš„åœ°æ–¹ï¼ˆzå€¼æ›´å°ï¼‰æ˜ å°„åˆ°1ã€‚ä¸è¿‡gluPerspectiveåªæ˜¯ä¸€éƒ¨åˆ†ï¼Œé™¤äº†é€å°„æŠ•å½±ä»¥å¤–ï¼Œè¿˜è¦å¾—åˆ°å¾—åˆ°å¹³é¢åæ ‡ï¼Œç„¶åæ˜ å°„åˆ°å±å¹•ä¸Šã€‚</li><li>ä¼ å…¥å‡½æ•°çš„nearï¼Œfarï¼Œè®¡ç®—çš„åˆ°çš„dç­‰éƒ½æ˜¯è·ç¦»ï¼Œä¹Ÿå°±æ˜¯éƒ½æ˜¯æ­£å€¼ï¼Œä½†æ˜¯ä¸ºäº†å¤„ç†è´Ÿçš„åæ ‡å€¼ï¼Œå¤šäº†å¾ˆå¤šéº»çƒ¦ã€‚</li></ul>]]></content>
      
      
      <categories>
          
          <category> å›¾å½¢å­¦ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> computer graphics </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SLAMâ€”â€”ç›¸æœºä¸å›¾åƒ</title>
      <link href="/2018/11/12/SLAM%E2%80%94%E2%80%94%E7%9B%B8%E6%9C%BA%E4%B8%8E%E5%9B%BE%E5%83%8F/"/>
      <url>/2018/11/12/SLAM%E2%80%94%E2%80%94%E7%9B%B8%E6%9C%BA%E4%B8%8E%E5%9B%BE%E5%83%8F/</url>
      
        <content type="html"><![CDATA[<p>SLAMå»ºæ¨¡çš„è¿‡ç¨‹ä¸­ï¼Œç›¸æœºæ˜¯å¾ˆé‡è¦çš„ä¸€ä¸ªéƒ¨åˆ†ã€‚å› æ­¤è¿™ä¸€è®²ä¸»è¦æ¢è®¨ç›¸æœºçš„æˆåƒæ¨¡å‹ã€‚<br><a id="more"></a></p><h2 id="ç›¸æœºæ¨¡å‹"><a href="#ç›¸æœºæ¨¡å‹" class="headerlink" title="ç›¸æœºæ¨¡å‹"></a>ç›¸æœºæ¨¡å‹</h2><p>ç›¸æœºå°†ä¸‰ç»´ä¸–ç•Œä¸­çš„åæ ‡æ˜ å°„åˆ°äºŒç»´å¹³é¢ä¸Šã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥ç”¨ä¸€äº›å‡ ä½•æ¨¡å‹æ¥æè¿°ï¼Œæœ€ç®€å•çš„ç§°ä¸ºé’ˆå­”æ¨¡å‹ã€‚æˆ‘ä»¬åˆä¸­ç‰©ç†åº”è¯¥ä¹Ÿéƒ½å­¦è¿‡è¿™ä¸ªä¸œè¥¿ã€‚</p><h3 id="é’ˆå­”ç›¸æœºæ¨¡å‹"><a href="#é’ˆå­”ç›¸æœºæ¨¡å‹" class="headerlink" title="é’ˆå­”ç›¸æœºæ¨¡å‹"></a>é’ˆå­”ç›¸æœºæ¨¡å‹</h3><p>å‡è®¾ç›¸æœºå‰æ–¹ä¸ºzè½´ï¼Œå³ä¾§ä¸ºxè½´ï¼Œåˆ™æ ¹æ®å³æ‰‹åæ ‡å®šåˆ™ï¼Œyè½´æœä¸‹ã€‚å®é™…ä¸Šï¼Œè¿™ä¸ªå®šä¹‰ä½¿å¾—æŠ•å½±åˆ°ç›¸æœºå¹³é¢åçš„å¹³é¢åæ ‡å’Œåƒç´ åæ ‡æ–¹å‘ä¸€è‡´ã€‚è¿™ä¸ªåæ ‡è½´å°±æ˜¯ç›¸æœºåæ ‡ã€‚è€Œå¹³é¢åæ ‡ï¼Œä¹Ÿå°±æ˜¯é’ˆå­”åæ–¹çš„æŠ•å½±å¹³é¢æ˜¯ä¸€ä¸ªäºŒå…ƒåæ ‡ã€‚é’ˆå­”æˆåƒç»“æœæ˜¯å€’ç«‹çš„ï¼Œå‡å¦‚é’ˆå­”æ¨¡å‹çš„ç„¦è·ä¸ºfï¼Œåˆ™ï¼š</p><script type="math/tex; mode=display">\frac Z f = - \frac {X}{X'} = -\frac {Y}{Y'}</script><p>è´Ÿå·è¡¨ç¤ºå‰åå·¦å³é¢ å€’ã€‚</p><p>ä¸ºäº†ç®€åŒ–æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æˆåƒå¹³é¢æ”¾åˆ°é’ˆå­”å‰é¢ï¼Œä¸ä¸‰ç»´ç©ºé—´ç‚¹ä¸€ä¾§ï¼Œè¿™æ—¶å€™å¾—åˆ°çš„ç»“æœæ˜¯æ­£å¸¸çš„ï¼ˆå®é™…ä¸Šä¸€èˆ¬ç”Ÿæ´»ä¸­æˆ‘ä»¬éƒ½ä¼šå°†è¿™ä¸ªé¢ å€’ç»“æœè½¬æ¢ä¸ºæ­£å¸¸ç»“æœï¼ŒåŒ…æ‹¬æˆ‘ä»¬çš„è„‘å­ä¹Ÿæ˜¯è¿™ä¹ˆåšçš„ï¼‰ã€‚è¿™æ—¶å€™ï¼š</p><script type="math/tex; mode=display">\frac Z f =  \frac {X}{X'} = \frac {Y}{Y'}</script><p>æ•´ç†å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">X' = f\frac X Z\\Y' = f\frac Y Z</script><p>åœ¨ç›¸æœºä¸­ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°çš„æ˜¯ä¸€ä¸ªä¸ªåƒç´ ç‚¹ã€‚å‡å¦‚æˆåƒå¹³é¢æœ‰è¿™ä¹ˆä¸€ä¸ªåƒç´ åæ ‡è½´ï¼š$o-u-v$.åƒç´ åæ ‡è½´å’Œå¹³é¢åæ ‡è½´æœ‰å¹³ç§»å’Œç¼©æ”¾çš„å…³ç³»ï¼Œå‡å¦‚åƒç´ åæ ‡åŸç‚¹å¹³ç§»äº†$[c_x,c_y ]$,uè½´ç¼©æ”¾å¤§å°ä¸º$\alpha$,vè½´ç¼©æ”¾å¤§å°ä¸º$\beta$,åˆ™ï¼ˆç¼©æ”¾ä¹Ÿå°±æ˜¯ä¸€ç±³æœ‰å¤šå°‘åƒç´ ç‚¹ï¼Œå¦‚æœä¸€ç±³æœ‰10ä¸ªåƒç´ ç‚¹ï¼Œåˆ™åŸæ¥çš„1å¯èƒ½è¦å˜ä¸º10ï¼‰ï¼š</p><script type="math/tex; mode=display">u = X'\alpha + c_x\\v = Y'\beta + c_y</script><p>å°†ä¹‹å‰çš„å¼å­å¸¦å…¥ï¼Œå¹¶ä¸”å°†$f{\alpha}ï¼Œf(\beta)$åˆ†åˆ«è®°ä¸º$f_x,f_y$,å› ä¸ºfçš„å•ä½ä¸ºç±³ï¼Œè€Œ$\alpha,\beta$çš„å•ä½ä¸ºåƒç´ /ç±³ï¼Œå› æ­¤$f_x,f_y$çš„å•ä½ä¸ºåƒç´ ã€‚</p><p>å› æ­¤ä¸Šé¢çš„å¼å­å°±å¯ä»¥å†™ä¸ºï¼š</p><script type="math/tex; mode=display">\left \{\begin{matrix}u = f_x\frac X Z+c_x\\v = f_y\frac Y Z+c_y\end{matrix}\right .</script><p>å¦‚æœå†™æˆçŸ©é˜µå½¢å¼å°±æ›´æ˜ç™½ä¸€ç‚¹ï¼š</p><script type="math/tex; mode=display">\begin{pmatrix}u\\v\\1\end{pmatrix} = \frac 1 Z \begin{pmatrix}f_x&0&c_x\\0&f_y&c_y\\0&0&1\end{pmatrix}\begin{pmatrix}X\\Y\\Z \end{pmatrix}\triangleq \frac 1 Z K P</script><p>ä¸€èˆ¬æ¥è¯´ä¹ æƒ¯å°†Zç§»åˆ°å³ä¾§ã€‚ä¸Šå¼ä¸­ä¸­é—´é‡ä¸ºç›¸æœºå†…å‚ï¼Œä¸€èˆ¬æ¥è¯´åœ¨å‡ºå‚ä¹‹åå°±å›ºå®šäº†ã€‚ä¸çŸ¥é“çš„è¯ä¹Ÿå¯ä»¥ç”¨ç®—æ³•è¿›è¡Œæ ‡å®šã€‚</p><p>æ—¢ç„¶æœ‰å†…å‚ï¼Œå¯¹åº”çš„ä¹Ÿå°±æœ‰ä¸€ä¸ªå¤–å‚ã€‚æˆ‘ä»¬ä¸Šé¢çš„ä»‹ç»éƒ½æ˜¯ä»¥ç›¸æœºåæ ‡ä¸ºåŸºç¡€çš„ï¼Œè€Œç›¸æœºåæ ‡å®é™…ä¸Šæ˜¯ç”±ä¸–ç•Œåæ ‡è½¬æ¢çš„ã€‚ä»ä¸–ç•Œåæ ‡åˆ°ç›¸æœºåæ ‡çš„è½¬æ¢ï¼Œä¹‹å‰å›¾å½¢å­¦ä¸­ä»‹ç»ï¼Œéœ€è¦æ—‹è½¬Rå’Œå¹³ç§»tæ“ä½œï¼Œè€ŒRå’Œtå°±å¤Ÿæˆæ¥ç›¸æœºçš„å¤–å‚ï¼Œä¹Ÿå«ç›¸æœºçš„ä½å§¿ã€‚å‡å¦‚æŸä¸ªç‚¹åœ¨ä¸–ç•Œåæ ‡ä¸‹ä¸º$P_w$,åˆ™ï¼š</p><script type="math/tex; mode=display">ZP_{uv} = Z\begin{bmatrix}u\\v\\1\end{bmatrix} = K(RP_w+t) = KTP_w</script><p>ä¸Šå¼ä¸­ï¼ŒTä¸ºæ¬§å¼è½¬æ¢çŸ©é˜µã€‚æ‰€ä»¥è¿™å…¶ä¸­åŒ…å«äº†é½æ¬¡åæ ‡ä¸éé½æ¬¡åæ ‡çš„è½¬æ¢ã€‚</p><p>ä»å›¾å½¢å­¦çš„ä»‹ç»ä¸­æˆ‘ä»¬çŸ¥é“ä»ä¸–ç•Œåˆ°ç›¸æœºåæ ‡çš„è½¬æ¢æ˜¯å…ˆæ—‹è½¬åå¹³ç§»çš„ï¼Œæ‰€ä»¥ä¸Šå¸‚ä¸­çš„tæ˜¯å·²ç»è½¬æ¢åçš„å¹³ç§»çŸ©é˜µï¼Œè€Œéç›¸æœºçš„åæ ‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œä½¿å¾—æœ€åä¸€ç»´å€¼ä¸º1ï¼Œå®é™…ä¸Šåªè¦å°†Zè½´é™¤è¿›å»å°±å¥½.</p><h3 id="ç•¸å˜"><a href="#ç•¸å˜" class="headerlink" title="ç•¸å˜"></a>ç•¸å˜</h3><p>ç•¸å˜åˆ†ä¸ºå¾„å‘ç•¸å˜å’Œåˆ‡å‘ç•¸å˜ã€‚</p><p>å®é™…ä¸Šï¼Œè¿™äº›ç•¸å˜æˆ‘ä»¬åœ¨ç”Ÿæ´»ä¸­éƒ½ä¼šç»å¸¸é‡åˆ°ï¼Œç”±äºç°å®ä¸­æˆ‘ä»¬ç›¸æœºä¼šæœ‰é€é•œï¼Œæ‰€ä»¥ä¼šå¼•å…¥å¾„å‘ç•¸å˜ã€‚å¦‚æ‹çš„ç…§ç‰‡å‘ç°ä¸€ä¸ªç›´çš„ç”µçº¿æ†å˜æˆå¼¯çš„äº†ï¼Œè¿™å°±æ˜¯å¾„å‘ç•¸å˜ã€‚å¾„å‘ç•¸å˜ç¦»å…‰å¿ƒè¶Šè¿œè¶Šæ˜æ˜¾ï¼Œæœ‰æ—¶å€™æ˜¯æ¡¶å½¢ç•¸å˜ï¼Œæœ‰æ—¶å€™æ˜¯æ•å½¢ç•¸å˜ï¼ŒåŸç†ç±»ä¼¼ï¼Œå› ä¸ºå›¾åƒæ”¾å¤§ç‡éšç€ä¸å…‰è½´ä¹‹é—´çš„è·ç¦»å¢åŠ è€Œå˜å°æˆ–è€…å¢å¤§ã€‚</p><p>åˆ‡å‘ç•¸å˜æ˜¯ç”±äºæˆåƒå¹³é¢ä¸é€é•œä¸ä¸¥æ ¼å¹³è¡Œå¯¼è‡´çš„ã€‚ä¸ºäº†çº æ­£ç•¸å˜ï¼Œå°±è¦ç”¨æ•°å­¦æŠŠç•¸å˜æè¿°å‡ºæ¥ã€‚</p><p>å¯¹äºå¾„å‘ç•¸å˜ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªå¤šé¡¹å¼å‡½æ•°æ¥æè¿°ç•¸å˜å‰åçš„åæ ‡å˜åŒ–ï¼š</p><script type="math/tex; mode=display">x_{distorted} = x(1+k_1r^2 + k_2r^4 +k_3r^6)\\y_{distorted} = y(1+k_1r^2 + k_2r^4 +k_3r^6)</script><p>ä¸Šå¼ä¸­ï¼Œ$[x,y ]^T$æ˜¯å½’ä¸€åŒ–å¹³é¢ç‚¹çš„åæ ‡ï¼Œè€Œ$[x_{distorted},y_{distorted} ]$æ˜¯ç•¸å˜åçš„åæ ‡ã€‚</p><p>å¯¹äºåˆ‡å‘ç•¸å˜ï¼š</p><script type="math/tex; mode=display">x_{distorted} = x+2p_1xy+p_2(r^2+2x^2)\\y_{distorted} = y+p_1(r^2+2y^2) + 2p_2xy</script><p>ç»“åˆèµ·æ¥ï¼š</p><script type="math/tex; mode=display">\left\{\begin{matrix}x_{distorted} = x(1+k_1r^2 + k_2r^4 +k_3r^6)+2p_1xy+p_2(r^2+2x^2)\\y_{distorted} = y(1+k_1r^2 + k_2r^4 +k_3r^6)+p_1(r^2+2y^2) + 2p_2xy\end{matrix}\right.</script><p>å› æ­¤ï¼Œé€šè¿‡ç•¸å˜ç³»æ•°ï¼Œå’Œç›¸æœºåæ ‡ç³»ä¸­çš„ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°æ­£ç¡®çš„åæ ‡ç‚¹ã€‚ç„¶åå†é€šè¿‡å†…å‚çŸ©é˜µå¾—åˆ°æ­£ç¡®çš„å›¾åƒåæ ‡ã€‚</p><p>å»ç•¸å˜æ˜¯ä¸€ä¸ªä¸­é—´è¿‡ç¨‹ï¼Œæˆ‘ä»¬å‡è®¾ä»¥åçš„å¤„ç†å›¾åƒå·²ç»å¤„ç†è¿‡ç•¸å˜ã€‚</p><p>æ‰€ä»¥å•ç›®ç›¸æœºæˆåƒè¿‡ç¨‹å°±æ˜¯é¦–å…ˆé€šè¿‡ä½å§¿ä¼°è®¡å¾—åˆ°ç›¸æœºå¤–å‚ï¼ˆå®é™…ä¸Šè¿™ä¸ªä¸æ˜¯æˆåƒè¿‡ç¨‹çš„ä¸€éƒ¨åˆ†ï¼Œä½†æ˜¯å¯¹äºSLAMæ˜¯éå¸¸é‡è¦çš„ä¸€æ­¥ï¼‰ï¼Œç„¶åå°†ä¸–ç•Œåæ ‡ç‚¹è½¬æ¢ä¸ºç›¸æœºåæ ‡ï¼Œç»è¿‡å½’ä¸€åŒ–å¤„ç†ï¼Œé€šè¿‡å†…å‚è®¡ç®—å‡ºæ¥åƒç´ ä½ç½®å“¦ã€‚</p><h3 id="åŒç›®ç›¸æœºæ¨¡å‹"><a href="#åŒç›®ç›¸æœºæ¨¡å‹" class="headerlink" title="åŒç›®ç›¸æœºæ¨¡å‹"></a>åŒç›®ç›¸æœºæ¨¡å‹</h3><p>é€šè¿‡å•ç›®ç›¸æœºçš„æˆåƒè¿‡ç¨‹ï¼Œæˆ‘ä»¬æ— æ³•çŸ¥é“åƒç´ å¯¹åº”çš„ç‚¹åœ¨ç©ºé—´ç‚¹çš„ä½ç½®ã€‚è¿™æ˜¯å› ä¸ºåªè¦åœ¨å…‰å¿ƒå’Œåƒç´ ç‚¹è¿™æ¡çº¿ä¸Šï¼Œä»–ä»¬éƒ½æ˜¯å¯ä»¥æŠ•å½±åˆ°ä¸€ä¸ªç‚¹ï¼Œæƒ³è¦ç¡®å®šå®ƒçš„ç©ºé—´ä½ç½®ï¼Œéœ€è¦çŸ¥é“æ·±åº¦ï¼Œä¹Ÿå°±éœ€è¦é€šè¿‡åŒç›®ç›¸æœºæˆ–è€…RGB-Dç›¸æœºã€‚</p><p>åŒç›®ç›¸æœºç±»ä¼¼äºæˆ‘ä»¬çš„çœ¼ç›ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡çœ¼ç›æ¥åˆ¤æ–­ç‰©ä½“ä¸æˆ‘ä»¬ä¹‹é—´çš„è·ç¦»ã€‚ä¸€èˆ¬åŒç›®ç›¸æœºéƒ½æ˜¯å·¦å³åˆ†å¸ƒçš„ï¼Œä½†ä¹Ÿæœ‰ä¸Šä¸‹çš„ã€‚ä¸è¿‡æˆ‘ä»¬åœ¨è¿™é‡Œä»‹ç»çš„æ˜¯ä»¥å·¦å³åˆ†å¸ƒçš„ã€‚è¿™ä¿è¯äº†ä¸¤ä¸ªç›¸æœºçš„æˆåƒåªåœ¨xåæ ‡ä¸Šæœ‰åç§»ã€‚</p><p>ä¸ºäº†è¯´æ˜ç™½åŒç›®ç›¸æœºçš„æˆåƒæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™ä¸ªæ¨¡å‹ç”»å‡ºæ¥ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬åƒä¹‹å‰ä¸€æ ·å°†æˆåƒå¹³é¢æ”¾åˆ°å…‰å¿ƒå‰é¢ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/IMG_0516.JPG" alt=""><br>åŒç›®ç›¸æœºä¸¤ä¸ªå…‰å¿ƒçš„è·ç¦»ç§°ä¸ºåŸºçº¿bï¼Œä¹Ÿå°±æ˜¯$O_l,O_r$ä¹‹é—´çš„è·ç¦»ã€‚$u_r,u_l$åˆ†åˆ«ä¸ºå„è‡ªæˆåƒå¹³é¢ä¸Šçš„åæ ‡ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯$u_l$ä¸ºè´Ÿæ•°ã€‚</p><p>è€ƒè™‘ä¸€ä¸ªç©ºé—´ç‚¹Pï¼Œå®ƒåœ¨å·¦çœ¼å’Œå³çœ¼å„æˆä¸€ä¸ªåƒï¼Œè®°åš$P_l,P_r$.æ ¹æ®ç›¸ä¼¼ä¸‰è§’å½¢ï¼š</p><script type="math/tex; mode=display">\frac{z-f}{z} = \frac{b-u_l+u_r}{b}</script><p>å³ï¼š</p><script type="math/tex; mode=display">z = \frac{fb}{u_l - u_r}</script><p>è¿™æ ·å°±è®¡ç®—å‡ºæ¥äº†è·ç¦»zã€‚ä¸Šå¼ä¸­ï¼š$u_l - u_r=d$,æˆ‘ä»¬ç§°ä¸ºè§†å·®ã€‚å¯ä»¥çœ‹åˆ°è§†å·®è¶Šå¤§ï¼Œåˆ™è·ç¦»è¶Šè¿œã€‚å®é™…ä¸­ä¸Šè§†å·®æœ€å°ä¸º1,æ‰€ä»¥æœ€è¿œçš„è·ç¦»å°±ç”±fbé™å®šäº†ã€‚æ‰€ä»¥åŒç›®ç›¸æœºå¯ä»¥æµ‹é‡çš„è·ç¦»æ˜¯æœ‰é™çš„ã€‚</p><p>åŸç†ç®€å•ï¼Œä½†æ˜¯dä¸å¥½è®¡ç®—ã€‚å› ä¸ºæˆ‘ä»¬éœ€è¦çŸ¥é“å·¦çœ¼å›¾åƒçš„åƒç´ å‡ºç°åœ¨å³çœ¼å›¾åƒçš„å“ªä¸ªä½ç½®ã€‚å› æ­¤è®¡ç®—é‡å’Œç²¾åº¦éƒ½ä¼šé€ æˆé—®é¢˜ã€‚</p><h3 id="RGB-Dç›¸æœºæ¨¡å‹"><a href="#RGB-Dç›¸æœºæ¨¡å‹" class="headerlink" title="RGB-Dç›¸æœºæ¨¡å‹"></a>RGB-Dç›¸æœºæ¨¡å‹</h3><p>RGB-Dç›¸æœºä¹Ÿå°±æ˜¯æ·±åº¦ç›¸æœºï¼Œå®ƒèƒ½å¤Ÿä¸»åŠ¨æµ‹é‡æ·±åº¦ã€‚ç›®å‰æ·±åº¦ç›¸æœºä¸»è¦æœ‰ä¸¤ä¸ªåŸç†ï¼š</p><ol><li>çº¢å¤–ç»“æ„å…‰ï¼Œå¦‚kinect 1ä»£</li><li>é£è¡Œæ—¶é—´æ³•ï¼Œå¦‚kinect 2ä»£</li></ol><p>å®ƒä»¬åŸç†éƒ½éœ€è¦å‘ç›®æ ‡å‘å°„å…‰çº¿ã€‚ç»“æ„å…‰æ ¹æ®è¿”å›çš„ç»“æ„å…‰å›¾æ¡ˆæ¥è®¡ç®—è·ç¦»ï¼Œè€Œé£è¡Œæ—¶é—´æ³•åˆ™æ ¹æ®å…‰æŸé£è¡Œçš„æ—¶é—´æ¥è®¡ç®—ã€‚RGB-Dç›¸æœºå¯ä»¥è·å¾—æ•´ä¸ªå›¾åƒçš„åƒç´ æ·±åº¦ï¼Œä¸€èˆ¬ä¼šè¾“å‡ºä¸€ä¸€å¯¹åº”çš„å½©è‰²å›¾å’Œæ·±åº¦å›¾ã€‚</p><p>å½“ç„¶ï¼ŒRGB-Dç›¸æœºèƒ½å¤Ÿå®æ—¶æµ‹é‡æ·±åº¦ï¼Œä½†æ˜¯ä½¿ç”¨æ¡ä»¶ä¼šå—é™ï¼Œå®¹æ˜“æ”¶åˆ°å…¶ä»–å…‰çº¿å¹²æ‰°ã€‚å¯¹äºé€å°„æè´¨çš„ç‰©ä½“ï¼Œå› ä¸ºæ¥å—ä¸åˆ°åå°„å…‰ï¼Œæ‰€ä»¥æ— æ³•æµ‹é‡è¿™äº›ç‚¹çš„ä½ç½®ã€‚æ­¤å¤–ï¼Œå®ƒåœ¨æˆæœ¬ï¼ŒåŠŸè€—æ–¹é¢éƒ½æœ‰åŠ£åŠ¿ã€‚</p><h2 id="OpenCV"><a href="#OpenCV" class="headerlink" title="OpenCV"></a>OpenCV</h2><p>æ¥ä¸‹æ¥ç†Ÿæ‚‰ä¸€ä¸‹OpenCVï¼Œæ˜¯ä¸€ä¸ªå¼€æºè§†è§‰åº“ã€‚</p><p>ä¸‹é¢è¿™æ®µä»£ç ç®€å•ä»‹ç»äº†ä¸€äº›OpenCVçš„æ“ä½œï¼š<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;chrono&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;opencv2/highgui/highgui.hpp&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   cv:: Mat image =   cv::imread(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="keyword">if</span>(image.data == <span class="literal">nullptr</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">"Error:the image file may not exist"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//output some basic information</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"Width:"</span>&lt;&lt;image.cols&lt;&lt;<span class="string">" height:"</span>&lt;&lt;image.rows&lt;&lt;<span class="string">" channel number:"</span>&lt;&lt;image.channels()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">      cv::imshow(<span class="string">"image"</span>,image);</span><br><span class="line">      cv::waitKey(<span class="number">0</span>);</span><br><span class="line">    <span class="comment">// judge the type of the image</span></span><br><span class="line">    <span class="keyword">if</span>(image.type()!=CV_8UC1 &amp;&amp; image.type()!=CV_8UC3)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">"the image must be rgb image or grey-scale map"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//iteration</span></span><br><span class="line">    <span class="comment">//use chrono to compute the time.</span></span><br><span class="line">    chrono::steady_clock::time_point t1 = chrono::steady_clock::now();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">size_t</span> y=<span class="number">0</span>;y&lt;image.cols;++y)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">size_t</span> x = <span class="number">0</span>;x&lt;image.rows;++x)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//left the ptr point at the y row.</span></span><br><span class="line">            <span class="keyword">unsigned</span> <span class="keyword">char</span>* row_ptr = image.ptr&lt;<span class="keyword">unsigned</span> <span class="keyword">char</span>&gt;(y);</span><br><span class="line">            <span class="comment">//get position (x,y)'s ptr,because there are image.channels()*1 char.</span></span><br><span class="line">            <span class="keyword">unsigned</span> <span class="keyword">char</span> *data_ptr = &amp;row_ptr[x*image.channels()];</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> c = <span class="number">0</span>;c!=image.channels();++c)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">//Do someting ;</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    chrono::steady_clock::time_point t2 = chrono::steady_clock::now();</span><br><span class="line">    chrono::duration&lt;<span class="keyword">double</span>&gt; time_used = chrono::duration_cast&lt;chrono::duration&lt;<span class="keyword">double</span>&gt;&gt;(t2-t1);</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"We use "</span>&lt;&lt;time_used.count()&lt;&lt;<span class="string">" seconds to scan the image"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// copy or reference</span></span><br><span class="line">    <span class="comment">//this is a reference,or ptr, so if we change the image_another,the original image will be changed</span></span><br><span class="line">      cv::Mat image_another = image;</span><br><span class="line">    <span class="comment">//let the left top corner(100,100) block to be 0</span></span><br><span class="line">    image_another(  cv::Rect(<span class="number">0</span>,<span class="number">0</span>,<span class="number">100</span>,<span class="number">100</span>)).setTo(<span class="number">0</span>);</span><br><span class="line">      cv::imshow(<span class="string">"image"</span>,image);</span><br><span class="line">      cv::waitKey(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//copy ,clone</span></span><br><span class="line">      cv::Mat image_clone = image.clone();</span><br><span class="line">      image_clone(cv::Rect(<span class="number">0</span>,<span class="number">0</span>,<span class="number">100</span>,<span class="number">100</span>)).setTo(<span class="number">255</span>);</span><br><span class="line">      cv::imshow(<span class="string">"image"</span>,image);</span><br><span class="line">      cv::imshow(<span class="string">"imageclone"</span>,image_clone);</span><br><span class="line">      cv::waitKey(<span class="number">0</span>);</span><br><span class="line">      cv::destroyAllWindows();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>CMakeLists.txt:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">"-std=c++11"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">find_package</span>(OpenCV REQUIRED)</span><br><span class="line"></span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;OpenCV_INCLUDE_DIRS&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(imageOP imageOP.cpp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(imageOP <span class="variable">$&#123;OpenCV_LIBS&#125;</span>)</span><br></pre></td></tr></table></figure></p><h2 id="æ€»ç»“ï¼šç‚¹äº‘æ‹¼æ¥"><a href="#æ€»ç»“ï¼šç‚¹äº‘æ‹¼æ¥" class="headerlink" title="æ€»ç»“ï¼šç‚¹äº‘æ‹¼æ¥"></a>æ€»ç»“ï¼šç‚¹äº‘æ‹¼æ¥</h2><p>ç°åœ¨ä½¿ç”¨ä¹‹å‰ä»‹ç»çš„çŸ¥è¯†ï¼Œæ¥å®Œæˆä¸€ä¸ªç‚¹äº‘æ‹¼æ¥çš„ä»»åŠ¡ã€‚è¿™ä¸ªéœ€è¦ç”¨åˆ°é«˜åšæä¾›çš„ä¸€äº›å›¾ç‰‡æ•°æ®ã€‚åœ°å€ï¼š<a href="https://github.com/gaoxiang12/slambook/tree/master/ch5/joinMap" target="_blank" rel="noopener">joinMap</a>.</p><p>ç‚¹äº‘çš„æ“ä½œéœ€è¦ç”¨åˆ°ç‚¹äº‘åº“PCLã€‚</p><p>é¦–å…ˆè¯´æ˜ä»¥ä¸‹ï¼ŒçŸ¥é“åƒç´ åæ ‡ä»¥åŠæ·±åº¦ä¿¡æ¯ï¼Œå¦‚ä½•æ¢å¤ç›¸æœºåæ ‡ï¼Ÿå®é™…ä¸Šåæ¨éå¸¸å®¹æ˜“ï¼š</p><script type="math/tex; mode=display">X = \frac{u - c_x}{f_x}Z\\Y = \frac{v - c_y}{f_y}Z</script><p>è€Œæˆ‘ä»¬åšçš„ä¹Ÿæ­£æ˜¯ç”¨ä¸Šé¢çš„è¿‡ç¨‹æ¥è¿›è¡Œç‚¹äº‘æ¢å¤ã€‚</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;fstream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;opencv2/core/core.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;opencv2/highgui/highgui.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;Eigen/Geometry&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;boost/format.hpp&gt;//format strings</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;pcl/point_types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;pcl/io/pcd_io.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;pcl/visualization/pcl_visualizer.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;cv::Mat&gt; colorImgs,depthImgs;</span><br><span class="line">    <span class="built_in">vector</span>&lt;Eigen::Isometry3d,Eigen::aligned_allocator&lt;Eigen::Isometry3d&gt;&gt; poses;<span class="comment">//poses of camera</span></span><br><span class="line">    <span class="function">ifstream <span class="title">fin</span><span class="params">(argv[<span class="number">2</span>])</span></span>;</span><br><span class="line">    <span class="keyword">if</span>(!fin)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">"We need information about poses"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i!=<span class="number">5</span>;++i)</span><br><span class="line">    &#123;</span><br><span class="line">        boost::<span class="function">format <span class="title">fmt</span><span class="params">(argv[<span class="number">1</span>]+<span class="built_in">string</span>(<span class="string">"/%s/%d.%s"</span>))</span></span>;<span class="comment">//format of image files</span></span><br><span class="line">        colorImgs.push_back(cv::imread((fmt%<span class="string">"color"</span>%(i+<span class="number">1</span>)%<span class="string">"png"</span>).str()));</span><br><span class="line">        depthImgs.push_back(cv::imread((fmt%<span class="string">"depth"</span>%(i+<span class="number">1</span>)%<span class="string">"pgm"</span>).str(),<span class="number">-1</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">double</span> data[<span class="number">7</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">        <span class="comment">//get poses to data</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">auto</span> &amp;d:data)</span><br><span class="line">           fin&gt;&gt;d;</span><br><span class="line">        Eigen::<span class="function">Quaterniond <span class="title">q</span><span class="params">(data[<span class="number">6</span>],data[<span class="number">3</span>],data[<span class="number">4</span>],data[<span class="number">5</span>])</span></span>;</span><br><span class="line">        Eigen::<span class="function">Isometry3d <span class="title">T</span><span class="params">(q)</span></span>;</span><br><span class="line">        T.pretranslate(Eigen::Vector3d(data[<span class="number">0</span>],data[<span class="number">1</span>],data[<span class="number">2</span>]));</span><br><span class="line">        poses.push_back(T);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">double</span> cx = <span class="number">325.5</span>;</span><br><span class="line"><span class="keyword">double</span> cy=<span class="number">253.5</span>;</span><br><span class="line"><span class="keyword">double</span> fx = <span class="number">518.0</span>;</span><br><span class="line"><span class="keyword">double</span> fy = <span class="number">519.0</span>;</span><br><span class="line"><span class="keyword">double</span> depthScale = <span class="number">1000.0</span>;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">"try to get pointcloud..."</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">//use XYZRGB format </span></span><br><span class="line"><span class="keyword">typedef</span> pcl::PointXYZRGB PointT;</span><br><span class="line"><span class="keyword">typedef</span> pcl:: PointCloud&lt;PointT&gt; PointCloud;</span><br><span class="line"></span><br><span class="line">PointCloud::<span class="function">Ptr <span class="title">pointCloud</span><span class="params">(<span class="keyword">new</span> PointCloud())</span></span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;<span class="number">5</span>;++i)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"transform the images..."</span>&lt;&lt;i&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    cv::Mat color = colorImgs[i];</span><br><span class="line">    cv::Mat depth = depthImgs[i];</span><br><span class="line">    Eigen::Isometry3d T = poses[i];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> v = <span class="number">0</span>;v&lt;color.rows;++v)</span><br><span class="line">     <span class="keyword">for</span> (<span class="keyword">int</span> u = <span class="number">0</span>;u&lt;color.cols;++u)</span><br><span class="line">     &#123;</span><br><span class="line">         <span class="keyword">unsigned</span> <span class="keyword">int</span> d = depth.ptr&lt;<span class="keyword">unsigned</span> <span class="keyword">short</span>&gt;(v)[u];<span class="comment">//get depth of the pixel</span></span><br><span class="line">         <span class="keyword">unsigned</span> <span class="keyword">char</span>* cptr = color.ptr&lt;<span class="keyword">unsigned</span> <span class="keyword">char</span>&gt;(v);</span><br><span class="line">         </span><br><span class="line">         <span class="keyword">if</span>(d == <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">         Eigen::Vector3d point;</span><br><span class="line">         point[<span class="number">2</span>] = <span class="keyword">double</span>(d) / depthScale;</span><br><span class="line">         point[<span class="number">0</span>] = (u - cx)*point[<span class="number">2</span>]/fx;</span><br><span class="line">         point[<span class="number">1</span>] = (v - cy)*point[<span class="number">2</span>]/fy;</span><br><span class="line">         Eigen::Vector3d pointWorld = T*point;</span><br><span class="line"></span><br><span class="line">         PointT p;</span><br><span class="line">         p.x = pointWorld[<span class="number">0</span>];</span><br><span class="line">         p.y = pointWorld[<span class="number">1</span>];</span><br><span class="line">         p.z = pointWorld[<span class="number">2</span>];</span><br><span class="line">         p.b = cptr[u*color.channels()];</span><br><span class="line">         p.g = cptr[u*color.channels()+<span class="number">1</span>];</span><br><span class="line">         p.r = cptr[u*color.channels()+<span class="number">2</span>];</span><br><span class="line">         pointCloud-&gt;points.push_back(p);</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line">pointCloud-&gt;is_dense = <span class="literal">false</span>;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">"points number:"</span>&lt;&lt;pointCloud-&gt;size()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">pcl::io::savePCDFileBinary(<span class="string">"map.pcd"</span>,*pointCloud);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CMakeLists.txt:<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">find_package</span>(OpenCV REQUIRED)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;OpenCV_INCLUDE_DIRS&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">include_directories</span>(<span class="string">"usr/include/eigen3/"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">find_package</span>(PCL REQUIRED COMPONENT common io)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;PCL_INCLUDE_DIRS&#125;</span>)</span><br><span class="line"><span class="keyword">add_definitions</span>(<span class="variable">$&#123;PCL_DEFINITIONS&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(joinMap joinMap.cpp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(joinMap <span class="variable">$&#123;OpenCV_LIBS&#125;</span> <span class="variable">$&#123;PCL_LIBRARIES&#125;</span>)</span><br></pre></td></tr></table></figure></p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/IMG_0519.PNG" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> opencv </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SLAMâ€”â€”æç¾¤å’Œæä»£æ•°</title>
      <link href="/2018/11/09/SLAM%E2%80%94%E2%80%94%E6%9D%8E%E7%BE%A4%E5%92%8C%E6%9D%8E%E4%BB%A3%E6%95%B0/"/>
      <url>/2018/11/09/SLAM%E2%80%94%E2%80%94%E6%9D%8E%E7%BE%A4%E5%92%8C%E6%9D%8E%E4%BB%A3%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>åœ¨SLAMä¸­ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å§¿æ€è¿›è¡Œä¼°è®¡å’Œä¼˜åŒ–ã€‚ä½†æ˜¯æ—‹è½¬çŸ©é˜µè‡ªèº«æ˜¯æœ‰çº¦æŸçš„ï¼Œå¢åŠ ä¼˜åŒ–éš¾åº¦ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦å¼•å…¥æç¾¤å’Œæä»£æ•°ï¼Œå¯ä»¥å°†ä½å§¿ä¼°è®¡è½¬æ¢ä¸ºæ— çº¦æŸçš„ä¼˜åŒ–é—®é¢˜ã€‚<br><a id="more"></a></p><h2 id="ç¾¤-Group"><a href="#ç¾¤-Group" class="headerlink" title="ç¾¤(Group)"></a>ç¾¤(Group)</h2><p>ç¾¤æ˜¯ä¸€ç§é›†åˆåŠ ä¸Šä¸€ç§è¿ç®—çš„ä»£æ•°ç»“æ„ã€‚å¦‚æœè®°é›†åˆä¸º$A$ï¼Œè¿ç®—ä¸º$\cdot$,å¦‚æœæ»¡è¶³ä»¥ä¸‹æ€§è´¨ï¼Œåˆ™ç§°$(A,\cdot)$ä¸ºç¾¤ï¼š</p><ol><li>å°é—­æ€§ï¼š $\forall a_1,a_2\in A,a_1 \cdot a_2 \in A$.</li><li>ç»“åˆå¾‹ï¼š$\forall a_1,a_2,a_3 \in A (a_1 \cdot a_2) \cdot a_3 = a_1 \cdot (a_2 \cdot a_3)$.</li><li>å¹ºå…ƒï¼š$\exists a_0 \in A, s.t. \forall a \in A,a_0\cdot a = a \cdot a_0 = a$.</li><li>é€†ï¼š$\forall a\in A,\exists a^{-1} \in A, a\cdot a^{-1} = a_0$.</li></ol><p>å¯ä»¥éªŒè¯æ—‹è½¬çŸ©é˜µå’Œè½¬æ¢çŸ©é˜µä¸çŸ©é˜µä¹˜æ³•è¿ç®—éƒ½å¯ä»¥æ„æˆç¾¤ã€‚å…¶ä¸­æ—‹è½¬çŸ©é˜µä¸ä¹˜æ³•æ„æˆçš„ç¾¤ä¸º$SO(3)$,ç‰¹æ®Šæ­£äº¤ç¾¤ï¼Œæ¬§å¼è½¬æ¢çŸ©é˜µä¸ä¹˜æ³•æ„æˆç‰¹æ®Šæ¬§æ°ç¾¤$SE(3)$.</p><h3 id="æç¾¤ï¼ˆLie-Groupï¼‰"><a href="#æç¾¤ï¼ˆLie-Groupï¼‰" class="headerlink" title="æç¾¤ï¼ˆLie Groupï¼‰"></a>æç¾¤ï¼ˆLie Groupï¼‰</h3><p>å…·æœ‰è¿ç»­ï¼ˆå…‰æ»‘ï¼‰æ€§è´¨çš„ç¾¤ã€‚æç¾¤æ—¢æ˜¯ç¾¤ä¹Ÿæ˜¯æµå½¢ã€‚SO(3),SE(3)éƒ½æ˜¯æç¾¤ï¼Œä½†æ˜¯åªæœ‰å®šä¹‰è‰¯å¥½çš„ä¹˜æ³•ï¼Œæ²¡æœ‰åŠ æ³•ï¼Œæ‰€ä»¥éš¾ä»¥è¿›è¡Œæé™æ±‚å¯¼ï¼ˆ$+\Delta x$ï¼‰ç­‰æ“ä½œã€‚</p><h2 id="å¼•å‡ºæä»£æ•°"><a href="#å¼•å‡ºæä»£æ•°" class="headerlink" title="å¼•å‡ºæä»£æ•°"></a>å¼•å‡ºæä»£æ•°</h2><p>ä¸€ç§æç¾¤ï¼Œå¯¹åº”ä¸€ä¸ªæä»£æ•°ã€‚æä»£æ•°ç”¨å°å†™è¡¨ç¤ºï¼Œå¦‚$so(3),se(3)$.</p><p>ä»»æ„æ—‹è½¬çŸ©é˜µï¼Œåˆ™$RR^T = I$.</p><p>è€ƒè™‘Réšæ—¶é—´å˜åŒ–ï¼Œæœ‰$R(t)R(t)^T = I$.</p><p>ä¸¤ä¾§å¯¹tæ±‚å¯¼ï¼š$Râ€™(t)R(t)^T + R(t)Râ€™(t)^T = 0$.</p><p>åˆ™ï¼š$Râ€™(t)R(t)^T = -(Râ€™(t)^T R(t)^T)^T$.</p><p>æ‰€ä»¥æˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼š$Râ€™(t)R(t)^T$æ˜¯ä¸€ä¸ªåå¯¹ç§°çŸ©é˜µã€‚åå¯¹ç§°çŸ©é˜µéƒ½ä¼šæœ‰ä¸€ä¸ªå¯¹åº”çš„å‘é‡ï¼Œå‡è®¾$Râ€™(t)R(t)^T$å¯¹åº”çš„å‘é‡ä¸º$\phi(t)$,åˆ™ï¼š</p><p>$Râ€™(t)R(t)^T = \phi(t)$</p><p>ä¸¤ä¾§åŒæ—¶å³ä¹˜$R(t)$ï¼Œå¾—åˆ°ï¼š</p><p>$Râ€™(t) = \phi(t)^{\hat{}} R(t)$</p><p>æ‰€ä»¥æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¯¹Ræ±‚å¯¼ä¹‹æµ·ï¼Œå¤šå‡ºæ¥ä¸€ä¸ª$\phi(t)$.</p><p>å‡å¦‚$t_0=0,R(t_0) = I$ï¼Œå¯¹äº$R(t)$è¿›è¡Œæ³°å‹’å±•å¼€ï¼š</p><script type="math/tex; mode=display">R(t) \approx R(t_0) + R'(t_0)(t - t_0) = I+\phi(0)t.</script><p>åœ¨å¾ˆçŸ­çš„æ—¶é—´($t=t_0+\Delta t$)é‡Œï¼Œå‡è®¾$\phi(t_0)$ä¸ä¼šå˜åŒ–ã€‚</p><p>$Râ€™(t) = \phi(t_0)^{\hat{}}R(t) = \phi_0^{\hat{}}R(t)$</p><p>å¦‚æœè§£ä¸Šè¿°å¾®åˆ†æ–¹ç¨‹ï¼ˆ$R(0) = I$ï¼‰ï¼Œå¯ä»¥å¾—åˆ°ï¼š$R(t) = exp(\phi_0^{\hat{}} t)$</p><p>æ‰€ä»¥æˆ‘ä»¬å°±å¾—åˆ°äº†ä¸€ä¸ªåœ¨$t_0$é™„è¿‘çš„è¿‘ä¼¼ä¼°è®¡ã€‚</p><p>å®é™…ä¸Š$phi$å°±æ˜¯$SO(3)$å¯¹åº”çš„æä»£æ•°ã€‚</p><h2 id="æä»£æ•°"><a href="#æä»£æ•°" class="headerlink" title="æä»£æ•°"></a>æä»£æ•°</h2><p>æä»£æ•°ç”±ä¸€ä¸ªé›†åˆ$\mathbb{V}$ï¼Œä¸€ä¸ªæ•°åŸŸ$\mathbb{F}$,ä¸€ä¸ªäºŒå…ƒè¿ç®—$[ ,]$ç»„æˆã€‚è¿™ä¸ªè¿ç®—æŸä¸€å®šç¨‹åº¦ä¸Šåæ˜ äº†ä¸¤ä¸ªæ•°çš„å·®å¼‚ã€‚æä»£æ•°æ»¡è¶³ä¸‹é¢å‡ ä¸ªæ€§è´¨ï¼š</p><ol><li>å°é—­æ€§ï¼š$\forall X,Y \in \mathbb{V},[X,Y ] \in \mathbb{V}$</li><li>åŒçº¿æ€§ï¼š$\forall X,Y,Z \in \mathbb{V},a,b \in \mathbb{F}$,æœ‰ï¼š<script type="math/tex; mode=display">[aX+bY,Z ]= a[X,Z ] + b[Y,Z ],[Z,aX+bY ] = a[Z,X ]+b[Z,Y ].</script></li><li>è‡ªåæ€§ï¼š$\forall X \in \mathbb{V},[X, X] = 0$</li><li>é›…ç§‘æ¯”ç­‰ä»·ï¼š$\forall X,Y,Z \in \mathbb{V},[X, [Y,Z ]] + [Z, [X,Y ]]+ [Y, [Z,X ]] = 0$</li></ol><p>å…¶ä¸­äºŒå…ƒè¿ç®—è¢«ç§°ä¸ºææ‹¬å·ã€‚</p><h3 id="æä»£æ•°-so-3-ï¼š"><a href="#æä»£æ•°-so-3-ï¼š" class="headerlink" title="æä»£æ•°$so(3)$ ###ï¼š"></a>æä»£æ•°$so(3)$ ###ï¼š</h3><p>$so(3) = \{\phi \in \mathbb{R}^3 ,\Phi = \phi ^{\hat{}} \in \mathbb{R}^{3 \times 3}\}$.</p><p>ææ‹¬å·ï¼š$[\phi_1,\phi_2 ]= (\Phi_1\Phi_2 - \Phi_2 \Phi_1)^{\hat{}}$.</p><p>Note: $\Phi = \phi^{\hat{}},\phi = \Phi^{\hat{}}$.</p><h3 id="æä»£æ•°-se-3"><a href="#æä»£æ•°-se-3" class="headerlink" title="æä»£æ•°$se(3)$"></a>æä»£æ•°$se(3)$</h3><p>$se(3) = \left\{<br>    \xi = \begin{bmatrix}<br>    \rho\\<br>    \phi<br>    \end{bmatrix} \in \mathbb{R}^6,\rho \in \mathbb{R}^3,\phi \in so(3),\xi ^{\hat{}} = \begin{bmatrix}<br>    \phi^{\hat{}}&amp;\rho\\<br>    0&amp;0<br>    \end{bmatrix}<br>    \right\}$</p><h2 id="æŒ‡æ•°ä¸å¯¹æ•°æ˜ å°„"><a href="#æŒ‡æ•°ä¸å¯¹æ•°æ˜ å°„" class="headerlink" title="æŒ‡æ•°ä¸å¯¹æ•°æ˜ å°„"></a>æŒ‡æ•°ä¸å¯¹æ•°æ˜ å°„</h2><p>ä¹‹å‰å¼•å‡ºæä»£æ•°ä¸­æ¨å¯¼çš„$R(t) = exp(\phi_0t)$,Ræ˜¯æç¾¤SO(3)ï¼Œè€Œ$\phi$æ˜¯æä»£æ•°so(3).ä»–ä»¬ä¹‹é—´æ˜¯æœ‰ä¸€ä¸€å¯¹åº”çš„å…³ç³»çš„ã€‚æ©ï¼Œä»ä¸Šé¢çš„æ¨å¯¼ä¸­å¹¶ä¸èƒ½ä¿è¯Rå°±æ˜¯$\phi$çš„expæ˜ å°„ï¼Œä½†æ˜¯å®é™…ä¸Šå°±æ˜¯è¿™æ ·åšçš„ã€‚</p><p>ä½†æ˜¯å¦‚ä½•å¯¹ä¸€ä¸ª$\phi$è¿›è¡ŒæŒ‡æ•°æ˜ å°„ï¼Ÿæ˜ å°„è¿˜éœ€è½¬æ¢ä¸º$\Phi$.å¯¹äºä¸€ä¸ªçŸ©é˜µï¼Œæˆ‘ä»¬æ˜¯æ²¡æ³•è¿›è¡Œexpè¿ç®—çš„ã€‚å› æ­¤æˆ‘ä»¬ä½¿ç”¨æ³°å‹’å±•å¼€ï¼š</p><script type="math/tex; mode=display">exp(A) = \sum_{n=0}^\infty \frac{1}{n!}A^n</script><p>åŒæ ·ä¸Šè¿°å±•å¼€ä¹Ÿæ²¡é‚£ä¹ˆå®¹æ˜“è¿›è¡Œï¼Œå¥½åœ¨è¿˜æœ‰ä¸€äº›åˆ«çš„æ–¹æ³•ã€‚</p><p>ä»»ä½•ä¸€ä¸ªå‘é‡ï¼Œæˆ‘ä»¬éƒ½å¯ä»¥åˆ†è§£æˆæ–¹å‘å’Œé•¿åº¦ï¼Œä¹Ÿå°±æ˜¯$\theta \mathbf{a}$.å…¶ä¸­$\mathbf{a}$ä¸ºå•ä½å‘é‡ã€‚</p><p>ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥éªŒè¯çš„æ˜¯ï¼š</p><p>$\mathbf{a}^{\hat{}}\mathbf{a}^{\hat{}} = \mathbf{aa}^T -I$</p><p>$\mathbf{a}^{\hat{}}\mathbf{a}^{\hat{}}\mathbf{a}^{\hat{}} = -\mathbf{a}^{\hat{}}$</p><p>å› ä¸ºæˆ‘ä»¬$\Phi = \phi^{\hat{}}$,æ‰€ä»¥ä¸Šå¼æä¾›äº†å¾ˆå¥½çš„åŠæ³•æ¥è§£å†³expé—®é¢˜ã€‚ç»è¿‡æ¨å¯¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">\begin{aligned}exp(\phi^{\hat{}}) &= exp(\theta \mathbf{a}^T) = \sum_{n=0}^\infty \frac {1} {n!} (\theta \mathbf{a}^{\hat{}})^n\\&=\cos \theta I + (1 - \cos \theta)\mathbf{aa}^T + \sin \theta \mathbf{a}^{\hat{}}\end{aligned}</script><p>å¾ˆç¥å¥‡çš„äº‹æƒ…ï¼Œä¸Šé¢ç«Ÿç„¶å°±æ˜¯ç½—å¾·é‡Œå…‹æ–¯å…¬å¼ï¼ˆæ¨å¯¼è¿‡ç¨‹å®é™…ä¸Šä¸å¤æ‚ï¼Œçœç•¥æ‰äº†ï¼‰ã€‚</p><p>å¯¹åº”çš„è¿˜æœ‰ä¸€ä¸ªå¯¹æ•°æ˜ å°„ï¼š</p><p>$\phi$ = \ln(R)^{\hat{}} = \left (\sum_{n=0}^\infty \frac{(-1)^n}{n+1}(R-I)^{n+1} \right)$</p><p>å½“ç„¶ï¼Œæ—¢ç„¶æˆ‘ä»¬å·²ç»çŸ¥é“ç½—å¾·é‡Œå…‹æ–¯å…¬å¼äº†ï¼Œå°±ä¸ç”¨ä¸Šå¼è¿™ä¹ˆå¤æ‚çš„å¼å­å»æ±‚å¯¹æ•°æ˜ å°„äº†ã€‚</p><p>æ—‹è½¬è§’ï¼Œå’Œæ—‹è½¬çŸ©é˜µä¸æ˜¯ä¸€ä¸€å¯¹åº”çš„ï¼Œå› ä¸ºæ—‹è½¬çŸ©é˜µå”¯ä¸€çš„æƒ…å†µä¸‹ï¼Œæ—‹è½¬è§’å¯èƒ½æœ‰å¤šä¸ªï¼ˆè§’åº¦å¤šäº†$2\pi$ï¼‰ã€‚è¿™æ„å‘³ç€æŒ‡æ•°æ˜ å°„æ˜¯æ»¡å°„çš„ã€‚å¦‚æœè§’åº¦é™åˆ¶åˆ°$[-\pi,\pi ]$ï¼Œé‚£ä¹ˆæç¾¤å’Œæä»£æ•°å…ƒç´ å°±æ˜¯ä¸€ä¸€å¯¹æ˜ çš„äº†ã€‚</p><p>åŒæ ·ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å¾—åˆ°$SE(3)$ä¸$se(3)$çš„æŒ‡æ•°æ˜ å°„å’Œå¯¹æ•°æ˜ å°„:</p><script type="math/tex; mode=display">\begin{aligned}exp(\xi^{\hat{}}) &= \begin{bmatrix}\sum_{n=0}^\infty \frac{1}{n!}(\phi ^{\hat{}})^n& \sum_{n=0}^\infty \frac{1}{(n+1)!}(\phi^{\hat{}})^n \rho\\0&1\end{bmatrix}& \triangleq \begin{bmatrix}R&J\rho\\0&1\end{bmatrix} = T\end{aligned}</script><p>ä¸Šå¼ä¸­$J = \frac{\sin \theta}{\theta}I + \left(1 - \frac {\sin\theta}{\theta}\right)\mathbf{aa}^T + \frac{1 - \cos \theta}{\theta} \mathbf{a}^{\hat{}}$</p><p>å®ƒçš„å¯¹æ•°æ˜ å°„ï¼Œå³ä¸Šè§’ä¸ä¹‹å‰ä¸€æ ·ï¼Œè€Œè¿™é‡Œçš„$t = J\rho$,é€šè¿‡$\phi$å¯ä»¥è®¡ç®—å‡ºæ¥Jï¼Œä»è€Œè§£çº¿æ€§æ–¹ç¨‹å¾—åˆ°$\rho$.</p><p>Jä¸ºé›…å…‹æ¯”çŸ©é˜µã€‚</p><h2 id="æä»£æ•°çš„æ±‚å¯¼ä¸æ‰°åŠ¨æ¨¡å‹"><a href="#æä»£æ•°çš„æ±‚å¯¼ä¸æ‰°åŠ¨æ¨¡å‹" class="headerlink" title="æä»£æ•°çš„æ±‚å¯¼ä¸æ‰°åŠ¨æ¨¡å‹"></a>æä»£æ•°çš„æ±‚å¯¼ä¸æ‰°åŠ¨æ¨¡å‹</h2><p>ä¹‹æ‰€ä»¥è¦å­¦ä¹ æç¾¤å’Œæä»£æ•°ï¼Œæ˜¯å› ä¸ºæˆ‘ä»¬çš„SO(3)å’ŒSE(3)éƒ½æ˜¯æ²¡æœ‰å®šä¹‰åŠ æ³•çš„ã€‚æ‰€ä»¥æ— æ³•æ±‚å¯¼ï¼Œå¯¹äºä¼˜åŒ–éå¸¸éš¾åŠã€‚å› æ­¤æˆ‘ä»¬æƒ³åšçš„æ˜¯èƒ½å¦å°†æç¾¤å’Œæä»£æ•°ä¸Šå¯¹åº”å…³ç³»è¿ç”¨åˆ°æ±‚å¯¼ä¸­ï¼ŒæŠŠå¯¹æç¾¤çš„æ±‚å¯¼ï¼Œè½¬åŒ–ä¸ºå¯¹æä»£æ•°çš„æ±‚å¯¼ã€‚</p><p>æ‰€ä»¥æˆ‘ä»¬å¸Œæœ›çš„æ˜¯ï¼š$exp(\Phi_1 +\Phi_2) = exp(\Phi_1)exp(\Phi_2)$.</p><p>å¾ˆé—æ†¾çš„æ˜¯ï¼Œä¸Šé¢çš„å¼å­ä¸æˆç«‹ã€‚ï¼ˆå‘Šè¾ï¼‰</p><h3 id="BCHå…¬å¼"><a href="#BCHå…¬å¼" class="headerlink" title="BCHå…¬å¼"></a>BCHå…¬å¼</h3><p>æœ‰ä¸€ä¸ªBCHå…¬å¼ï¼Œå®ƒæ˜¯å¯¹äºçŸ©é˜µæ¥è¯´çš„ä»¥åŠæç¾¤æä»£æ•°ç›¸å…³çš„æŒ‡æ•°ç›¸ä¹˜å±•å¼€ï¼š</p><p>$<br>\ln(exp(A)exp(B)) \approx A+B+\frac 1 2 [A,B ]+\frac 1 {12} [A, [A ,B]]+â€¦<br>$</p><p>å¯ä»¥çœ‹åˆ°ï¼Œåœ¨æ•°å­¦ä¸Šï¼Œå®ƒä¼šæœ‰è¿™ä¹ˆå¤šçš„ä½™é¡¹å­˜åœ¨çš„ã€‚ä¸è¿‡æˆ‘ä»¬åšè¿‘ä¼¼çš„ä¼°è®¡ã€‚å¦‚æœ$\phi_1$æˆ–è€…$\phi_2$æœ‰ä¸€ä¸ªé‡æ˜¯å°é‡ï¼Œæˆ‘ä»¬å¯ä»¥å¿½ç•¥å…¶äºŒæ¬¡é¡¹ã€‚ç›´æ¥è€ƒè™‘åˆ°æä»£æ•°çš„è½¬æ¢ï¼š</p><p>$<br>\ln(exp(\phi_1^{\hat{}})exp(\phi_2^{\hat{}}))^{\hat{}} \approx<br>\left \{\begin{matrix}<br>J_l(\phi_2)^{-1}\phi_1 +\phi_2&amp;\phi_1ä¸ºå°é‡\\<br>J_r(\phi_1)^{-1}\phi_2 +\phi_1&amp;\phi_2ä¸ºå°é‡\\<br>\end{matrix}<br>\right .<br>$</p><p>æ—¢ç„¶å¯¼æ•°æ¨¡å‹å·¦æˆ–è€…å³åŠ ä¸Šä¸€ä¸ªå°é‡ï¼Œä¸Šå¼ä¸­ï¼Œç¬¬ä¸€ä¸ªå¯¹åº”çš„æ˜¯$\phi_2$ä¸ºRå¯¹åº”çš„æä»£æ•°ï¼Œè€Œ$\phi_1$ä¸ºå°é‡ï¼Œå¯¹åº”åˆ°æç¾¤ï¼Œå°±æ˜¯Rå·¦ä¹˜ä¸€ä¸ªå°é‡ï¼Œç¬¬äºŒä¸ªå¼å­å¯¹åº”çš„å°±æ˜¯å³ä¹˜ä¸€ä¸ªå°é‡äº†ã€‚æˆ‘ä»¬åé¢è§„å®šä½¿ç”¨ä¸ºå·¦ä¹˜ï¼Œå®é™…ä¸Šå³ä¹˜çš„Jå’Œå·¦ä¹˜ä¹Ÿç›¸å·®ä¸å¤šã€‚</p><script type="math/tex; mode=display">J_l = J = \frac {\sin\theta}{\theta} I + (1 - \frac {\sin \theta}{\theta})\mathbf{aa}^T + \frac{1 - \cos \theta}{\theta} \mathbf{a}^{\hat{}}</script><script type="math/tex; mode=display">J_l^{-1} = \frac{\theta}{2} \cot \frac{\theta}{2}I + (1 - \frac{\theta}{2}\cot\frac{\theta}{2})\mathbf{aa}^T - \frac{\theta}{2}\mathbf{a}^{\hat{}}</script><p>å³ä¹˜çš„é›…ç§‘æ¯”çŸ©é˜µï¼š$J_r (\phi) = J_l(-\phi)$.</p><p>ä¹Ÿå°±æ˜¯ï¼š$\exp(\Delta \phi ^{\hat{}})\exp(\phi^{\hat{}}) \approx \exp ((\phi + J_l^{-1}(\phi)\Delta \phi)^{\hat{}})$.</p><p>åŒç†ï¼Œå¦‚æœæˆ‘ä»¬åœ¨æä»£æ•°ä¸Šè¿›è¡ŒåŠ æ³•ï¼š</p><script type="math/tex; mode=display">\exp((\phi+\Delta \phi)^{\hat{}}) = \exp((J\Delta \phi)^{\hat{}})\exp(\phi^{\hat{}})</script><p>ä¸Šè¿°éƒ½æ˜¯å·¦ä¹˜æ¨¡å‹ã€‚</p><p>åŒæ ·çš„ï¼Œå¯¹äºSE(3)ä¹Ÿæœ‰ç±»ä¼¼çš„BCHè¿‘ä¼¼å…¬å¼ï¼š</p><script type="math/tex; mode=display">\exp(\Delta \xi ^{\hat{}}) \exp(\xi^{\hat{}}) \approx \exp((\mathcal{J}_l^{-1}\Delta \xi + \xi)^{\hat{}}),\\\exp(\xi^{\hat{}})\exp(\Delta \xi ^{\hat{}})  \approx \exp((\mathcal{J}_r^{-1}\Delta \xi + \xi)^{\hat{}}).</script><p>$\mathcal{J}_l$çš„å½¢å¼æ¯”è¾ƒå¤æ‚ï¼Œæˆ‘ä»¬ä¹Ÿç”¨ä¸ä¸Šï¼Œå°±ä¸æäº†ã€‚</p><h3 id="so-3-æ±‚å¯¼"><a href="#so-3-æ±‚å¯¼" class="headerlink" title="so(3)æ±‚å¯¼"></a>so(3)æ±‚å¯¼</h3><p>æˆ‘ä»¬æƒ³è¦å®ç°å¯¹æ—‹è½¬çŸ©é˜µçš„æ±‚å¯¼ï¼Œç”±äºæ—‹è½¬çŸ©é˜µæ˜¯ä¹˜æ³•å®šä¹‰çš„ï¼Œæ‰€ä»¥ç›´æ¥åœ¨SO(3)ä¸Šæ— æ³•å®šä¹‰å‡ºæ¥å¯¼æ•°ã€‚å› æ­¤è¦è½¬æ¢åˆ°å¯¹åº”çš„æä»£æ•°ä¸Šæ¥æ±‚å¾—å¯¼æ•°ã€‚</p><p>ä¸ºäº†å¾—åˆ°so(3)ä¸Šçš„å¯¼æ•°ï¼Œæœ‰ä¸¤ç§åŠæ³•ã€‚ç¬¬ä¸€ç§ï¼Œæ˜¯é€šè¿‡so(3)æ¥å®ç°ï¼Œå¦ä¸€ç§æ˜¯ç»™SO(3)å·¦ä¹˜ä¸€ä¸ªçŸ©é˜µæ¥å®ç°ã€‚</p><h4 id="å¯¼æ•°æ¨¡å‹"><a href="#å¯¼æ•°æ¨¡å‹" class="headerlink" title="å¯¼æ•°æ¨¡å‹"></a>å¯¼æ•°æ¨¡å‹</h4><p>æ ¹æ®å¯¼æ•°çš„å®šä¹‰ï¼š</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial (\exp(\phi^{\hat{}})p)} {\partial \phi}&= \lim_{\delta \phi \rightarrow 0} \frac{\exp((\phi + \delta \phi)^{\hat{}})p - exp(\phi^{\hat{}})p} {\delta \phi}\\&= \lim_{\delta \phi \rightarrow 0} \frac{exp((J_l\delta \phi)^{\hat{}})exp(\phi^{\hat{}})p - \exp(\phi^{\hat{}})p}{\delta \phi}\\&\approx \lim_{\delta \phi \rightarrow 0} \frac{(I+(J_l\delta \phi)^{\hat{}})exp(\phi^{\hat{}})p - \exp(\phi^{\hat{}})p}{\delta \phi}\\&= \lim_{\delta \phi \rightarrow 0} \frac{(J_l\delta\phi)^{\hat{}}\exp(\phi^{\hat{}})p}{\delta \phi}\\&= \lim_{\delta \phi \rightarrow 0} \frac{-(\exp(\phi^{\hat{}})p)^{\hat{}}J_l\delta\phi}{\delta \phi}\\&= -(Rp)^{\hat{}} J_l\end{aligned}</script><p>ä¸Šé¢çš„è¿‡ç¨‹ç”¨åˆ°äº†BCHå±•å¼€ï¼Œæ³°å‹’è¿‘ä¼¼ï¼Œä»¥åŠ$\hat{}$çš„æ€§è´¨ï¼š$a^{\hat{}}b = -b^{\hat{}}a $.<br>è¿™é‡Œæœ‰å½¢å¼æ¯”è¾ƒå¤æ‚çš„$J_l$,æˆ‘ä»¬ä¸æƒ³è®¡ç®—å®ƒã€‚æ‰€ä»¥çœ‹çœ‹æ‰°åŠ¨æ¨¡å‹ã€‚</p><h4 id="æ‰°åŠ¨æ¨¡å‹"><a href="#æ‰°åŠ¨æ¨¡å‹" class="headerlink" title="æ‰°åŠ¨æ¨¡å‹"></a>æ‰°åŠ¨æ¨¡å‹</h4><p>æ‰°åŠ¨æ¨¡å‹æ˜¯åœ¨æç¾¤ä¸Šå·¦ä¹˜ä¸€ä¸ªå¾ˆå°çš„çŸ©é˜µ$\Delta R$,å‡è®¾å®ƒå¯¹åº”çš„æä»£æ•°ä¸º$\theta$ã€‚é€šè¿‡æ‰°åŠ¨æ¨¡å‹å¾—åˆ°çš„å¯¼æ•°ä¸º$-(Rp)^{\hat{}}$.<br>\begin{aligned}<br>\frac{\partial (Rp)} {\partial \phi}&amp;= \lim_{\theta \rightarrow 0} \frac{\exp(\theta^{\hat{}})\exp(\phi)^{\hat{}})p - exp(\phi^{\hat{}})p} {\theta}\\<br>&amp;=-(Rp)^{\hat{}}<br>\end{aligned}<br>æ‰°åŠ¨æ¨¡å‹æ±‚å¯¼å’Œä¸Šé¢å¯¼æ•°æ¨¡å‹å¯ä»¥ä½¿ç”¨åŒæ ·çš„å¥—è·¯ï¼Œè€Œä¸”æ›´ç®€å•ã€‚å› æ­¤å°±ä¸è¯¦ç»†å†™å‡ºæ¥äº†ã€‚</p><p>å¯ä»¥çœ‹åˆ°æ‰°åŠ¨æ¨¡å‹çš„æ¯”å¯¼æ•°æ¨¡å‹å¾—åˆ°çš„ç»“æœæ›´åŠ ç®€ä¾¿ä¸€äº›ã€‚å› æ­¤æ‰°åŠ¨æ¨¡å‹ç›¸å¯¹äºå¯¼æ•°æ¨¡å‹æ¥è¯´æ›´åŠ çš„å®ç”¨ã€‚</p><h3 id="se-3-æ±‚å¯¼"><a href="#se-3-æ±‚å¯¼" class="headerlink" title="se(3)æ±‚å¯¼"></a>se(3)æ±‚å¯¼</h3><p>å¯¹äºse(3)çš„æ±‚å¯¼ï¼Œæˆ‘ä»¬ç›´æ¥ç»™å‡ºæ‰°åŠ¨æ¨¡å‹ï¼š</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial (Tp)}{\partial \delta \xi} &= \lim_{\delta \xi \rightarrow 0} \frac{\begin{bmatrix}\delta \phi^{\hat{}}(Rp+t) + \delta p\\0\end{bmatrix}}{\delta \xi}\\&= \begin{bmatrix}I&-(Rp+t)^{\hat{}}\\0&0\end{bmatrix}\\&\triangleq (Tp)^{\bigodot}\end{aligned}</script><p>å¦‚æœæ²¡æœ‰æç¾¤å’Œæä»£æ•°çš„æå‡ºï¼Œæ±‚å¯¼å°±æ²¡æœ‰ç†è®ºä¾æ®äº†ã€‚è€Œå› ä¸ºæœ‰äº†æç¾¤å’Œæä»£æ•°è¿™ç§æ˜ å°„å…³ç³»ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å°†æç¾¤ç”¨æä»£æ•°æ¥è¡¨ç¤ºï¼Œè€Œæä»£æ•°æ˜¯å¯ä»¥è¿›è¡Œæ±‚å¯¼çš„ã€‚ä»è€Œå®ç°å¯¹æ—‹è½¬çŸ©é˜µçš„æ±‚å¯¼ã€‚</p><h2 id="Sophusåº“"><a href="#Sophusåº“" class="headerlink" title="Sophusåº“"></a>Sophusåº“</h2><p>æç¾¤æä»£æ•°å¬å¾—äººå¤´å¤§ã€‚ä¸è¿‡ï¼Œæˆ‘ä»¬ä¸ç”¨è‡ªå·±å»å®Œæˆè¿™äº›ä¸œè¥¿ã€‚å¯¹äºæç¾¤æä»£æ•°æ”¯æŒæ¯”è¾ƒå¥½çš„åº“æ˜¯sophusï¼Œæˆ‘ä»¬å°±ä½¿ç”¨è¿™ä¸ªåº“æ¥å®ç°slamä¸­æç¾¤æä»£æ•°éœ€è¦çš„åº”ç”¨ã€‚</p><p>sophusæ˜¯eigençš„ä¸€ä¸ªæ‰©å±•ï¼Œå®ƒåœ¨eigençš„åŸºç¡€ä¸Šå®ç°äº†ä¸€äº›æç¾¤æä»£æ•°çš„æ“ä½œï¼Œæ²¡æœ‰ä»»ä½•åˆ«çš„ä¾èµ–é¡¹ã€‚</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>; </span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;Eigen/Core&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;Eigen/Geometry&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// æç¾¤æä»£æ•° åº“ </span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"sophus/so3.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"sophus/se3.h"</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">( <span class="keyword">int</span> argc, <span class="keyword">char</span>** argv )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"> <span class="comment">/*******************************************/</span></span><br><span class="line">ã€€</span><br><span class="line">    Eigen::Matrix3d R = Eigen::AngleAxisd(M_PI/<span class="number">2</span>, Eigen::Vector3d(<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>)).toRotationMatrix();</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"RotationMatrix R: \n"</span>&lt;&lt;R&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/***æç¾¤*****/</span></span><br><span class="line">    Sophus::<span class="function">SO3 <span class="title">SO3_R</span><span class="params">(R)</span></span>;               <span class="comment">// Sophus::SO(3)å¯ä»¥ç›´æ¥ä»æ—‹è½¬çŸ©é˜µæ„é€ </span></span><br><span class="line">    Sophus::<span class="function">SO3 <span class="title">SO3_v</span><span class="params">( <span class="number">0</span>, <span class="number">0</span>, M_PI/<span class="number">2</span> )</span></span>;  <span class="comment">// äº¦å¯ä»æ—‹è½¬å‘é‡æ„é€   è¿™é‡Œæ³¨æ„ï¼Œä¸æ˜¯æ—‹è½¬å‘é‡çš„ä¸‰ä¸ªåæ ‡å€¼ï¼Œæœ‰ç‚¹åƒæ¬§æ‹‰è§’æ„é€ ã€‚</span></span><br><span class="line">    Eigen::<span class="function">Quaterniond <span class="title">q</span><span class="params">(R)</span></span>;            <span class="comment">// æˆ–è€…å››å…ƒæ•°(ä»æ—‹è½¬çŸ©é˜µæ„é€ )</span></span><br><span class="line">    Sophus::<span class="function">SO3 <span class="title">SO3_q</span><span class="params">( q )</span></span>;</span><br><span class="line">    <span class="comment">// ä¸Šè¿°è¡¨è¾¾æ–¹å¼éƒ½æ˜¯ç­‰ä»·çš„</span></span><br><span class="line">    <span class="comment">// è¾“å‡ºSO(3)æ—¶ï¼Œä»¥so(3)å½¢å¼è¾“å‡º</span></span><br><span class="line">    <span class="comment">//ä»è¾“å‡ºçš„å½¢å¼å¯ä»¥çœ‹å‡ºï¼Œè™½ç„¶SO3æ˜¯æç¾¤ï¼Œæ˜¯æ—‹è½¬çŸ©é˜µï¼Œä½†æ˜¯è¾“å‡ºå½¢å¼è¿˜æ˜¯å‘é‡ï¼ˆè¢«è½¬åŒ–æˆæä»£æ•°è¾“å‡ºï¼‰ã€‚</span></span><br><span class="line">    <span class="comment">// é‡è½½äº† &lt;&lt; è¿ç®—ç¬¦  out_str &lt;&lt; so3.log().transpose() &lt;&lt; std::endl;  </span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"SO(3) from matrix: "</span>&lt;&lt;SO3_R&lt;&lt;<span class="built_in">endl</span>;  <span class="comment">//SO(3) from matrix:      0      0 1.5708  </span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"SO(3) from vector: "</span>&lt;&lt;SO3_v&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"SO(3) from quaternion :"</span>&lt;&lt;SO3_q&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/****æä»£æ•°*****/</span></span><br><span class="line">    <span class="comment">// ä½¿ç”¨å¯¹æ•°æ˜ å°„è·å¾—å®ƒçš„æä»£æ•°</span></span><br><span class="line">    <span class="comment">// æ‰€ä»¥ï¼Œæä»£æ•° so3çš„æœ¬è´¨å°±æ˜¯ä¸ªä¸‰ç»´å‘é‡ï¼Œç›´æ¥Eigen::Vector3då®šä¹‰ã€‚</span></span><br><span class="line">    Eigen::Vector3d so3 = SO3_R.<span class="built_in">log</span>();</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"so3 = "</span>&lt;&lt;so3.transpose()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="comment">// hat ä¸ºå‘é‡ åˆ°åå¯¹ç§°çŸ©é˜µ  ç›¸å½“äºã€€^ã€€è¿ç®—ã€‚</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"so3 hat=\n"</span>&lt;&lt;Sophus::SO3::hat(so3)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="comment">// ç›¸å¯¹çš„ï¼Œveeä¸º åå¯¹ç§°çŸ©é˜µ åˆ° å‘é‡  ç›¸å½“äºä¸‹å°–å°–è¿ç®— </span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"so3 hat vee= "</span>&lt;&lt;Sophus::SO3::vee( Sophus::SO3::hat(so3) ).transpose()&lt;&lt;<span class="built_in">endl</span>; <span class="comment">// transposeçº¯ç²¹æ˜¯ä¸ºäº†è¾“å‡ºç¾è§‚ä¸€äº›</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">/****æä»£æ•°æ±‚å¯¼ã€€æ›´æ–°*****/</span></span><br><span class="line">    <span class="comment">// å¢é‡æ‰°åŠ¨æ¨¡å‹çš„æ›´æ–°</span></span><br><span class="line">    Eigen::<span class="function">Vector3d <span class="title">update_so3</span><span class="params">(<span class="number">1e-4</span>, <span class="number">0</span>, <span class="number">0</span>)</span></span>; <span class="comment">//å‡è®¾æ›´æ–°é‡ä¸ºè¿™ä¹ˆå¤š</span></span><br><span class="line">    Sophus::SO3 SO3_updated = Sophus::SO3::<span class="built_in">exp</span>(update_so3)*SO3_R;<span class="comment">// å¢é‡æŒ‡æ•°æ˜ å°„Ã—åŸæä»£æ•°</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"SO3 updated = "</span>&lt;&lt;SO3_updated&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">/********************èŒèŒçš„åˆ†å‰²çº¿*****************************/</span></span><br><span class="line">    <span class="comment">/************ç‰¹æ®Šæ¬§å¼ç¾¤ã€€å˜æ¢çŸ©é˜µç¾¤ã€€æœ‰æ—‹è½¬æœ‰å¹³ç§»*********************/</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"************æˆ‘æ˜¯åˆ†å‰²çº¿*************"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="comment">// æç¾¤ å¯¹SE(3)æ“ä½œå¤§åŒå°å¼‚</span></span><br><span class="line">    Eigen::<span class="function">Vector3d <span class="title">t</span><span class="params">(<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>)</span></span>;           <span class="comment">// æ²¿Xè½´å¹³ç§»1</span></span><br><span class="line">    Sophus::<span class="function">SE3 <span class="title">SE3_Rt</span><span class="params">(R, t)</span></span>;           <span class="comment">// ä»R,tæ„é€ SE(3)</span></span><br><span class="line">    Sophus::<span class="function">SE3 <span class="title">SE3_qt</span><span class="params">(q,t)</span></span>;            <span class="comment">// ä»q,tæ„é€ SE(3)</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"SE3 from R,t= "</span>&lt;&lt;<span class="built_in">endl</span>&lt;&lt;SE3_Rt&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"SE3 from q,t= "</span>&lt;&lt;<span class="built_in">endl</span>&lt;&lt;SE3_qt&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="comment">// æä»£æ•°se(3) æ˜¯ä¸€ä¸ªå…­ç»´å‘é‡ï¼Œæ–¹ä¾¿èµ·è§å…ˆtypedefä¸€ä¸‹</span></span><br><span class="line">    <span class="keyword">typedef</span> Eigen::Matrix&lt;<span class="keyword">double</span>,<span class="number">6</span>,<span class="number">1</span>&gt; Vector6d;<span class="comment">// Vector6dæŒ‡ä»£ã€€Eigen::Matrix&lt;double,6,1&gt;</span></span><br><span class="line">    Vector6d se3 = SE3_Rt.<span class="built_in">log</span>();</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"se3 = "</span>&lt;&lt;se3.transpose()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="comment">// è§‚å¯Ÿè¾“å‡ºï¼Œä¼šå‘ç°åœ¨Sophusä¸­ï¼Œse(3)çš„å¹³ç§»åœ¨å‰ï¼Œæ—‹è½¬åœ¨å.</span></span><br><span class="line">    <span class="comment">// åŒæ ·çš„ï¼Œæœ‰hatå’Œveeä¸¤ä¸ªç®—ç¬¦</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"se3 hat = "</span>&lt;&lt;<span class="built_in">endl</span>&lt;&lt;Sophus::SE3::hat(se3)&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"se3 hat vee = "</span>&lt;&lt;Sophus::SE3::vee( Sophus::SE3::hat(se3) ).transpose()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// æœ€åï¼Œæ¼”ç¤ºä¸€ä¸‹æ›´æ–°</span></span><br><span class="line">    Vector6d update_se3; <span class="comment">//æ›´æ–°é‡</span></span><br><span class="line">    update_se3.setZero();</span><br><span class="line">    update_se3(<span class="number">0</span>,<span class="number">0</span>) = <span class="number">1e-4</span>d;</span><br><span class="line">    Sophus::SE3 SE3_updated = Sophus::SE3::<span class="built_in">exp</span>(update_se3)*SE3_Rt;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">"SE3 updated = "</span>&lt;&lt;<span class="built_in">endl</span>&lt;&lt;SE3_updated.matrix()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mathematics </tag>
            
            <tag> SLAM </tag>
            
            <tag> sophus </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SLAMâ€”â€”åˆšä½“è¿åŠ¨ä»¥åŠEigen</title>
      <link href="/2018/11/07/SLAM%E2%80%94%E2%80%94%E5%88%9A%E4%BD%93%E8%BF%90%E5%8A%A8%E4%BB%A5%E5%8F%8AEigen/"/>
      <url>/2018/11/07/SLAM%E2%80%94%E2%80%94%E5%88%9A%E4%BD%93%E8%BF%90%E5%8A%A8%E4%BB%A5%E5%8F%8AEigen/</url>
      
        <content type="html"><![CDATA[<p>è¿™æ¬¡ä¸»è¦ä»‹ç»ä¸€äº›åˆšä½“è¿åŠ¨ä¸­éœ€è¦çš„æ•°å­¦çŸ¥è¯†ä»¥åŠEigenåº“çš„åŸºæœ¬ä½¿ç”¨ã€‚<br><a id="more"></a></p><p>ä»€ä¹ˆæ˜¯åˆšä½“è¿åŠ¨ï¼Ÿè¿åŠ¨è¿‡ç¨‹ä¸­ä¸ä¼šå‘ç”Ÿå½¢å˜ã€‚æ‰€ä»¥å®é™…ä¸Šåˆšä½“çš„è¿åŠ¨ä¹Ÿå°±åªæœ‰ä¸¤ç§ï¼šæ—‹è½¬å’Œå¹³ç§»ã€‚</p><p>å¯¹äºä¸‰ç»´ç©ºé—´çš„æ—‹è½¬å¹³ç§»è¡¨ç¤ºï¼Œä¹‹å‰çš„è®¡ç®—æœºå›¾å½¢å­¦ä¸­ä»‹ç»çš„æ¯”è¾ƒæ¸…æ¥šäº†ã€‚ä¸è¿‡è¿˜æœ‰ä¸€äº›åˆ«çš„æ¦‚å¿µæ²¡æœ‰æ¥è§¦ï¼Œåœ¨è¿™é‡Œåšä¸ªè¡¥å……ã€‚</p><h2 id="æ—‹è½¬å‘é‡"><a href="#æ—‹è½¬å‘é‡" class="headerlink" title="æ—‹è½¬å‘é‡"></a>æ—‹è½¬å‘é‡</h2><p>ä¹‹å‰çš„æ—‹è½¬ä½¿ç”¨çš„æ˜¯æ—‹è½¬çŸ©é˜µï¼Œæ—‹è½¬çŸ©é˜µæ˜¯å•ä½æ­£äº¤çŸ©é˜µï¼ˆè¡Œåˆ—å¼ä¸º1ï¼‰ã€‚å®é™…ä¸Šä¼šæœ‰ä¸ªé—®é¢˜ï¼šæ—‹è½¬æ“ä½œåªéœ€è¦3ä¸ªè‡ªç”±åº¦ï¼Œä½†æ˜¯å´ç”¨äº†9ä¸ªé‡æ¥è¡¨ç¤ºã€‚è¿™è¯´æ˜ä½¿ç”¨æ—‹è½¬çŸ©é˜µæ˜¯æœ‰å†—ä½™çš„ã€‚å› æ­¤è¿™é‡Œä»‹ç»æ—‹è½¬å‘é‡ã€‚</p><p>æˆ‘ä»¬çŸ¥é“ä»»ä½•æ—‹è½¬éƒ½å¯ä»¥ä½¿ç”¨ä¸€ä¸ªæ—‹è½¬è½´å’Œæ—‹è½¬è§’æ¥æè¿°ã€‚æ—‹è½¬å‘é‡æ˜¯ä¸€ä¸ªéå¸¸èªæ˜çš„åšæ³•ã€‚å®ƒçš„æ–¹å‘ä»£è¡¨äº†æ—‹è½¬è½´ï¼Œè€Œå®ƒçš„é•¿åº¦ä»£è¡¨äº†æ—‹è½¬çš„è§’åº¦ã€‚</p><p>é‚£ä¹ˆæ—‹è½¬å‘é‡ä¸æ—‹è½¬çŸ©é˜µå¦‚ä½•è½¬æ¢å‘¢ï¼Ÿå¦‚æœæ—‹è½¬è½´ï¼ˆå•ä½å‘é‡ï¼‰ä¸º$\mathbf{n}$ï¼Œæ—‹è½¬è§’åº¦ä¸º$\theta$ï¼Œé‚£ä¹ˆæ—‹è½¬å‘é‡ä¸º$\theta\mathbf{n}$ï¼Œæ—‹è½¬çŸ©é˜µå¯ä»¥ä½¿ç”¨Rodrigueså…¬å¼æ¥è®¡ç®—å‡ºæ¥ï¼š</p><script type="math/tex; mode=display">R = \cos \theta I + (1-\cos \theta)\mathbf{nn}^T + \sin \theta \mathbf{n}^\^</script><p>$\mathbf{n}^\^$ä¸ºå‘é‡$\mathbf{n}$çš„å¯¹å¶çŸ©é˜µã€‚è¿™ä¸ªå…¬å¼ä¹Ÿåœ¨ä¹‹å‰çš„å›¾å½¢å­¦åšå®¢ä¸­åšè¿‡ä»‹ç»äº†ã€‚</p><p>å¦‚ä½•ä»æ—‹è½¬çŸ©é˜µå¾—åˆ°æ—‹è½¬å‘é‡ï¼Ÿ</p><p>é¦–å…ˆï¼Œå¯¹äºæ—‹è½¬è§’ï¼š</p><script type="math/tex; mode=display">\begin{aligned}tr(R) &= \cos\theta tr(I) +(1 - \cos \theta) tr(\mathbf{nn}^T) + \sin \theta tr (\mathbf{n}^\^)\\&= 3\cos \theta + (1-\cos \theta)\\&= 1+ 2\cos\theta\end{aligned}</script><p>æ‰€ä»¥æ ¹æ®ä¸Šå¼å¯ä»¥å¾ˆç®€å•æ±‚å¾—ï¼š</p><script type="math/tex; mode=display">\theta = \arccos\left( \frac{tr(R)-1}{2}\right)</script><p>è‡³äº$\mathbf{n}$ï¼Œç”±äºæ—‹è½¬è½´çš„å‘é‡æ—‹è½¬åä¸ä¼šå‘ç”Ÿå˜åŒ–ï¼Œå› æ­¤$R\mathbf{n} = \mathbf{n}$.</p><p>æ‰€ä»¥$\mathbf{n}$ä¸ºçŸ©é˜µRç‰¹å¾å€¼ä¸º1çš„å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚</p><h2 id="æ¬§æ‹‰è§’"><a href="#æ¬§æ‹‰è§’" class="headerlink" title="æ¬§æ‹‰è§’"></a>æ¬§æ‹‰è§’</h2><p>æ—‹è½¬å‘é‡ä»¥åŠæ—‹è½¬çŸ©é˜µå¯¹äºäººçš„è§’åº¦æ¥è¯´éƒ½ä¸å¤Ÿç›´è§‚ã€‚å› æ­¤æœ‰äº†æ¬§æ‹‰è§’çš„è¯ç”Ÿã€‚å…¶å®æ¬§æ‹‰è§’å°±æ˜¯å°†ä¸€ä¸ªæ—‹è½¬è½¬åŒ–ä¸º3ä¸ªç»•åæ ‡è½´çš„è½¬åŠ¨ã€‚å› æ­¤è¿™ä¸ªè½¬åŠ¨çš„é¡ºåºå°±ä¸å”¯ä¸€äº†ã€‚åœ¨èˆªç©ºé‡Œå¯èƒ½ç»å¸¸å¬åˆ°â€œåèˆª-ä¿¯ä»°-æ»šè½¬â€ï¼ˆyam-pitch-rollï¼‰ï¼Œå®é™…ä¸Šå°±æ˜¯æ¬§æ‹‰è§’çš„ä¸€ç§ï¼Œç­‰ä»·ä¸ZYXè½´çš„æ—‹è½¬ã€‚</p><p>å› æ­¤æ¬§æ‹‰è§’å°±æ˜¯ç”¨ä¸€ä¸ªå‘é‡$[ r,p,y ]^T$ï¼ˆ3ä¸ªè§’åº¦ï¼‰æ¥è¡¨ç¤ºä¸€ä¸ªæ—‹è½¬ã€‚ä½†æ˜¯æ¬§æ‹‰è§’æœ‰ä¸ªè‘—åçš„ä¸‡èƒ½é”é—®é¢˜ã€‚å¦‚æœpitchè½¬åŠ¨äº†$\pm90Â°$,åˆ™æœ€åä¸€ä¸ªè½¬åŠ¨ç»•çš„è½´å®é™…ä¸Šå’Œzè½´ä¸€æ ·ï¼Œè¿™å°±ä½¿å¾—æŸå¤±äº†ä¸€ä¸ªè‡ªç”±åº¦ï¼Œè¿™è¢«ç§°ä¸ºå¥‡å¼‚æ€§é—®é¢˜ï¼ˆï¼Ÿï¼‰ã€‚å› æ­¤æ¬§æ‹‰è§’ä¸é€‚ç”¨ä¸æ’å€¼å’Œè¿­ä»£ï¼Œå¾€å¾€ç”¨äºäººå·¥äº¤äº’ã€‚</p><h2 id="å››å…ƒæ•°"><a href="#å››å…ƒæ•°" class="headerlink" title="å››å…ƒæ•°"></a>å››å…ƒæ•°</h2><p>æ—‹è½¬çŸ©é˜µç”¨9ä¸ªé‡æè¿°ï¼Œæœ‰å†—ä½™ï¼Œè€Œæ—‹è½¬å‘é‡å’Œæ¬§æ‹‰è§’å´å…·æœ‰å¥‡å¼‚æ€§ã€‚å®é™…ä¸Šæˆ‘ä»¬æ‰¾ä¸åˆ°ä¸å¸¦å¥‡å¼‚æ€§çš„ä¸‰ç»´å‘é‡æè¿°æ–¹å¼ã€‚<br>å› æ­¤åœ¨è¿™é‡Œå†ä»‹ç»ä¸€ä¸ªå››å…ƒæ•°ã€‚å®ƒç”¨å››ä¸ªé‡æ¥æè¿°æ—‹è½¬è¶³å¤Ÿç´§å‡‘åŒæ—¶ä¹Ÿæ²¡æœ‰å¥‡å¼‚æ€§ã€‚</p><p>ä¸€ä¸ªå››å…ƒæ•°$\mathbf{q} = q_0+q_1i+q_2j+q_3k$,æœ‰ä¸€ä¸ªå®éƒ¨ï¼Œ3ä¸ªè™šéƒ¨ã€‚å…¶ä¸­è™šéƒ¨æ»¡è¶³ä¸‹é¢çš„ä¸€ç»„å¼å­ï¼š</p><script type="math/tex; mode=display">\left\{\begin{aligned}i^2 = j^2 = k^2 = -1\\ij=k,ji = -k\\jk=i,kj = -i\\ki=j,ik = -j\end{aligned}\right.</script><p>æœ‰æ—¶å€™ä¹Ÿä½¿ç”¨ä¸€ä¸ªæ ‡é‡å’Œä¸€ä¸ªå‘é‡æ¥è¡¨ç¤ºå››å…ƒæ•°ï¼š</p><p>$\mathbf{q} = [ s,\mathbf{v} ],s = q_0 \in \mathbb{R},\mathbf{v} = [ q_1,q_2,q_3 ]^T \in \mathbb{R}^3 $.</p><p>å¦‚æœå®éƒ¨ä¸º0,ç§°ä¸ºè™šå››å…ƒæ•°ï¼Œå¦‚æœè™šéƒ¨ä¸º0ç§°ä¸ºå®å››å…ƒæ•°ã€‚</p><p>æˆ‘ä»¬èƒ½ç”¨å•ä½å››å…ƒæ•°è¡¨ç¤ºä¸‰ç»´ç©ºé—´ä¸­çš„ä»»æ„æ—‹è½¬ã€‚ä¸è¿‡ç”±äºå¤æ•°çš„å¼•å…¥ï¼Œå®ƒçš„è¡¨ç¤ºæ˜¯æœ‰ç‚¹åç›´è§‰çš„ã€‚å‡å¦‚æ—‹è½¬å‘é‡ä¸º$\mathbf{n} \theta$ï¼Œåˆ™å¯¹åº”çš„å››å…ƒæ•°ä¸ºï¼š</p><script type="math/tex; mode=display">\mathbf{q} = [ \cos \frac \theta 2, n_x\sin \frac \theta 2 ,n_y \sin \frac \theta 2,n_z,\frac \theta 2]^T.</script><p>å› æ­¤ä»å•ä½å››å…ƒæ•°ä¸­ä¹Ÿå¾ˆå®¹æ˜“å¾—åˆ°å¯¹åº”çš„æ—‹è½¬è½´å’Œæ—‹è½¬è§’åº¦ï¼š</p><script type="math/tex; mode=display">\left \{\begin{aligned}\theta = 2 \arccos q_0;[ n_x,n_y,n_z] = [ q_1, q_2,q_3]^T / \sin \frac \theta 2\end{aligned}\right .</script><p>è¿™ä¸ªå¼å­ç¡®å®åç›´è§‰ï¼Œç»™æˆ‘ä»¬èµšäº†ä¸€èˆ¬çš„æ„Ÿè§‰ã€‚å¦‚æœå¯¹$\theta $åŠ ä¸Š$2\pi$ï¼Œç›¸å½“äºæ²¡æœ‰è½¬åŠ¨ï¼Œä½†æ˜¯ä»–ä»¬å¯¹åº”çš„å››å…ƒæ•°å´å˜æˆåŸæ¥çš„ç›¸åæ•°äº†ã€‚å› æ­¤ï¼Œäº’ä¸ºå››å…ƒæ•°çš„ç›¸åæ•°è¡¨ç¤ºåŒä¸€ä¸ªæ—‹è½¬ã€‚</p><p>å››å…ƒæ•°çš„æ¦‚å¿µçš„è¯ç°åœ¨è¿˜å¾ˆæ‡µæ¯”ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆè¦è¿™æ ·æ¥è¡¨ç¤ºä¸€ä¸ªæ—‹è½¬ã€‚ä½†æ˜¯èƒ½å‘å±•åˆ°ç°åœ¨è¿˜ç•™ä¸‹æ¥çš„ä¸€å®šæ˜¯æœ‰è‡ªå·±çš„é“ç†ã€‚å› ä¸ºå››å…ƒæ•°æ¯ä¸ªå€¼éƒ½æ˜¯ç»è¿‡å¤„ç†çš„è½´å’Œè§’ç»“åˆï¼Œå› æ­¤å®ƒæ–¹ä¾¿è¿›è¡Œæ’å€¼ã€‚å¬è¯´å››å…ƒæ•°åœ¨æ¸¸æˆå¼€å‘é‡Œåº”ç”¨å¹¿æ³›ã€‚å…·ä½“è¿™äº›ä»¥åå†å»å¼„æ˜ç™½ï¼Œå…ˆçœ‹ä¸€çœ‹å››å…ƒæ•°çš„åŸºæœ¬è¿ç®—å§ã€‚</p><h3 id="åŠ å‡"><a href="#åŠ å‡" class="headerlink" title="åŠ å‡"></a>åŠ å‡</h3><p>$\mathbf{q}_a \pm \mathbf{q}_b = [ s_a \pm s_b ,\mathbf{v}_a \pm \mathbf{v}_b] $</p><h3 id="ä¹˜æ³•"><a href="#ä¹˜æ³•" class="headerlink" title="ä¹˜æ³•"></a>ä¹˜æ³•</h3><p>ä¹˜æ³•å°±æ˜¯å„é¡¹è½®ç€ç›¸ç§°ï¼Œæœ€åç›¸åŠ ï¼Œè™šéƒ¨è¦æŒ‰ç…§è™šéƒ¨çš„è§„åˆ™æ¥ä¹˜ï¼Œæœ€åå¾—åˆ°ç»“æœï¼š</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{q}_a \mathbf{q}_b &= s_as_b - x_ax_b - y_ay_b - z_az_b\\&+(s_ax_b+x_as_b+y_az_b-z_ay_b)i\\&+(s_ay_b - x_az_b + y_as_b + z_ax_b)j\\&+(s_az_b+x_ay_b-y_ax_b+z_as_b)\end{aligned}</script><p>å¦‚æœå†™æˆå‘é‡å½¢å¼ï¼š</p><script type="math/tex; mode=display">\mathbf{q}_a \mathbf{q}_b  = [ s_as_b - \mathbf{v}_a^T\mathbf{v}_b, s_a\mathbf{v}_b + s_b\mathbf{v}_a + \mathbf{v}_a\times \mathbf{v}_b]</script><p>ç”±äºæœ€åä¸€é¡¹å­˜åœ¨ï¼Œå››å…ƒæ•°çš„ä¹˜æ³•é€šå¸¸æ˜¯ä¸å¯äº¤æ¢çš„ã€‚</p><h3 id="å…±è½­"><a href="#å…±è½­" class="headerlink" title="å…±è½­"></a>å…±è½­</h3><p>$\mathbf{q}_a^* = s_a - x_ai - y_aj-z_ak = [ s_a,-\mathbf{v}_a]$</p><p>$\mathbf{q^ <em>q} = \mathbf{qq^ </em>} = [s_a^2 + \mathbf{vv}^T ,0]$ </p><p>å¯ä»¥çœ‹åˆ°å››å…ƒæ•°å’Œå…±è½­ç›¸ç§°å¾—åˆ°ä¸€ä¸ªå®æ•°ã€‚</p><h3 id="æ¨¡é•¿"><a href="#æ¨¡é•¿" class="headerlink" title="æ¨¡é•¿"></a>æ¨¡é•¿</h3><script type="math/tex; mode=display">\Vert \mathbf{q}\Vert = \sqrt{s^2 +x^2+y^2+z^2}</script><p>å¯ä»¥éªŒè¯ï¼š$\Vert \mathbf{q}_a\mathbf{q}_b \Vert = \Vert  \mathbf{q}_a\Vert \Vert  \mathbf{q}_b\Vert$.</p><p>è¿™ä¿è¯äº†å•ä½å››å…ƒæ•°çš„ä¹˜ç§¯ä¾ç„¶ä¸ºå•ä½å››å…ƒæ•°ã€‚</p><h3 id="é€†"><a href="#é€†" class="headerlink" title="é€†"></a>é€†</h3><p>$\mathbf{q}^{-1} = \frac {\mathbf{q}^*}{\Vert \mathbf{q}\Vert^2}$</p><p>$\mathbf{q}^{-1}\mathbf{q} = \mathbf{qq}^{-1} = 1$</p><p>åŒæ—¶å¯ä»¥çŸ¥é“ï¼Œå•ä½å››å…ƒæ•°çš„é€†å°±æ˜¯å•ä½å››å…ƒæ•°çš„å…±è½­ï¼Œå› ä¸º$\mathbf{qq}^* = 1$.</p><p>ä¹˜ç§¯çš„é€†å’ŒçŸ©é˜µçš„é€†æœ‰åŒæ ·çš„æ€§è´¨ï¼š$(\mathbf{q_a}\mathbf{q_b})^{-1} = \mathbf{q_b}^{-1}\mathbf{q_a}^{-1}$</p><h3 id="æ•°ä¹˜å’Œç‚¹ä¹˜"><a href="#æ•°ä¹˜å’Œç‚¹ä¹˜" class="headerlink" title="æ•°ä¹˜å’Œç‚¹ä¹˜"></a>æ•°ä¹˜å’Œç‚¹ä¹˜</h3><p>$k\mathbf{q} = [ks,k\mathbf{v} ]$</p><p>$\mathbf{q_a} \cdot \mathbf{q_b} = s_as_b + x_ax_b + y_ay_b + z_az_b$</p><h3 id="ç”¨å››å…ƒæ•°è¡¨ç¤ºæ—‹è½¬"><a href="#ç”¨å››å…ƒæ•°è¡¨ç¤ºæ—‹è½¬" class="headerlink" title="ç”¨å››å…ƒæ•°è¡¨ç¤ºæ—‹è½¬"></a>ç”¨å››å…ƒæ•°è¡¨ç¤ºæ—‹è½¬</h3><p>å¯¹äºä¸€ä¸ªä¸‰ç»´ç‚¹$p=[x,y,z ]$,ç»•ç€è½¬è½´$\mathbf{n}\theta$æ—‹è½¬ï¼Œå˜ä¸º$pâ€™$.æˆ‘ä»¬çŸ¥é“ä½¿ç”¨çŸ©é˜µçš„è¯ï¼Œå¯ä»¥è¿™æ ·æè¿°ï¼š$pâ€™ = Rp$.ä½†æ˜¯ä½¿ç”¨å››å…ƒæ•°å¦‚ä½•æè¿°å‘¢ï¼Ÿ</p><p>æˆ‘ä»¬æŠŠç©ºé—´çš„ç‚¹ç”¨è™šå››å…ƒæ•°æ¥æè¿°ï¼Œåˆ™$\mathbf{p} = [0,x,y,z ]$.</p><p>ç”¨$\mathbf{q}$æ¥è¡¨ç¤ºæ—‹è½¬: $\mathbf{q} = [\cos \frac \theta 2,\mathbf{n} \sin \theta 2]$.</p><p>åˆ™ï¼š$\mathbf{p}â€™ = \mathbf{qpq}^{-1}$.</p><p>å¯ä»¥éªŒè¯çš„æ˜¯ç»è¿‡è®¡ç®—çš„å®éƒ¨ä¸º0,è™šéƒ¨å¯¹åº”çš„å°±æ˜¯$qâ€™$çš„åæ ‡ç‚¹ã€‚</p><p>å¯ä»¥çœ‹åˆ°ä½¿ç”¨å››å…ƒæ•°æ¥æ—‹è½¬çš„è¯ä¹Ÿæ˜¯éå¸¸æ–¹ä¾¿çš„ã€‚</p><h3 id="å››å…ƒæ•°å’Œæ—‹è½¬çŸ©é˜µçš„è½¬æ¢"><a href="#å››å…ƒæ•°å’Œæ—‹è½¬çŸ©é˜µçš„è½¬æ¢" class="headerlink" title="å››å…ƒæ•°å’Œæ—‹è½¬çŸ©é˜µçš„è½¬æ¢"></a>å››å…ƒæ•°å’Œæ—‹è½¬çŸ©é˜µçš„è½¬æ¢</h3><p>å››å…ƒæ•°ä¸æ—‹è½¬çŸ©é˜µçš„è½¬æ¢ï¼Œæˆ‘ä»¬å¯ä»¥æƒ³åˆ°çš„æ˜¯åˆ©ç”¨æ—‹è½¬å‘é‡æ¥åšä¸­é—´çš„æ¡¥æ¢ã€‚ä¸è¿‡å…¶ä¸­æœ‰ä¸ªarccoså‡½æ•°ä»£ä»·è¾ƒå¤§ï¼Œä½†æ˜¯å®é™…ä¸Šå¯ä»¥é€šè¿‡ä¸€å®šçš„æŠ€å·§ç»•è¿‡ã€‚åœ¨è¿™é‡Œç›´æ¥ç»™å‡ºå››å…ƒæ•°åˆ°æ—‹è½¬çŸ©é˜µçš„è½¬æ¢ç»“æœï¼ˆçœç•¥æ¨å¯¼è¿‡ç¨‹ï¼‰ï¼š</p><p>è®¾å››å…ƒæ•°ä¸ºï¼š$\mathbf{q} = q_0+q_1i+q_2j+q_3k$,åˆ™ï¼š</p><script type="math/tex; mode=display">R = \begin{bmatrix}1-2q_2^2 - 2q_3^2&2q_1q_2 - 2q_0q_3& 2q_1q_3 + 2q_0q_2\\2q_1q_2+2q_0q_3& 1-2q_1^2-2q_3^2& 2q_2q_3 - 2q_0q_1 \\2q_1q_3-2q_0q_2&2q_2q_3+2q_0q_1&1 - 2q_1^2 - 2q_2^2\end{bmatrix}</script><p>å¦‚æœçŸ¥é“äº†æ—‹è½¬çŸ©é˜µï¼Œæƒ³è¦å¾—åˆ°å››å…ƒæ•°ï¼š</p><p>$q_0 = \frac{\sqrt{tr(R) + 1}}{2},q_1 = \frac{r_{2,3} - r_{3,2}}{4q_0},q_2 = \frac{r_{3,1} - r_{1,3}}{4q_0},q_3 = \frac{r_{1,2} - r_{2,1}}{4q_0}$</p><p>è¿™é‡Œé¢$r_{i,j}$è¡¨ç¤º$R$çš„ç¬¬iè¡Œjåˆ—ã€‚åœ¨è®¡ç®—è¿‡ç¨‹ä¸­ï¼Œå¦‚æœ$q_0$æ¥è¿‘äº0,åˆ™å…¶ä»–3ä¸ªé‡å°±ä¼šå¾ˆå¤§ï¼Œè¿™æ˜¯å¾ˆéœ€è¦è€ƒè™‘åˆ«çš„æ–¹å¼æ¥è¡¨ç¤ºæ—‹è½¬ã€‚</p><h2 id="ç›¸ä¼¼ï¼Œä»¿å°„ï¼Œå°„å½±å˜æ¢"><a href="#ç›¸ä¼¼ï¼Œä»¿å°„ï¼Œå°„å½±å˜æ¢" class="headerlink" title="ç›¸ä¼¼ï¼Œä»¿å°„ï¼Œå°„å½±å˜æ¢"></a>ç›¸ä¼¼ï¼Œä»¿å°„ï¼Œå°„å½±å˜æ¢</h2><h3 id="ç›¸ä¼¼å˜æ¢"><a href="#ç›¸ä¼¼å˜æ¢" class="headerlink" title="ç›¸ä¼¼å˜æ¢"></a>ç›¸ä¼¼å˜æ¢</h3><p>ç›¸ä¼¼å˜æ¢æ¯”ä¹‹å‰çš„æ¬§å¼å˜æ¢å¤šäº†ä¸€ä¸ªè‡ªç”±åº¦ï¼š</p><script type="math/tex; mode=display">T_S = \begin{bmatrix}sR&\mathbf{t}\\0&1\end{bmatrix}</script><p>è¿™ä¸ªså…è®¸æˆ‘ä»¬å¯¹ç‰©ä½“è¿›è¡Œå‡åŒ€ç¼©æ”¾ã€‚</p><h3 id="ä»¿å°„å˜æ¢"><a href="#ä»¿å°„å˜æ¢" class="headerlink" title="ä»¿å°„å˜æ¢"></a>ä»¿å°„å˜æ¢</h3><script type="math/tex; mode=display">T_A = \begin{bmatrix}A&\mathbf{t}\\0&1\end{bmatrix}</script><p>ä»¿å°„å˜æ¢ä¸è¦æ±‚Aä¸ºæ­£äº¤çŸ©é˜µï¼Œåªè¦å¯é€†å³å¯ã€‚ä»¿å°„å˜æ¢åˆå«æ­£äº¤æŠ•å½±å˜æ¢ã€‚</p><h3 id="å°„å½±å˜æ¢"><a href="#å°„å½±å˜æ¢" class="headerlink" title="å°„å½±å˜æ¢"></a>å°„å½±å˜æ¢</h3><p>å°„å½±å˜æ¢æ˜¯æœ€æ˜“èˆ¬çš„å˜æ¢ã€‚</p><script type="math/tex; mode=display">T_P = \begin{bmatrix}sR&\mathbf{t}\\\mathbf{a}^T&v\end{bmatrix}</script><p>å·¦ä¸Šè§’å¯é€†ï¼Œå³ä¸Šè§’ä¸ºå¹³ç§»t,å·¦ä¸‹è§’ä¸ºç¼©æ”¾$\mathbf{a}^T$</p><p>ä»çœŸå®ä¸–ç•Œåˆ°ç›¸æœºç…§ç‰‡çš„å˜æ¢å¯ä»¥çœ‹ä½œä¸ºä¸€ä¸ªå°„å½±å˜æ¢ã€‚å¦‚æœç„¦è·ä¸ºæ— é™è¿œï¼Œåˆ™ä¸ºä»¿å°„å˜æ¢ã€‚</p><h2 id="Eigen"><a href="#Eigen" class="headerlink" title="Eigen"></a>Eigen</h2><p>æœ€åä»‹ç»ä¸€äº›Eigenåº“ç›¸å…³çš„ä¸œè¥¿ã€‚Eigenæ˜¯ä¸€ä¸ªC++å¼€æºçº¿æ€§ä»£æ•°åº“.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;eigen/core&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;eigen/dense&gt;</span></span></span><br><span class="line"><span class="comment">/*3*3çŸ©é˜µfloatå‹*/</span></span><br><span class="line">Eigen::Matrix&lt;<span class="keyword">float</span>,<span class="number">3</span>,<span class="number">3</span>,&gt; matrix_33;</span><br><span class="line"><span class="comment">/*3ç»´å‘é‡ï¼Œä½†å®é™…ä¸Šå°±æ˜¯Eigen::Matrix&lt;double,3,1&gt;*/</span></span><br><span class="line">Eigen::Vector3d v_3d;</span><br><span class="line"><span class="comment">//è¾“å…¥</span></span><br><span class="line">v_3d&lt;&lt;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>;</span><br><span class="line"><span class="comment">//è¾“å‡º</span></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;v_3d&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">//è®¿é—®iè¡Œjåˆ—</span></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;v_3d(<span class="number">1</span>,<span class="number">0</span>);</span><br><span class="line"><span class="comment">//è½¬ç½®</span></span><br><span class="line">matrix_33.transpose();</span><br><span class="line"><span class="comment">//å„é¡¹å’Œ</span></span><br><span class="line">matrix_33.sum();</span><br><span class="line"><span class="comment">//è¿¹</span></span><br><span class="line">matrix_33.trace();</span><br><span class="line"><span class="comment">//é€†</span></span><br><span class="line">matrix_33.inverse();</span><br><span class="line"><span class="comment">//è¡Œåˆ—å¼</span></span><br><span class="line">matrix_33.determinant();</span><br><span class="line"><span class="comment">//ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œå®å¯¹ç§°çŸ©é˜µç¡®ä¿å¯¹è§’åŒ–æˆåŠŸ</span></span><br><span class="line">Eigen::SelfAdjointEigenSolver&lt;Eigen::Matrix3d&gt; eigen_solver(matrix_33*matrix_33.transpose());</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; eigen_solver.eigenvalues() &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt; eigen_solver.eigenvectors() &lt;&lt; <span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure><div class="table-container"><table><thead><tr><th>Module</th><th>Header file</th><th>Contents</th></tr></thead><tbody><tr><td>Core</td><td>#include &lt; Eigen/Core &gt;</td><td>Matrixå’ŒArrayç±»ï¼ŒåŸºç¡€çš„çº¿æ€§ä»£æ•°è¿ç®—å’Œæ•°ç»„æ“ä½œ</td></tr><tr><td>Geometry</td><td>#include&lt; Eigen/Geometry &gt;</td><td>æ—‹è½¬ã€å¹³ç§»ã€ç¼©æ”¾ã€2ç»´å’Œ3ç»´çš„å„ç§å˜æ¢</td></tr><tr><td>LU</td><td>#include&lt; Eigen/LU &gt;</td><td>æ±‚é€†ï¼Œè¡Œåˆ—å¼ï¼ŒLUåˆ†è§£</td></tr><tr><td>Cholesky</td><td>#include &lt; Eigen/Cholesky &gt;</td><td>LLTå’ŒLDLT Choleskyåˆ†è§£</td></tr><tr><td>Householder</td><td>#include&lt; Eigen/Householder &gt;</td><td>è±ªæ–¯éœå°”å¾·å˜æ¢ï¼Œç”¨äºçº¿æ€§ä»£æ•°è¿ç®—</td></tr><tr><td>SVD</td><td>#include&lt; Eigen/SVD &gt;</td><td>SVDåˆ†è§£</td></tr><tr><td>QR</td><td>#include&lt; Eigen/QR &gt;</td><td>QRåˆ†è§£</td></tr><tr><td>Eigenvalues</td><td>#include&lt; Eigen/Eigenvalues &gt;</td><td>ç‰¹å¾å€¼ï¼Œç‰¹å¾å‘é‡åˆ†è§£</td></tr><tr><td>Sparse</td><td>#include&lt; Eigen/Sparse &gt;</td><td>ç¨€ç–çŸ©é˜µçš„å­˜å‚¨å’Œä¸€äº›åŸºæœ¬çš„çº¿æ€§è¿ç®—</td></tr><tr><td>ç¨ å¯†çŸ©é˜µ</td><td>#include&lt; Eigen/Dense &gt;</td><td>åŒ…å«Core/Geometry/LU/Cholesky/SVD/QR/Eigenvaluesæ¨¡å—</td></tr><tr><td>çŸ©é˜µ</td><td>#include&lt; Eigen/Eigen &gt;</td><td>åŒ…æ‹¬Denseå’ŒSparse(æ•´åˆåº“)</td></tr></tbody></table></div><p>è¿™äº›ä¸œè¥¿éƒ½è¢«æ•´åˆåœ¨denseæ¨¡å—ä¸­ã€‚</p><h3 id="eigenå‡ ä½•æ¨¡å—ï¼š"><a href="#eigenå‡ ä½•æ¨¡å—ï¼š" class="headerlink" title="eigenå‡ ä½•æ¨¡å—ï¼š"></a>eigenå‡ ä½•æ¨¡å—ï¼š</h3><p>æ—‹è½¬çŸ©é˜µç›´æ¥ä½¿ç”¨ Matrix3d æˆ– Matrix3fï¼š</p><p>Eigen::Matrix3d rotationMatrix=Eigen::Matrix3d::Identity();//åˆå§‹åŒ–ä¸ºä¸€ä¸ªå•ä½é˜µã€‚</p><p>æ—‹è½¬å‘é‡ä½¿ç”¨ AngleAxisï¼š</p><p>Eigen::AngleAxisd rotationVector(M_PI/4,Eigen::Vector3d(0,0,1)); //è§’+è½´ï¼šæ²¿ Z è½´æ—‹è½¬ 45 åº¦</p><p>æ¬§æ‹‰è§’ï¼š</p><p>Eigen::Vector3d ea0(yaw,pitching,droll);</p><p>æ—‹è½¬å‘é‡-&gt;æ—‹è½¬çŸ©é˜µï¼šrotationMatrix=rotation_vector.toRotationMatrix();</p><p>æ—‹è½¬å‘é‡-&gt;å››å…ƒæ•°ï¼šEigen::Quaterniond q = Eigen::Quaterniond ( rotation_vector );</p><p>æ—‹è½¬çŸ©é˜µ-&gt;å››å…ƒæ•°ï¼šEigen::Quaterniond q = Eigen::Quaterniond ( rotation_matrix );</p><p>å››å…ƒç´ -&gt;æ—‹è½¬çŸ©é˜µï¼šEigen::Matrix3d Rx = q.toRotationMatrix();</p><p>æ—‹è½¬å‘é‡-&gt;æ¬§æ‹‰è§’ï¼šEigen::Vector3d eulerAngle=rotationVector.matrix().eulerAngles(0,1,2);</p><p>æ—‹è½¬çŸ©é˜µ-&gt;æ¬§æ‹‰è§’ï¼šEigen::Vector3d euler_angles = rotation_matrix.eulerAngles ( 2,1,0 ); // ZYXé¡ºåºï¼Œå³roll pitch yawé¡ºåº</p><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><p>è¿™ä¸€è®²æœ€åè¯´æ˜äº†ä¸–ç•Œåæ ‡å’Œç›¸æœºåæ ‡çš„è½¬æ¢ã€‚ä¸–ç•Œåæ ‡ä¸‹çš„åæ ‡ä¸º$\mathbf{p}_w$ï¼Œç›¸æœºåæ ‡ä¸‹çš„åæ ‡ä¸º$mathbf{p}_c$ï¼Œåˆ™äºŒè€…è½¬æ¢ä¸ºï¼š</p><script type="math/tex; mode=display">\mathbf{p}_w = T_{c2w}\mathbf{p}_c</script><script type="math/tex; mode=display">\mathbf{p}_c = T_{w2c}\mathbf{p}_w</script><p>å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒæåˆ°äº†ä¸€èˆ¬æ›´å¸¸ç”¨çš„æ˜¯ä»ä¸–ç•Œåæ ‡åˆ°ç›¸æœºåæ ‡çš„è½¬æ¢ï¼Œä½†æ˜¯ä»ç›¸æœºåæ ‡åˆ°ä¸–ç•Œåæ ‡çš„è½¬æ¢å´æ›´ç›´è§‚ã€‚å› ä¸ºå¦‚æœç›¸æœºåæ ‡$\mathbf{p}_c$ä¸‹ä¸º0,åˆ™ä¸–ç•Œåæ ‡$\mathbf{p}_w$å°±æ˜¯ç›¸æœºåœ¨ä¸–ç•Œåæ ‡çš„ä½ç½®:</p><script type="math/tex; mode=display">\mathbf{p}_w = T_{c2w} = t_{c2w}</script><p>ä¸Šå¼ä¸­$t_{c2w}$æ­£æ˜¯ç›¸æœºçš„ä½ç½®ã€‚ä¹Ÿæ˜¯å¹³ç§»å‘é‡ã€‚</p><p>æ‰€ä»¥ä»ç›¸æœºåæ ‡åˆ°ä¸–ç•Œåæ ‡çš„è½¬æ¢å¯ä»¥ç›´æ¥çœ‹åˆ°ç›¸æœºä½ç½®ã€‚è¿™ä¹Ÿæ˜¯å› ä¸ºä»ç›¸æœºåæ ‡åˆ°ä¸–ç•Œåæ ‡çš„è½¬æ¢æ˜¯å…ˆæ—‹è½¬åå¹³ç§»çš„ç‰¹æ€§ã€‚å…ˆæ—‹è½¬åå¹³ç§»ï¼Œåˆ™å¹³ç§»å‘é‡æ˜¯ä¸ç”¨æ”¹å˜çš„ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> Eigen </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>SLAMâ€”â€”åŸºæœ¬ä»‹ç»</title>
      <link href="/2018/11/06/SLAM%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
      <url>/2018/11/06/SLAM%E2%80%94%E2%80%94%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<p>ä¸å‡ºæ„å¤–çš„è¯ï¼Œä»¥åæˆ‘çš„æ–¹å‘åº”è¯¥å°±æ˜¯ä¸‰ç»´é‡å»ºæ–¹å‘äº†ï¼Œè€ŒSLAMæ˜¯ä¸€ä¸ªé€ƒä¸å¼€çš„ä¸œè¥¿ã€‚<a id="more"></a></p><p>SLAMï¼ˆsimultaneous localization and mappingï¼‰ï¼Œå³æ—¶å®šä½ä¸åœ°å›¾æ„å»ºã€‚å®ƒæ˜¯ä¸ªä»€ä¹ˆä¸œè¥¿ï¼Ÿå°±æ˜¯å°†ä¸€ä¸ªæœºå™¨äººæ”¾åˆ°ä¸€ä¸ªé™Œç”Ÿçš„ç¯å¢ƒï¼Œå®ƒèƒ½å¤Ÿè‡ªæˆ‘å®šä½å¹¶æ„å»ºå‡ºå½“å‰ç¯å¢ƒçš„ä¸‰ç»´åœ°å›¾ã€‚æˆ‘ä»¬å®éªŒå®¤çš„æœ‰ä¸€ä¸ªé¡¹ç›®ï¼š<a href="http://luvision.net/FlashFusion/" target="_blank" rel="noopener">FlashFusion</a>ï¼Œå’ŒSLAMä¹Ÿæœ‰è¿™åƒä¸ä¸‡ç¼•çš„å…³ç³»ã€‚</p><p>SLAMå·²ç»è¯ç”Ÿäº†30å¤šå¹´äº†ï¼Œä¹Ÿå–å¾—äº†å¾ˆé•¿è¶³çš„è¿›æ­¥å¸å¼•äº†å¾ˆå¤šå­¦æœ¯ç•Œçš„å…³æ³¨ï¼Œä½†æ˜¯ä¸€ç›´è¾¾ä¸åˆ°ä¸šç•Œä½¿ç”¨çš„è¦æ±‚ã€‚</p><p>SLAMå­¦ä¹ çš„é—¨æ§›æ¯”è¾ƒé«˜ï¼Œå› ä¸ºå¯¹çŸ¥è¯†å‚¨å¤‡ä»¥åŠå·¥ç¨‹èƒ½åŠ›éƒ½æœ‰è¾ƒé«˜çš„è¦æ±‚ã€‚</p><p>æˆ‘çš„è¿™ä¸ªSLAMæ‰“ç®—åˆ©ç”¨é«˜ç¿”çš„ã€ŠSLAMåå››è®²ã€‹æ¥å®Œæˆã€‚é¦–å…ˆç¬¬ä¸€ç« æ˜¯ä¸€äº›å¤§æ¦‚çš„ä»‹ç»ä»¥åŠä¸€äº›ç¼–ç¨‹çš„ç¯å¢ƒçš„æ­å»ºã€‚å®é™…ä¸Šæˆ‘å¯¹linuxè¿˜ä¸å¤Ÿç†Ÿæ‚‰ï¼Œå¯¹äºCMakeä¹Ÿä½¿ç”¨è¾ƒå°‘ã€‚æ‰€ä»¥è¿™ç¯‡åšå®¢ä¹Ÿä¼šä»‹ç»ä¸€äº›è¿™æ–¹é¢çš„ä¸œè¥¿ã€‚</p><h2 id="ä¼ æ„Ÿå™¨ä»‹ç»"><a href="#ä¼ æ„Ÿå™¨ä»‹ç»" class="headerlink" title="ä¼ æ„Ÿå™¨ä»‹ç»"></a>ä¼ æ„Ÿå™¨ä»‹ç»</h2><p>é¦–å…ˆï¼ŒSLAMéœ€è¦çš„ä¸€äº›ä¼ æ„Ÿå™¨ï¼Œæœ‰æ¿€å…‰ï¼Œä¹Ÿæœ‰ç›¸æœºã€‚å®é™…ä¸Šæˆ‘ä»¬çš„é‡ç‚¹åœ¨äºç›¸æœºï¼Œå› ä¸ºç›¸æœºä¾¿å®œï¼Œè€Œæ¿€å…‰å¾ˆè´µã€‚ç›¸æœºåˆ†ä¸ºå•ç›®ç›¸æœºï¼ŒåŒç›®ç›¸æœºï¼ˆStereoï¼‰ä»¥åŠæ·±åº¦ç›¸æœºï¼ˆRGBDï¼‰ï¼Œäº‹ä»¶ç›¸æœºï¼ˆEventï¼‰ã€‚ä¸€èˆ¬æ¥è¯´ä½¿ç”¨è¾ƒå¤šçš„Stereoå’ŒRGBDï¼Œå•ç›®æ²¡æœ‰æ·±åº¦ï¼Œåªèƒ½åŒè¿‡ç§»åŠ¨ç›¸æœºæ¥æƒ³åŠæ³•äº§ç”Ÿæ·±åº¦ï¼ŒStereoç›¸æœºé€šè¿‡ä¸¤ä¸ªé•œå¤´æ¥è·å¾—æ·±åº¦ï¼Œè€ŒRGBDç›¸æœºé€šè¿‡ä¸€å®šçš„ç‰©ç†æ‰‹æ®µæ¥è·å–æ·±åº¦ï¼ˆå¦‚çº¢å¤–çº¿ï¼Œç»“æ„å…‰ç­‰ï¼‰ã€‚è€Œåˆ°åé¢æˆ‘ä»¬ä¼šçŸ¥é“ï¼Œæ·±åº¦å¯¹äºSLAMæ˜¯éå¸¸é‡è¦çš„ä¸€ä¸ªä¿¡æ¯ã€‚</p><p>å½“æˆ‘ä»¬æ‹æ‘„ä¸€å¼ ç…§ç‰‡çš„æ—¶å€™ï¼Œä»3Dåˆ°2Dï¼Œä¼šæŸå¤±äº†å¾ˆå¤šä¿¡æ¯ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦æ·±åº¦æ‰èƒ½æ„å»ºä¸‰ç»´æ¨¡å‹ã€‚å•ç›®ç›¸æœºï¼Œåªèƒ½é€šè¿‡è¿åŠ¨æ¥æ¨ç®—è·ç¦»ï¼ˆè¿œçš„è¿åŠ¨æ…¢ï¼Œè¿‘çš„è¿åŠ¨å¿«ï¼‰ï¼Œä½†æ˜¯è®¡ç®—æ¯”è¾ƒå¤æ‚ï¼Œä¹Ÿç»å¸¸å‡ºé—®é¢˜ï¼Œä¸èƒ½é¿å…å¾ˆå¤šä¸ç¡®å®šæ€§ã€‚</p><h2 id="è§†è§‰SLAMæ¡†æ¶"><a href="#è§†è§‰SLAMæ¡†æ¶" class="headerlink" title="è§†è§‰SLAMæ¡†æ¶"></a>è§†è§‰SLAMæ¡†æ¶</h2><ul><li>å‰ç«¯ï¼ˆVisual Odometryï¼‰</li><li>åç«¯ï¼ˆOptimizationï¼‰</li><li>å›ç¯æ£€æµ‹ï¼ˆLoop Closure Detectionï¼‰</li><li>å»ºå›¾ï¼ˆMappingï¼‰</li></ul><p>è¿™äº›æ¨¡å—æ¯ä¸ªéƒ½éœ€è¦å¾ˆå¤šçš„çŸ¥è¯†å’Œç²¾åŠ›æ¥å­¦ä¹ ï¼Œæ‰€ä»¥è¿™é‡Œåªåˆ—å‡ºæ¥æ¡†æ¶ã€‚ä»¥åå­¦ä¹ å®Œæ¯•ä¹‹åï¼Œåœ¨ç»™å®ƒä»¬åŠ ä¸Šè¶…é“¾æ¥ã€‚</p><h2 id="æ•°å­¦æè¿°"><a href="#æ•°å­¦æè¿°" class="headerlink" title="æ•°å­¦æè¿°"></a>æ•°å­¦æè¿°</h2><p>æˆ‘ä»¬å‡è®¾åœ°å›¾æ˜¯ç”±è·¯æ ‡æè¿°çš„ï¼Œè·¯æ ‡æœ‰Nä¸ªï¼Œåˆ™è·¯æ ‡åˆ†åˆ«ä¸ºï¼š$\mathbf{y}_1,â€¦,\mathbf{y}_N$. è€Œå„ä¸ªæ—¶åˆ»çš„æœºå™¨äººçš„ä½ç½®è¡¨ç¤ºä¸º$\mathbf{x}_1,â€¦,\mathbf{x}_k$.å…¶ä¸­kä¸ºæ—¶åˆ»ã€‚</p><p>åˆ™æˆ‘ä»¬å¯ä»¥ç”¨ä¸‹é¢ä¸¤ä¸ªå¼å­æ¥æè¿°SLAM:</p><script type="math/tex; mode=display">\left \{\begin{aligned}\mathbf{x}_k = f(\mathbf{x}_{k-1},\mu_k,\mathbf{w}_k)\\z_{k,j} = h(\mathbf{y}_j,\mathbf{x}_k,\mathbf{v}_{k,j})\end{aligned}\right.</script><p>ä¸Šå¼ä¸­ï¼Œ$\mu_k$ä¸ºä¼ æ„Ÿå™¨è¯»æ•°,$\mathbf{w}_k$ä¸ºå™ªå£°ã€‚ç¬¬ä¸€ä¸ªå¼å­ä¸ºè¿åŠ¨æ–¹ç¨‹ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬é€šè¿‡ä¹‹å‰çš„ä½ç½®å’Œè¿åŠ¨ä¼ æ„Ÿå™¨çš„è¾“å…¥å¾—åˆ°äº†ç›®å‰æ—¶åˆ»çš„ä½ç½®ã€‚</p><p>ç¬¬äºŒä¸ªå¼å­ä¸ºè§‚æµ‹æ–¹ç¨‹ï¼Œzä¸ºè§‚æµ‹æ•°æ®ï¼Œ$\mathbf{v}_{k,j}$ä¸ºè§‚æµ‹å™ªå£°ã€‚è§‚æµ‹æ–¹ç¨‹ä¸­$z$æ˜¯æˆ‘ä»¬ç›´æ¥è§‚æµ‹åˆ°çš„ã€‚</p><p>å¦‚æœæˆ‘ä»¬å¯ä»¥å¾—åˆ°$\mathbf{x}_k$ä¸$\mathbf{y}_k$çš„å€¼ï¼Œä¸å°±å®ç°äº†å®šä½ä¸å»ºå›¾å—ï¼Ÿ</p><h2 id="CMake"><a href="#CMake" class="headerlink" title="CMake"></a>CMake</h2><p>åœ¨SLAMä¸­C++è¯­è¨€æ˜¯å æœ‰ç»å¯¹ä¼˜åŠ¿çš„ã€‚ä»»ä½•ç¨‹åºéƒ½å¯ä»¥ä½¿ç”¨g++ç¼–è¯‘ï¼Œä½†æ˜¯å¯¹äºè¿‡äºå¤æ‚çš„å·¥ç¨‹ï¼Œg++çš„å‘½ä»¤ä¼šå¤ªé•¿ä¸å¥½æ“ä½œï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä½¿ç”¨CMakeå·¥å…·ã€‚</p><p>CMakeæ˜¯ä¸€ç§è·¨å¹³å°ç¼–è¯‘å·¥å…·ï¼Œæ¯”makeæ›´ä¸ºé«˜çº§ï¼Œä½¿ç”¨èµ·æ¥è¦æ–¹ä¾¿å¾—å¤šã€‚CMakeä¸»è¦æ˜¯ç¼–å†™CMakeLists.txtæ–‡ä»¶ï¼Œç„¶åç”¨cmakeå‘½ä»¤å°†CMakeLists.txtæ–‡ä»¶è½¬åŒ–ä¸ºmakeæ‰€éœ€è¦çš„makefileæ–‡ä»¶ï¼Œæœ€åç”¨makeå‘½ä»¤ç¼–è¯‘æºç ç”Ÿæˆå¯æ‰§è¡Œç¨‹åºæˆ–é™æ€åº“(.a)æˆ–è€…å…±äº«åº“ï¼ˆ.so(shared object)ï¼‰ã€‚</p><p>å®é™…ä¸Šï¼ŒCMakeçš„ä½¿ç”¨ä¸»è¦åœ¨äºCMakeList.txtçš„ç¼–å†™ã€‚</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.cmake versonï¼ŒæŒ‡å®šcmakeç‰ˆæœ¬ </span></span><br><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.project nameï¼ŒæŒ‡å®šé¡¹ç›®çš„åç§°ï¼Œä¸€èˆ¬å’Œé¡¹ç›®çš„æ–‡ä»¶å¤¹åç§°å¯¹åº”</span></span><br><span class="line"><span class="keyword">PROJECT</span>(test_sqrt)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.head file pathï¼Œå¤´æ–‡ä»¶ç›®å½•</span></span><br><span class="line"><span class="keyword">INCLUDE_DIRECTORIES</span>(</span><br><span class="line"><span class="keyword">include</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.source directoryï¼Œæºæ–‡ä»¶ç›®å½•</span></span><br><span class="line"><span class="keyword">AUX_SOURCE_DIRECTORY</span>(src DIR_SRCS)</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.set environment variableï¼Œè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œç¼–è¯‘ç”¨åˆ°çš„æºæ–‡ä»¶å…¨éƒ¨éƒ½è¦æ”¾åˆ°è¿™é‡Œï¼Œå¦åˆ™ç¼–è¯‘èƒ½å¤Ÿé€šè¿‡ï¼Œä½†æ˜¯æ‰§è¡Œçš„æ—¶å€™ä¼šå‡ºç°å„ç§é—®é¢˜ï¼Œæ¯”å¦‚"symbol lookup error xxxxx , undefined symbol"</span></span><br><span class="line"><span class="keyword">SET</span>(TEST_MATH</span><br><span class="line"><span class="variable">$&#123;DIR_SRCS&#125;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#6.add executable fileï¼Œæ·»åŠ è¦ç¼–è¯‘çš„å¯æ‰§è¡Œæ–‡ä»¶</span></span><br><span class="line"><span class="keyword">ADD_EXECUTABLE</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> <span class="variable">$&#123;TEST_MATH&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#7.add link libraryï¼Œæ·»åŠ å¯æ‰§è¡Œæ–‡ä»¶æ‰€éœ€è¦çš„åº“ï¼Œæ¯”å¦‚æˆ‘ä»¬ç”¨åˆ°äº†libm.soï¼ˆå‘½åè§„åˆ™ï¼šlib+name+.soï¼‰ï¼Œå°±æ·»åŠ è¯¥åº“çš„åç§°</span></span><br><span class="line"><span class="keyword">TARGET_LINK_LIBRARIES</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> m)</span><br></pre></td></tr></table></figure><p>aux_source_directory(&lt; dir &gt; &lt; variable &gt;)</p><p>æœé›†æ‰€æœ‰åœ¨æŒ‡å®šè·¯å¾„ä¸‹çš„æºæ–‡ä»¶çš„æ–‡ä»¶åï¼Œå°†è¾“å‡ºç»“æœåˆ—è¡¨å‚¨å­˜åœ¨æŒ‡å®šçš„å˜é‡ä¸­ã€‚<br>å¦‚æœæƒ³è¦ç”Ÿæˆåº“æ–‡ä»¶ï¼š<br><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#é™æ€åº“</span></span><br><span class="line"><span class="keyword">add_library</span>( name libname.cpp)</span><br><span class="line"><span class="comment">#å…±äº«åº“</span></span><br><span class="line"><span class="keyword">add_library</span>(name_shared SHARED libname.cpp)</span><br></pre></td></tr></table></figure></p><p>å½“ç„¶ï¼ŒCmakeå·¥å…·è¿˜æœ‰æ›´å¤šä½¿ç”¨çš„æŠ€å·§ï¼Œéœ€è¦å¹³æ—¶åšé¡¹ç›®çš„æ—¶å€™å»ç§¯ç´¯ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> SLAM </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLAM </tag>
            
            <tag> 3D reconstruction </tag>
            
            <tag> CMake </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Learning From Dataâ€”â€”Covariance Matrix Derivation</title>
      <link href="/2018/11/06/Learning-From-Data%E2%80%94%E2%80%94Covariance-Matrix-Derivation/"/>
      <url>/2018/11/06/Learning-From-Data%E2%80%94%E2%80%94Covariance-Matrix-Derivation/</url>
      
        <content type="html"><![CDATA[<p>ä¸Šå‘¨çš„æ•°æ®å­¦ä¹ è¯¾ç¨‹å¸ƒç½®äº†ä¸€ä¸ªä½œä¸šï¼Œä¸»è¦åšçš„æ˜¯å¯¹å¤šç»´é«˜æ–¯åˆ†å¸ƒä¸‹æ±‚å¾—åæ–¹å·®çŸ©é˜µçš„å–å€¼ã€‚è¿™ä¸ªå’Œä¹‹å‰å°†çš„Generative Learning Algorithmå¾ˆç›¸å…³ï¼Œä½†æ˜¯å½“æ—¶æ˜¯ç›´æ¥ç»™å‡ºäº†åæ–¹å·®çŸ©é˜µçš„å–å€¼ã€‚ç»“æœæ˜¯å¼‚å¸¸ç®€å•çš„ï¼Œä½†æ˜¯å…¶ä¸­çš„è¯æ˜å¯èƒ½è¦è´¹ç‚¹åŠŸå¤«ã€‚<br><a id="more"></a></p><p>é¢˜ç›®æè¿°å¦‚ä¸‹ï¼š</p><p>Linear Discriminant Analysis (LDA) is a special case of Gaussian Discriminant Analysis (GDA) which assumes that the classes have a common covariance matrix $\Sigma_j = \Sigma, \forall j$. Now suppose all the $\Sigma_j$â€™s are not equal, and we will get the Quadratic Discriminant Analysis (QDA). The estimations for QDA are similar to those for LDA, except that separate covariance matrices must be estimated for each class. Give the maximum likelihood estimate of Î£ j â€™s for the case K = 2.</p><p>é¢˜ç›®ä¸­è¯´ï¼Œä¹‹å‰åšå®¢ä¸­ä»‹ç»çš„å„ä¸ªåˆ†ç±»çš„$\Sigma$éƒ½æ˜¯ä¸€æ ·çš„ï¼Œé‚£å«åšLDAï¼Œå¦‚æœæ¯ä¸ªç±»åˆ«éƒ½æœ‰è‡ªå·±çš„$\Sigma_j$ï¼Œåˆ™æ˜¯QDAã€‚è®©æˆ‘ä»¬æ¨å¯¼QDAçš„åæ–¹å·®çŸ©é˜µåº”è¯¥æ˜¯ä»€ä¹ˆæ ·å­ã€‚</p><p>å®é™…ä¸Šï¼Œå¯¹äºQDAè¿˜æ˜¯LDAï¼Œåæ–¹å·®çŸ©é˜µçš„æ¨å¯¼æ˜¯å¤§è‡´ä¸€æ ·çš„ï¼Œè€ŒQDAçš„æœ€åç»“æœä¹Ÿæ˜¯éå¸¸ç®€å•ã€‚è‡³äº$\phi_j,\mu_j$ç­‰ç›¸æ¯”ä¹‹ä¸‹æ›´ç®€å•ï¼Œç»“æœä¹Ÿå’Œä¹‹å‰ä¸€æ ·ï¼Œå°±ä¸åœ¨è¿™é‡Œè¿›è¡Œè¯æ˜äº†ã€‚</p><p>è¿™ç¯‡åšå®¢å®é™…ä¸Šå°±æ˜¯æŠŠä¹‹å‰çš„å†™çš„ä½œä¸šå‘å‡ºæ¥ï¼Œå› ä¸ºæˆ‘ä»¬ä½œä¸šè¦æ±‚ä¸ºè‹±æ–‡ï¼Œå› æ­¤ä¸‹é¢çš„è¯æ˜å°†ä¸ºè‹±æ–‡ã€‚</p><p>Firstly,  we need to know the log Maximum Likelihood Estimate:</p><script type="math/tex; mode=display">\begin{equation*}\begin{aligned}&\log L(\mu_1,...,\mu_k,\Sigma_1,...,\Sigma_k,\phi_1,...,\phi_k)\\ &= \log \prod_{i=1}^m p(x_i,y_i;\mu_1,...,\mu_k,\Sigma_1,...,\Sigma_k,\phi_1,...,\phi_k)\\ &=\log \prod_{i=1}^m p(x_i|y_i;\mu_{y_i},\Sigma_{y_i})p(y_i;\phi_{y_i})\\ &=\log \prod_{i=1}^m \prod_{j=1}^k \mathbf{1}\{y_i=j\} \frac{1}{(2\pi)^{\frac n 2}\vert \Sigma_j\vert ^{\frac 1 2}} e^{-\frac 1 2(x_i-u_j)^T\Sigma^{-1}(x_i-u_j)}p(y_i=k;\phi_{k}) \\ &= \sum_{i=1}^m  \sum_{j=1}^k \mathbf{1}\{y_i=j\}( - \frac 1 2(x_i-u_j)^T\Sigma^{-1}(x_i-u_j) -\frac n 2 \log (2\pi) + \frac 1 2 \log\vert \Sigma_j\vert+\log p(y_i;\phi_{y_i}))\\\end{aligned}\end{equation*}</script><p>If we want to find the Maximum, we need to get the derivative of Sigma. If we cut the useless parts,the function will be look like this:</p><script type="math/tex; mode=display">l = \frac 1 2\sum_{i=1}^m  \sum_{j=1}^k \mathbf{1}\{y_i=j\}(\log\vert \Sigma_j\vert - (x_i-u_j)^T\Sigma^{-1}(x_i-u_j))</script><p>I need to tell some basic rules about derivative of matrix:</p><script type="math/tex; mode=display">\begin{align}\frac { \partial \vert A\vert}{\partial A} = |A|(A^{-1})^T\\\frac {\partial A^{-1}}{\partial x} = A^{-1}\frac{\partial A}{\partial x} A^{-1}     \end{align}</script><p>We could use the (1) to get the $\log |\Sigma_k|$ â€˜s derivative. Because of the SPD, we could get:</p><script type="math/tex; mode=display">\begin{align}\frac {\partial \log \vert \Sigma_j \vert}{\partial \Sigma_j} = (\Sigma_j^{-1})^T = \Sigma_j^{-1}\end{align}</script><p>Then, use the rule (2). Because the x is a scalar, so we need to separate the process.First letâ€™s try to find the derivative of $\Sigma_{k,(i,j)}$:</p><script type="math/tex; mode=display">\begin{equation*}\begin{aligned}\frac{\partial \Sigma_k^{-1}}{\partial \Sigma_{k,(i,j)}} &= \Sigma_k^{-1} \frac{\partial \Sigma_k}{ \Sigma_{k,(i,j)}}\Sigma_k^{-1}\\(x_i-u_j)^T\frac{\partial \Sigma_k^{-1}}{\partial \Sigma_{k,(i,j)}}(x_i-u_j)&=  (x_i-u_j)^T\Sigma_k^{-1} \frac{\partial \Sigma_k}{ \Sigma_{k,(i,j)}}\Sigma_k^{-1}(x_i-u_j)\end{aligned}\end{equation*}</script><p>We noticed that $(x_i-u_j)^T\Sigma_k^{-1} = (\Sigma_k^{-1}(x_i-u_j))^T $.</p><p>And the matrix $\frac{\partial \Sigma_k^{-1}}{\partial \Sigma_{k,(i,j)}}$ will be like a n $\times$ n matrix with the exception that the value of the position(i,j) will be 1.</p><p>So we could get:</p><script type="math/tex; mode=display">\begin{equation*}\begin{aligned}(x_i-u_j)^T\frac{\partial \Sigma_k^{-1}}{\partial \Sigma_{k,(i,j)}}(x_i-u_j)&=  (x_i-u_j)^T\Sigma_k^{-1} \frac{\partial \Sigma_k}{ \Sigma_{k,(i,j)}}\Sigma_k^{-1}(x_i-u_j)\\&= [(\Sigma_k^{-1}(x_i-u_j)) (\Sigma_k^{-1}(x_i-u_j))^T]_{(i,j)}\end{aligned}\end{equation*}</script><p>So:</p><script type="math/tex; mode=display">\begin{align}(x_i-u_j)^T\frac{\partial \Sigma_k^{-1}}{\partial \Sigma_{k,(i,j)}}(x_i-u_j) = (\Sigma_k^{-1}(x_i-u_j)) (\Sigma_k^{-1}(x_i-u_j))^T\end{align}</script><p>Now use (3) and (4),we could get: </p><script type="math/tex; mode=display">\begin{equation*}\begin{aligned}\frac{\partial l}{\partial \Sigma_j} &=\frac 1 2\sum_{i=1}^m \mathbf{1}\{y_i=j\} (\Sigma_j ^{-1}-   (\Sigma_k^{-1}(x_i-u_j)) (\Sigma_k^{-1}(x_i-u_j))^T)\\ &=\frac 1 2\sum_{i=1}^m \mathbf{1}\{y_i=j\} (\Sigma_j ^{-1}-   \Sigma_j^{-1}(x_i-u_j)(x_i-u_j)^T\Sigma_j^{-1})\end{aligned}\end{equation*}</script><p>Because we want to let $\frac{\partial l}{\partial \Sigma_j} = \mathbf{0}$:</p><script type="math/tex; mode=display">\begin{equation*}\begin{aligned}\frac 1 2\sum_{i=1}^m  \mathbf{1}\{y_i=j\} (\Sigma_j ^{-1}-   \Sigma_j^{-1}(x_i-u_j)(x_i-u_j)^T\Sigma_j^{-1}) = \mathbf{0}\\\sum_{i=1}^m\mathbf{1}\{y_i=j\} (I - \Sigma_j^{-1}(x_i-u_j)(x_i-u_j)^T) = \mathbf{0}\\\sum_{i=1}^m\mathbf{1}\{y_i=j\} I = \Sigma_j^{-1}\sum_{i=1}^m \mathbf{1}\{y_i=j\}(x_i-u_j)(x_i-u_j)^T\end{aligned}\end{equation*}</script><p>So,for QDA,the $\Sigma_j$ will be like this:</p><script type="math/tex; mode=display">\begin{equation*}\begin{aligned}\Sigma_j = \frac{\sum_{i=1}^m \mathbf{1}\{y_i=j\}(x_i - \mu_{j}) (x_i - \mu_{j})^T}{\sum_{i=1}^m \mathbf{1}\{y_i=j\}}\end{aligned}\end{equation*}</script><p>where $j=1,2$.</p>]]></content>
      
      
      <categories>
          
          <category> æ•°æ®å­¦ä¹ è¯¾ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LFD class </tag>
            
            <tag> mathematics </tag>
            
            <tag> homework </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Learning From Dataâ€”â€”Neural Network</title>
      <link href="/2018/11/06/Learning-From-Data%E2%80%94%E2%80%94Neural-Network/"/>
      <url>/2018/11/06/Learning-From-Data%E2%80%94%E2%80%94Neural-Network/</url>
      
        <content type="html"><![CDATA[<p>è¿™å‘¨ä¸Šçš„æ•°æ®å­¦ä¹ ï¼Œä¸»è¦è®²äº†ä¸€äº›ç¥ç»ç½‘ç»œç›¸å…³çš„çŸ¥è¯†ã€‚ç¥ç»ç½‘ç»œæ˜¯ç›®å‰æœ€æµè¡Œçš„æœºå™¨å­¦ä¹ ç®—æ³•äº†ï¼Œç”šè‡³ç”±å®ƒè¯ç”Ÿäº†ä¸€ä¸ªæ–°çš„å­¦ç§‘ï¼šdeep learningã€‚å› æ­¤ä¸€ç¯‡åšå®¢ï¼Œåªèƒ½æµ…æµ…ä»‹ç»ä¸€äº›ç¥ç»ç½‘ç»œçš„åŸºæœ¬å†…å®¹ã€‚<br><a id="more"></a><br>æ®è¯´ç¥ç»ç½‘ç»œåˆ¶é€ å‡ºæ¥æ˜¯ä¸ºäº†æ¨¡æ‹Ÿå¤§è„‘ã€‚ä¸è¿‡æˆ‘è®¤ä¸ºç¦»è¿™ä¸ªç›®æ ‡è¿˜å·®çš„è¿œã€‚ä½†æ˜¯å‘¢ï¼ŒNeural Networkç¡®å®åšå‡ºæ¥ä¸€äº›å¾ˆç‰›é€¼çš„äº‹æƒ…ï¼Œè®©å®ƒæˆä¸ºç°åœ¨AIä¸­æœ€å—æ¬¢è¿çš„æŠ€æœ¯ã€‚ä¸è¿‡ç¥ç»ç½‘ç»œçš„æ¦‚å¿µå€’æ˜¯å¾ˆæ—©å¾ˆæ—©å°±æå‡ºäº†ï¼Œä¹‹å‰æ²¡è½æ˜¯å› ä¸ºè®¡ç®—çš„æ€§èƒ½è·Ÿä¸ä¸Šã€‚ç°åœ¨åˆä¸œå±±å†èµ·äº†ã€‚</p><p>å³ä½¿æ²¡æœ‰æ¥è§¦è¿‡æœºå™¨å­¦ä¹ ï¼Œä¹Ÿä¸€å®šå¬è¿‡ç¥ç»ç½‘ç»œå­¦ä¹ ï¼Œä»¥åŠè§è¿‡ç±»ä¼¼ä¸‹é¢çš„å›¾ï¼š</p><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/560px-Artificial_neural_network.svg.png" alt=""></p><p>å®é™…ä¸Šï¼Œè¿™å°±æ˜¯ç¥ç»ç½‘ç»œã€‚ç¥ç»ç½‘ç»œçš„åŸç†ä¸å¤æ‚ï¼Œä½†æ˜¯å¦‚æœåµŒå¥—å±‚æ•°æ¯”è¾ƒå¤šï¼Œå°±ä¼šéœ€è¦éå¸¸å¤§çš„è®¡ç®—é‡ã€‚</p><p>å¯¹äºæ¯ä¸€ä¸ªç¥ç»å…ƒï¼Œæˆ‘ä»¬éƒ½å¯ä»¥æŠŠå®ƒçœ‹ä½œä¹‹å‰çš„ä¸€ä¸ªlogistic regressionã€‚æ¯ä¸€ä¸ªç¥ç»å…ƒå¯ä»¥æ¥å—è¾“å…¥ï¼Œç„¶åæä¾›è¾“å‡ºï¼Œè¦ä¹ˆä½œä¸ºæœ€ç»ˆçš„è¾“å‡ºï¼Œè¦ä¹ˆç»™åˆ«çš„ç¥ç»å…ƒæä¾›è¾“å…¥ã€‚</p><h2 id="å‰å‘ä¼ æ’­ï¼ˆforward-propagationï¼‰"><a href="#å‰å‘ä¼ æ’­ï¼ˆforward-propagationï¼‰" class="headerlink" title="å‰å‘ä¼ æ’­ï¼ˆforward propagationï¼‰"></a>å‰å‘ä¼ æ’­ï¼ˆforward propagationï¼‰</h2><p>è¿™æ„å‘³ç€ï¼Œæˆ‘ä»¬çš„$W$å‚æ•°å°†ä¼šå˜æˆä¸€ä¸ªâ€å¼ é‡â€ï¼ˆè¿™ä¹ˆè¯´ä¹Ÿè®¸ä¸å¤Ÿå‡†ç¡®ï¼Œå› ä¸ºå®ƒä¸ç¬¬ä¸€å±‚å¯èƒ½æœ‰5ä¸ªç¥ç»å…ƒï¼Œç¬¬2å±‚å¯èƒ½æœ‰4ä¸ªï¼Œä¹Ÿå°±æ˜¯ä¸æ˜¯ä¸€ä¸ªç«‹æ–¹ä½“ï¼‰ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç”¨$\Theta$æ¥è¡¨ç¤ºè¿™ä¸ªå¼ é‡ã€‚$\Theta^{(i)}$è¡¨ç¤ºç¬¬iå±‚çš„$\theta$å‚æ•°ï¼Œè€Œ$\Theta^{(i)}_j$è¡¨ç¤ºç¬¬iå±‚ï¼Œç¬¬jä¸ªthetaå‘é‡ï¼Œ$Theta^{(i)}_{j,k}$è¡¨ç¤ºçš„å°±æ˜¯æŸä¸ªå…·ä½“çš„å‚æ•°å€¼äº†ã€‚</p><p>æˆ‘ä»¬ä½¿ç”¨$a^{(i)}_j$æŒ‰ç…§ä¸Šé¢çš„è§„åˆ™æ¥è¡¨ç¤ºç¬¬iå±‚ç¬¬jä¸ªç¥ç»å…ƒçš„è¾“å‡ºã€‚</p><p>æ‰€ä»¥ï¼Œå°±ä¸Šé¢çš„è¿™ä¸ªå›¾ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“å¾—å‡ºï¼š</p><script type="math/tex; mode=display">a_1^{(1)} = g(\Theta_{1,0}^{(1)}x_0 + \Theta_{1,1}^{(1)}x_1 + \Theta_{1,2}^{(2)}x_2+\Theta_{1,3}^{(3)}x_3 )\\a_2^{(1)} = g(\Theta_{2,0}^{(1)}x_0 + \Theta_{2,1}^{(1)}x_1 + \Theta_{2,2}^{(2)}x_2+\Theta_{2,3}^{(3)}x_3 )\\a_3^{(1)} = g(\Theta_{3,0}^{(1)}x_0 + \Theta_{3,1}^{(1)}x_1 + \Theta_{3,2}^{(2)}x_2+\Theta_{3,3}^{(3)}x_3 )a_4^{(1)} = g(\Theta_{4,0}^{(1)}x_0 + \Theta_{4,1}^{(1)}x_1 + \Theta_{4,2}^{(2)}x_2+\Theta_{4,3}^{(3)}x_3 )\\h_{\Theta}(X) =\\\begin{bmatrix} g(\Theta_{1,0}^{(2)}a_0^{(1)} + \Theta_{1,1}^{(2)}a_1^{(1)} + \Theta_{1,2}^{(2)}a_2^{(1)} +\Theta_{1,3}^{(2)}a_3^{(1)}+\Theta_{1,4}^{(2)}a_4^{(1)} )\\g(\Theta_{2,0}^{(2)}a_0^{(1)} + \Theta_{2,1}^{(2)}a_1^{(1)} + \Theta_{2,2}^{(2)}a_2^{(1)} +\Theta_{2,3}^{(2)}a_3^{(1)}+\Theta_{2,4}^{(2)}a_4^{(1)} )\end{bmatrix}\\= [a_1^{(2)},a_2^{(2)} ]^T</script><p>ä¸Šé¢çš„ç¥ç»ç½‘ç»œè¾“å‡ºæœ‰ä¸¤é¡¹ã€‚</p><p>ä¸Šé¢çš„å‡½æ•°ä¸­ï¼Œgä¸ºlogisticå‡½æ•°ï¼Œåˆå«sigmoidå‡½æ•°ã€‚å½“ç„¶è¿™ä¸ªå‡½æ•°ä¸ä»…ä»…å±€é™äºsigmoidå‡½æ•°ï¼Œä¹Ÿæœ‰reluå‡½æ•°ï¼Œtanhå‡½æ•°ï¼š</p><script type="math/tex; mode=display">\begin{matrix}g(z) = \frac 1 {1+e^{-z}} &(sigmoid)\\g(z) = \max(z,0) &(ReLU)\\g(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}& (tanh)\end{matrix}</script><p>æˆ‘ä»¬å®šä¹‰$a_{j}^1$ä¸ºåŸå§‹è¾“å…¥ã€‚</p><p>æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‘é‡åŒ–æ¥åŠ é€Ÿç¥ç»ç½‘ç»œçš„è®¡ç®—è¿‡ç¨‹ã€‚è¿™ä¸ªåº”è¯¥ä¸ç®—å¾ˆç¨€å¥‡çš„æŠ€æœ¯ã€‚å¯¹äºæ¯ä¸€å±‚æ¥è¯´ï¼Œ$theta$éƒ½æ˜¯ä¸¥æ ¼çš„çŸ©é˜µçš„,è€Œè¾“å…¥ä¹Ÿæ˜¯ä¸€ä¸ªçŸ©é˜µã€‚æ‰€ä»¥æ¯ä¸€å±‚çš„å‘é‡åŒ–éƒ½ä¸ç®—å›°éš¾ã€‚</p><p>é€šè¿‡è¾“å…¥ï¼Œè®¡ç®—å‡ºæ¯ä¸€å±‚çš„è¾“å‡ºï¼Œç„¶åå°†è¿™å±‚è¾“å‡ºå½“ä½œä¸‹ä¸€å±‚çš„è¾“å…¥ï¼Œæœ€åå¾—åˆ°æœ€åçš„ç»“æœï¼Œè¿™å°±æ˜¯å‰å‘ä¼ æ’­ã€‚å‰å‘ä¼ æ’­å®é™…ä¸Šå°±æ˜¯ç¥ç»ç½‘ç»œæ€ä¹ˆè®¡ç®—å‡ºç»“æœçš„è¿‡ç¨‹ã€‚</p><p>åˆ©ç”¨ç¥ç»ç½‘ç»œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°å®ç°å¾ˆå¤æ‚çš„éçº¿æ€§å‡½æ•°çš„è¾¹ç•Œï¼Œä»è€Œè¿›è¡Œåˆ†ç±»ã€‚å´æ©è¾¾åœ¨è§†é¢‘ä¸­ä»‹ç»äº†ç”¨ç¥ç»ç½‘ç»œå¯¹ä¸ï¼Œæˆ–éä»¥åŠå¼‚æˆ–çš„å®ç°ã€‚ä¸è¿‡è¿™äº›éƒ½æ˜¯ç›¸å¯¹ç®€å•çš„ï¼Œå¤æ‚çš„ç¥ç»ç½‘ç»œåˆ†æå…¶æ¥å®Œå…¨æ²¡æœ‰è¿™ä¹ˆå®¹æ˜“ï¼Œè¿™ä¹Ÿæ˜¯ç¥ç»ç½‘ç»œå¼ºå¤§çš„ä¸€ä¸ªåŸå› ã€‚</p><p>ä¸ºäº†å®ç°å¤šä¸ªåˆ†ç±»ï¼Œæˆ‘ä»¬å¯ä»¥å°†æœ€åçš„è¾“å‡ºå®šä½kä¸ªï¼Œåˆ†åˆ«æ¥åˆ¤æ–­æ˜¯å¦ä¸ºå½“å‰ç±»ã€‚è¿™ä¸ªåšæ³•å®é™…ä¸Šone-Vs-allçš„åšæ³•ã€‚</p><h2 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h2><p>æ¥ä¸‹æ¥æˆ‘ä»¬æ¥è°ˆè°ˆneural networkçš„cost functionã€‚ä¹‹å‰çš„logistic regressionçš„cost functionæ˜¯é€šè¿‡è®¡ç®—æå¤§ä¼¼ç„¶ä¼°è®¡å¾—åˆ°çš„ã€‚è€Œç¥ç»ç½‘ç»œçš„cost funtionå®é™…ä¸Šå‘¢ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œä¸è¿‡å®ƒçš„h(x)ä¸å†æ˜¯ä¹‹å‰é‚£ä¹ˆç®€å•äº†ï¼Œè¿™é‡Œçš„$W$å˜æˆäº†$\Theta$ã€‚è€Œä¸”ç”±äºè¾“å‡ºå¯èƒ½æ˜¯å¤šå…ƒçš„ï¼ˆå¤šå…ƒåˆ†ç±»ï¼‰ï¼Œæ‰€ä»¥è¿™ä¸ªcost funtionå®é™…ä¸Šæ˜¯å„ä¸ªç±»åˆ«çš„cost funtionçš„å åŠ ï¼š</p><script type="math/tex; mode=display">J(\Theta) = -\frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K y_i^{(k)} \log(h_{\Theta}(X_i))_k + (1-y_i^{(k)}) \log (1 - (h_\Theta(X_i))_k)</script><p>å¦‚æœåŠ ä¸Šæ­£åˆ™åŒ–çš„è¯ï¼Œåˆ™æ­£åˆ™åŒ–é¡¹ä¸ºï¼š$\frac \lambda {2m} \sum_{l=1}^{L}\sum_{i=1}^{s_{l}}\sum_{j=1}{s_{l-1}} (\Theta_{i,j}^{(l)})^2$</p><p>éœ€è¦æ³¨æ„çš„æ˜¯è¿™ä¸ª$s_{l+1}$,ä¸ºä½•æ˜¯è¿™æ ·ï¼Ÿ$s_l$è¡¨ç¤ºäº†ç¬¬lå±‚æœ‰å¤šå°‘ç¥ç»å…ƒï¼ˆå¦‚æœl=0åˆ™è¡¨ç¤ºæœ‰å¤šå°‘ä¸ªåŸå§‹è¾“å…¥ï¼‰ï¼Œä¹Ÿå°±è¡¨ç¤ºäº†æœ‰å¤šå°‘ä¸ªè¾“å…¥å‘é‡ï¼Œè€Œè¾“å…¥å‘é‡çš„é•¿åº¦åˆ™ç”±å‰ä¸€ä¸ªè¾“å…¥çš„ä¸ªæ•°å†³å®šï¼Œå› æ­¤$s_{l-1}$å®é™…ä¸Šè¾“å…¥å‘é‡çš„é•¿åº¦ã€‚æ‰€ä»¥å‘¢ï¼Œæˆ‘ä»¬å…ˆä»å±‚æ•°å¼€å§‹ï¼Œç„¶ååˆ°è¯¥å±‚çš„æ¯ä¸ªç¥ç»å…ƒï¼Œæœ€åå†åˆ°æ¯ä¸ªå‚æ•°çš„æ¯ä¸ªå–å€¼ã€‚<br>ï¼ˆè¿™é‡Œå’Œå´æ©è¾¾çš„è¯¾ç¨‹çš„è¡¨è¿°ç¨å¾®æœ‰ç‚¹åŒºåˆ«ï¼Œä¹Ÿå°±æ˜¯æˆ‘çš„è¾“å…¥æ˜¯è¡¨ç¤ºä¸ºç¬¬0å±‚ï¼Œä¹Ÿå°±æ˜¯ä¸€å…±æœ‰L+1å±‚ï¼Œè€Œå´æ©è¾¾çš„è¯¾ç¨‹ä¸­ï¼Œå…±æœ‰Lå±‚ï¼ŒåŸæ¥çš„0å˜æˆç°åœ¨çš„ç¬¬1å±‚ã€‚ä¸è¿‡è¿™æ„å‘³ç€ï¼Œä»1å¼€å§‹è®¡æ•°ï¼Œç”±äºç¬¬ä¸€å±‚æ˜¯æ²¡æœ‰thetaå‚æ•°çš„ï¼Œæ‰€ä»¥ç¬¬ä¸€å±‚è®¡ç®—çš„å®é™…æ˜¯ä¸‹ä¸€å±‚çš„thetaï¼Œè€Œ$s_l$æ˜¯ä¸‹ä¸€å±‚çš„è¾“å…¥å‘é‡é•¿åº¦ï¼Œè€Œ$s_{l+1}$æ‰æ˜¯è¾“å…¥å‘é‡çš„ä¸ªæ•°ï¼Œè‡³äºæœ€åä¸€å±‚çš„thetaç”±äºå·²ç»åœ¨å‰ä¸€æ¬¡è®¡ç®—è¿‡äº†ï¼Œæ‰€ä»¥è¿™é‡Œçš„regularizationä¸º$\frac \lambda {2m} \sum_{l=1}^{L-1}\sum_{i=1}^{s_{l}}\sum_{j=1}{s_{l+1}} (\Theta_{j,i}^{(l)})^2$ï¼‰.</p><h2 id="å‘é‡åŒ–ï¼ˆvectorizeï¼‰"><a href="#å‘é‡åŒ–ï¼ˆvectorizeï¼‰" class="headerlink" title="å‘é‡åŒ–ï¼ˆvectorizeï¼‰"></a>å‘é‡åŒ–ï¼ˆvectorizeï¼‰</h2><p>é¦–å…ˆï¼Œä¸ºäº†æ–¹ä¾¿åé¢çš„è¯´æ˜ï¼Œæˆ‘ä»¬å®šä¹‰</p><p>$z_{j}^{(i)} = (\Theta_{j}^{(i)})^Ta^{(i-1)}$.</p><p>è€Œ$a_{j}^{(i)} = g(z_{j}^{(i)})$.</p><p>$a^{(i-1)} = [a_0^{(i-1)},a_1^{(i-1)},â€¦,a_{s_{i-1}}^{(i-1)} ]$</p><p>å¸Œæœ›å¤§å®¶è¿˜æ²¡æœ‰å¿˜è®°è¿™äº›ç¬¦å·ä»¥åŠä¸‹æ ‡çš„æ„ä¹‰ã€‚</p><p>å¦‚æœä¸€ä¸ªç¬¦å·ï¼Œåªæœ‰å±‚æ•°çš„ä¸Šæ ‡ï¼Œæ²¡æœ‰ä¸‹æ ‡ï¼Œåˆ™æ„å‘³ç€å®ƒæ˜¯ä¸€ä¸ªå‘é‡(etc.$a^{(i)},z^{(i)}$)ï¼Œæˆ–æ˜¯ä¸€ä¸ªçŸ©é˜µ$\Theta^{(i)}$.</p><p>é€šè¿‡å‘é‡åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥åƒä¸‹é¢ä¸€æ ·é€šè¿‡çº¿æ€§ä»£æ•°åº“çš„å¹¶è¡Œä¼˜åŒ–ï¼Œå¾ˆå¿«çš„è®¡ç®—å‡ºæ¥$z^{(i)}$ï¼š</p><script type="math/tex; mode=display">z^{(i)} = \Theta^{(i)} a^{(i-1)}</script><p>ä¸Šå¼ä¸­ï¼š</p><script type="math/tex; mode=display"> \Theta^{(i)}  = \begin{bmatrix} ...  (\Theta_{1}^{(i)})^T ... \\ ...  (\Theta_{2}^{(i)})^T ... \\ ...\\  ...  (\Theta_{s_i}^{(i)})^T ...  \end{bmatrix}</script><p>ä»è¿™é‡Œï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥çŸ¥é“äº†ï¼Œä¸ºä»€ä¹ˆgä¸ç”¨identify functionï¼ˆg(z) = zï¼‰.å› ä¸ºç¥ç»ç½‘ç»œçš„æå‡ºï¼Œæ˜¯ä¸ºäº†è¿›è¡Œéçº¿æ€§çš„åˆ†ç±»å’Œé¢„æµ‹ã€‚è€Œï¼š</p><script type="math/tex; mode=display">\begin{aligned}z^{(i)} &= \Theta^{(i)} a^{(i-1)}\\&=\Theta^{(i)}  g(z^{(i-1)})\\&=\Theta^{(i)}  z^{(i-1)}\\&= \Theta^{(i)}  \Theta ^{(i-1)} z^{(i-2)}\\&= \Theta^{(i)} \Theta^{(i-1)}...\Theta^{(1)} X_1\end{aligned}</script><p>è¿™æ„å‘³ç€æˆ‘ä»¬é€šè¿‡çº¿æ€§çš„å‡½æ•°æ¥åšç¥ç»ç½‘ç»œæ˜¯æ— æ³•å¾—åˆ°éçº¿æ€§çš„åˆ†ç±»ç»“æœçš„ã€‚</p><p>å¦‚æœæˆ‘ä»¬å†å¯¹è®­ç»ƒæ ·æœ¬åˆ©ç”¨å‘é‡åŒ–ï¼Œ</p><script type="math/tex; mode=display">Z^{(l)} = \Theta^{(l)} ( A^{(l-1)})^T</script><p>è¿™æ—¶å€™å‘¢ï¼ŒZå˜æˆçŸ©é˜µäº†($s_{l} \times m)$)ï¼ŒAä¹Ÿå˜æˆçŸ©é˜µäº†ï¼ˆ$ m \times s_{l-1} $ï¼‰ã€‚è€ŒZ^{(l)}å®é™…å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">Z^{(l)} =\begin{bmatrix} \vert & ... & \vert\\z^{(l)[1 ]} & ... &z^{(l)[m ]}\\ \vert & ... & \vert\end{bmatrix}</script><h2 id="åå‘ä¼ æ’­ï¼ˆback-propagationï¼‰"><a href="#åå‘ä¼ æ’­ï¼ˆback-propagationï¼‰" class="headerlink" title="åå‘ä¼ æ’­ï¼ˆback propagationï¼‰"></a>åå‘ä¼ æ’­ï¼ˆback propagationï¼‰</h2><h3 id="å‚æ•°åˆå§‹åŒ–ï¼ˆParameter-Initializationï¼‰"><a href="#å‚æ•°åˆå§‹åŒ–ï¼ˆParameter-Initializationï¼‰" class="headerlink" title="å‚æ•°åˆå§‹åŒ–ï¼ˆParameter Initializationï¼‰"></a>å‚æ•°åˆå§‹åŒ–ï¼ˆParameter Initializationï¼‰</h3><p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç¥ç»ç½‘ç»œä¸­æˆ‘ä»¬ä¸èƒ½ç®€å•åœ°å°†å‚æ•°åˆå§‹åŒ–ä¸º0.ç»è¿‡è®¡ç®—ä½ å°±ä¼šæ˜ç™½ï¼Œå¦‚æœå‚æ•°åˆå§‹åŒ–ä¸º0,åˆ™è®¡ç®—å‡ºæ¥çš„æ¢¯åº¦å°±æ˜¯ä¸€æ ·çš„ï¼Œæ— æ³•è¿›è¡Œæ¢¯åº¦ä¸‹é™,æ— è®ºæ€ä¹ˆè¿è¡Œï¼Œæœ€åå¾—åˆ°çš„ç»“æœä¸º$0.5$(sigmoid)ã€‚å¯ä»¥è¿›è¡Œéšæœºåˆå§‹åŒ–ï¼Œç»™æ¯ä¸ªå‚æ•°ä¸€ä¸ªå¾ˆå°çš„å€¼$N(0,0.1)$.</p><p>åœ¨å®é™…ä¸­ï¼Œè¯æ˜äº†æœ‰æ¯”éšæœºåˆå§‹åŒ–æ›´å¥½çš„æ–¹æ³•æ¥è¿›è¡Œåˆå§‹åŒ–ï¼šXavier/He initialization.</p><script type="math/tex; mode=display">\Theta ^{(l)} \tilde{} N\left(0,\sqrt{\frac {2}{s_l + s_{l-1}}} \right)</script><h3 id="æ¢¯åº¦ï¼ˆgradientï¼‰"><a href="#æ¢¯åº¦ï¼ˆgradientï¼‰" class="headerlink" title="æ¢¯åº¦ï¼ˆgradientï¼‰"></a>æ¢¯åº¦ï¼ˆgradientï¼‰</h3><p>åå‘ä¼ æ’­å®é™…ä¸Šæ˜¯å»ºç«‹åœ¨æ¢¯åº¦ä¸‹é™çš„åŸºç¡€ä¸Šçš„ï¼Œæ‰€ä»¥æœ€å¤æ‚çš„éƒ¨åˆ†å°±æ˜¯è®¡ç®—æ¢¯åº¦äº†ã€‚</p><p>å‡è®¾ï¼Œè¿™ä¸ªå±‚æ•°ä¸€å…±æœ‰Lå±‚ï¼Œæœ€åçš„è¾“å‡ºæ˜¯$a^{L}$ï¼Œå› æ­¤cost funtionå’ŒLå±‚çš„å‚æ•°æ˜¯ç›´æ¥ç›¸å…³çš„ã€‚æ‰€ä»¥é¦–å…ˆè®¡ç®—çš„å°±æ˜¯cost funtionå¯¹$\Theta^{(L)}$çš„æ¢¯åº¦ã€‚</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L(\Theta)}{ \partial \Theta^{(L)}} &= -\frac{\partial}{\partial \Theta^{(L)}} \left((1-y)\log (1 - y')  + y\log y'\right)\\&=-(1 - y)\frac{\partial}{\partial \Theta^{(L)}} \log (1 - g(\Theta^{(L)}a^{(L-1)}))  - y \frac{\partial}{\partial \Theta^{(L)}} \log g(\Theta^{(L)}a^{(L-1)})\\&= (1-y)\frac{1}{1 - g(\Theta^{(L)}a^{(L-1)})} g'(\Theta^{(L)}a^{(L-1)}))(a^{(L-1)})^T - y \frac {1}{g(\Theta^{(L)}a^{(L-1)}} g'(\Theta^{(L)}a^{(L-1)}(a^{(L-1)})^T\\&= (1-y)\sigma(\Theta^{(L)}a^{(L-1)})(a^{(L-1)})^T - y(1 - \sigma(\Theta^{(L)}a^{(L-1)})) (a^{(L-1)})^T\\&= (1 - y)a^{(L)} (a^{(L-1)})^T - y (1 - a^{(L)})(a^{(L-1)})^T\\&= (a^{(L)} - y)(a^{(L-1)})^T\end{aligned}</script><p>ä¸Šè¿°æ¨å¯¼è¿‡ç¨‹ä¸­ï¼Œgä¸ºsigmoidå‡½æ•°ï¼Œå› æ­¤$gâ€™ = \sigmaâ€™ = \sigma(1 - \sigma)$.</p><p>$a^{(i)}$æˆ‘ä»¬éƒ½æ˜¯å¯ä»¥é€šè¿‡å‰å‘ä¼ æ’­å¾—åˆ°çš„ã€‚</p><p>ç„¶åæˆ‘ä»¬æƒ³è¦è®¡ç®—çš„æ˜¯$\Theta{(L-1)},â€¦,\Theta{(1)}$è¿™äº›çš„æ¢¯åº¦ã€‚ä½†æ˜¯å®ƒä»¬æ˜¯å’Œ$L(\Theta)$æ˜¯æ²¡æœ‰ç›´æ¥çš„å…³ç³»çš„ã€‚ä¸è¿‡ï¼Œåœ¨å¾®ç§¯åˆ†ä¸­æ˜¯æœ‰é“¾å¼æ±‚å¯¼æ³•åˆ™çš„ï¼š</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L}{\partial \Theta^{(L-1)}} &= \frac{\partial L}{a^{(L)}} \frac{a^{(L)}}{\partial \Theta^{(L-1)}}\\ &=\frac{\partial L}{a^{(L)}} \frac{a^{(L)}}{z^{(L)}} \frac{z^{(L)}}{\Theta^{(L-1)}}\\&=\frac{\partial L}{a^{(L)}} \frac{a^{(L)}}{z^{(L)}} \frac{z^{(L)}}{a^{(L-1)}}\frac{a^{(L-1)}}{\Theta^{(L-1)}}\\&=\underbrace{\frac{\partial L}{a^{(L)}} \frac{a^{(L)}}{z^{(L)}}}_{a^{(L)} - y}\underbrace{ \frac{z^{(L)}}{a^{(L-1)}}}_{\Theta^{(L)}}\underbrace{\frac{a^{(L-1)}}{z^{(L-1)}}}_{g'(z^{(L-1)})}\underbrace{\frac{z^{(L-1)}}{\Theta^{(L-1)}}}_{a^{(L-2)}}\end{aligned}</script><p>è¿™å…¶ä¸­ï¼Œä¸€ä¸ªä¸ªçš„å¯¼æ•°éƒ½æ˜¯å¯ä»¥è®¡ç®—å‡ºæ¥çš„ã€‚å› æ­¤æˆ‘ä»¬å°±å¾—åˆ°äº†æœ€ç»ˆçš„æ¢¯åº¦ï¼š</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial L}{a^{(L)}} \frac{a^{(L)}}{z^{(L)}} \frac{z^{(L)}}{a^{(L-1)}}\frac{a^{(L-1)}}{z^{(L-1)}}\frac{z^{(L-1)}}{\Theta^{(L-1)}} = \underbrace{(a^{(L)} - y)}_{s_L\times1}\underbrace{\Theta^{(L)}}_{s_l \times s_{l-1}}\underbrace{g'(z^{(L-1)})}_{s_{l-1} \times 1}\underbrace{a^{(L-2)}}_{s_{l-2} \times 1}\end{aligned}</script><p>ä¸è¿‡éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸Šé¢å¾—åˆ°çš„å¯¼æ•°ä½ ä¼šå‘ç°çŸ©é˜µçš„ç»´åº¦å¯èƒ½ä¸åˆé€‚ã€‚å› æ­¤è¿™ä¸ªå½¢å¼å¿…é¡»è¦é‡æ–°ç»„ç»‡ä¸€ä¸‹ï¼š</p><script type="math/tex; mode=display">\underbrace{\frac{\partial L}{\partial \Theta^{(L-1)}}}_{s_{l-1} \times s_{l-2}}=\underbrace{(\Theta^{(L)})^T }_{s_{l-1} \times s_{l}}\underbrace{(a^{(L)}-y)}_{s_l \times 1} .* \underbrace{g'(z^{(L-1)})}_{s_{l-1}\times 1}\underbrace{(a^{(L-2)})^T}_{1 \times s_{l-2}}</script><p>å¦‚æœæƒ³è¦è®¡ç®—æ›´å‰é¢çš„å‚æ•°çŸ©é˜µçš„å¯¼æ•°ï¼Œè¿™ä¸ªé“¾å¼æ³•åˆ™ä¼šè¶Šæ¥è¶Šé•¿ã€‚</p><p>ä¸ºäº†æ›´å¥½çš„è®¡ç®—å„ä¸ªå±‚çš„æ¢¯åº¦ï¼Œæˆ‘ä»¬æ–°å®šä¹‰ä¸€ä¸ªç¬¦å·:</p><script type="math/tex; mode=display">\sigma^{(l)} = \nabla_{z^{(l)}}L(y,y')</script><ul><li><strong>$l = L$</strong></li></ul><p>æœ‰æ—¶å€™æˆ‘ä»¬å¯ä»¥ç›´æ¥è®¡ç®—å‡ºæ¥$\nabla_{z^{(L)}}L(y,yâ€™)$(gä¸ºsoftmaxå‡½æ•°)ï¼Œæœ‰æ—¶å€™éœ€è¦ä½¿ç”¨é“¾å¼æ³•åˆ™:</p><p>$\nabla_{z^{(L)}}L(y,yâ€™) = \nabla_{yâ€™}L(yâ€™,y) .* gâ€™(z^{(L)})$</p><ul><li><strong>$l \ne L$</strong></li></ul><p>$\sigma^{l} = ((\Theta^{(l+1))^T}\sigma^{(l+1)}) .*gâ€™(z^{(l)})$</p><ul><li>$\nabla _{\Theta^{(l)}} L = \sigma^{(l)}(a^{(l-1)})^T$</li></ul><p>é€šè¿‡éªŒè¯ä½ ä¼šå‘ç°å®é™…ä¸Šä¸Šé¢è¯´çš„æ­£æ˜¯æˆ‘ä»¬æ¨å¯¼çš„å†…å®¹ã€‚</p><p>ä½¿ç”¨æ¢¯åº¦ä¸‹é™,æˆ–è€…SGDï¼ˆæ›´åŠ å¸¸ç”¨ï¼‰ï¼Œæœ€ç»ˆæ±‚å¾—åˆé€‚çš„$\Theta$.</p><p>å¯ä»¥çœ‹åˆ°çš„$\sigma^{(l)}$çš„è®¡ç®—ï¼Œéœ€è¦çš„æ˜¯åå‘è®¡ç®—ï¼Œæ‰€ä»¥è¿™ä¸ªå«åå‘ä¼ æ’­ã€‚</p><p>ä¸Šé¢çš„æ¨å¯¼è¿‡ç¨‹éƒ½æ˜¯ä»¥ä¸€ä¸ªtraining exampleçš„ï¼Œå¯¹äºå¤šä¸ªæ ·æœ¬å¯ä»¥é€šè¿‡å‘é‡åŒ–ä»¥åŠçŸ©é˜µåŒ–æ¥åŠ å¿«å®ç°ã€‚</p><p>å‚è€ƒæ–‡çŒ®ï¼š</p><p><a href="http://cs229.stanford.edu/notes/cs229-notes-deep_learning.pdf" target="_blank" rel="noopener">css229:deep learning</a></p><p><a href="http://cs229.stanford.edu/notes/cs229-notes-backprop.pdf" target="_blank" rel="noopener">css229:back-prop</a></p>]]></content>
      
      
      <categories>
          
          <category> æ•°æ®å­¦ä¹ è¯¾ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LFD class </tag>
            
            <tag> deep learning </tag>
            
            <tag> neural network </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”Kernel Logistic Regression</title>
      <link href="/2018/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Kernel-Logistic-Regression/"/>
      <url>/2018/11/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Kernel-Logistic-Regression/</url>
      
        <content type="html"><![CDATA[<p>åœ¨æˆ‘ä»¬çš„ç”Ÿæ´»ä¸­ï¼Œå…¶å®å¤§éƒ¨åˆ†ä½¿ç”¨çš„éƒ½æ˜¯soft-margin SVMï¼Œå¾ˆå°‘ä¼šæœ‰äººçœŸæ­£å»ä½¿ç”¨hard-marginï¼Œå› ä¸ºæˆ‘ä»¬æ— æ³•é¿å…å™ªå£°ã€‚ç°åœ¨æƒ³æƒ³ï¼Œèƒ½å¦å°†soft-margin svmä¸æˆ‘ä»¬ä¹‹å‰çš„losgistic regressç»“åˆèµ·æ¥ï¼Œä¼šå¾—åˆ°ä»€ä¹ˆæ ·çš„å­¦ä¹ ç®—æ³•ï¼Ÿ<br><a id="more"></a><br>ä¹‹å‰æˆ‘ä»¬è¯•ç€å°†soft-margin SVMçš„æ•°å­¦æè¿°å†™æˆäº†å¦å¤–ä¸€ç§å½¢å¼ï¼š</p><p><strong>min</strong>  $\frac 1 2 W^TW + C\sum_{n=1}^N max\{1-y_n(W^TX_n+b),0\}.$</p><p>å®é™…ä¸Šï¼Œè¿™ä¸ªå½¢å¼å¦‚æœä½ ä»”ç»†è§‚å¯Ÿè¿™ä¸ªå½¢å¼ï¼Œå°±ä¼šå‘ç°å®é™…ä¸Šå®ƒå’Œlosgistic regressionåŠ ä¸ŠL2æ­£åˆ™åŒ–ä¹‹åçš„å½¢å¼éå¸¸çš„ç›¸ä¼¼ï¼š</p><div class="table-container"><table><thead><tr><th>algorithm</th><th>minimize</th><th>constraint</th></tr></thead><tbody><tr><td> regularization by constraint</td><td>$E_{in}$</td><td>$W^TW \leq C$</td></tr><tr><td> hard-margin SVM</td><td>$W^TW$</td><td>$E_{in} = 0$[and more]</td></tr><tr><td> L2 regularization</td><td>$\frac \lambda N W^TW + E_{in}$</td><td></td></tr><tr><td> soft-margin SVM</td><td>$\frac 1 2 W^TW + CN\hat{E_{in}}$</td><td></td></tr></tbody></table></div><p>åªä¸è¿‡ä¹‹å‰æˆ‘ä»¬ç”¨$\lambda$ï¼Œè€Œç°åœ¨é‡‡ç”¨çš„æ˜¯ä¸€ä¸ªå¸¸æ•°Cã€‚æˆ‘ä»¬çŸ¥é“ï¼ŒCè¶Šå¤§ï¼Œä¹Ÿå°±æ˜¯å¯¹åº”çš„$\lambda$è¶Šå°ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†$ max\{1-y(W^TX_n+b),0\}$çœ‹ä½œä¸ºä¸€ç§error measurementã€‚</p><p>å¦‚æœç”»å‡ºè¿™å‡ ä¸ªé”™è¯¯çš„æ›²çº¿ä¸0\1é”™è¯¯çš„å¯¹æ¯”ï¼š</p><script type="math/tex; mode=display"></script><p>å®é™…ä¸Šï¼Œè¿™ä¸¤ä¸ªå‡½æ•°æ˜¯éå¸¸æ¥è¿‘çš„ã€‚å½“$y_n(W^TX_n+b)$å¾ˆå¤§çš„æ—¶å€™ï¼Œlogistic regressionçš„$E_{in}(\log_2^{1+e^{-ys}})$å’Œ SVMçš„$\hat{E_{in}}(max\{1-y(W^TX_n+b),0\})$éƒ½æ˜¯è¶‹äº0çš„,è€Œå½“$y_n(W^TX_n+b)$éå¸¸å°ï¼ˆè¿œå°äº0ï¼‰çš„æ—¶å€™ï¼Œå®ƒä»¬çš„$E_{in}$æœ‰éƒ½è¶‹äº$\vert y_n(W^TX_n+b) \vert$.</p><p>æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è§‰å¾—ï¼Œå®é™…ä¸ŠSVMå’Œlogistic regression with L2 regularizationå‡ ä¹åœ¨åšä¸€æ ·çš„äº‹æƒ…ã€‚</p><p>ç°åœ¨æˆ‘ä»¬å¸Œæœ›å¯ä»¥å°†äºŒè€…ç»“åˆï¼Œä¾‹å¦‚æˆ‘ä»¬ç”¨SVMçš„å€¼æ¥é¢„æµ‹æ¦‚ç‡ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å¾—åˆ°ä¸é”™çš„ç»“æœã€‚ä½†æ˜¯è¿™ä¸ªå®é™…ä¸Šæ²¡æœ‰ç”¨åˆ°logisticã€‚æˆ–è€…æˆ‘ä»¬ç”¨SVMçš„ç»“æœæ¥åšLogitsitc regressionçš„åˆå§‹å€¼ï¼Œä½†æ˜¯æ—¢ç„¶logistic regressionçš„$E_{in}$æ˜¯å‡¸å‡½æ•°ï¼Œå› æ­¤å®é™…ä¸Šå¾—åˆ°çš„æœ€ç»ˆç»“æœåŒºåˆ«ä¹Ÿä¸å¤§ï¼Œä¹Ÿå°±æ˜¯å®é™…ä¸Šå°±åƒæ²¡æœ‰ç”¨åˆ°SVMã€‚</p><h3 id="Plattâ€™s-Model"><a href="#Plattâ€™s-Model" class="headerlink" title="Plattâ€™s Model"></a>Plattâ€™s Model</h3><p>æœ‰ä¸€ç§è¿™æ ·çš„æ–¹æ³•ï¼Œå°†ç›®æ ‡å‡½æ•°å†™ä¸ºï¼š$g(X) = \theta (A(W_{SVM}^T \phi(X) + b_{SVM}) +B)$</p><p>å¯¹ä¸Šé¢çš„å‡½æ•°è¿›è¡ŒLogistic Regressionã€‚</p><p>æ‰€ä»¥è¿™æ—¶å€™çš„Cost Functionå˜æˆï¼š</p><script type="math/tex; mode=display">min_{A,B} \frac 1 N \sum_{n=1}^N \log\left( 1+\exp\left( -y_n(A\cdot(W_{SVM}^T\phi(X_n)+b_{SVM})+B)\right)\right)</script><p>è¿™æ—¶å€™çš„cost funtion å¥½åƒçœ‹ä¸Šå»éå¸¸å¤æ‚ï¼Œä½†æ˜¯ä»”ç»†æƒ³ä¸€æƒ³çš„è¯ï¼Œå®é™…ä¸Šæ—¢ç„¶æˆ‘ä»¬å·²ç»æœ‰äº†SVMçš„ç»“æœï¼Œå› æ­¤å®é™…ä¸Š$W_{SVM}^T\phi(X_n)+b_{SVM}$å°±æ˜¯ä¸€ä¸ªå€¼ï¼Œè€Œä¸å†æ˜¯ä¸€ä¸ªå‘é‡ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬åªéœ€è¦ä¸¤ä¸ªæ•°çš„å€¼ï¼šAï¼ŒBã€‚å°±å¯ä»¥èåˆSVMå’ŒLogistic Regressionã€‚</p><p>å½“ç„¶ï¼Œå¦‚æœSVMåšçš„å¥½çš„è¯ï¼ŒAçš„å€¼åº”è¯¥æ˜¯å¤§äº0çš„ï¼ŒBçš„å€¼åº”è¯¥æ˜¯æ¥è¿‘äº0çš„ï¼Œå› ä¸ºæˆ‘ä»¬å¾—åˆ°çš„æœ€ç»ˆçš„å‚æ•°åˆ†åˆ«ä¸º$AW_{SVM}^T$ä¸$Ab_{SVM}+AB$ï¼Œåˆ†åˆ«å¯¹åº”æœ€ç»ˆçš„Wå’Œbï¼Œå¦‚æœSVMåšçš„ä¸é”™ï¼Œæ„å‘³ç€ä»–ä»¬å’Œ$W_{SVM},b_{SVM}$å·®è·å¤ªå¤§ã€‚</p><p>ä¸Šè¿°ä¸­$\phi(X)$æ„å‘³ç€åˆ©ç”¨äº†ç‰¹å¾è½¬æ¢ï¼Œä¹Ÿå°±æ˜¯ä¼šä½¿ç”¨kernelã€‚å¯ä»¥å¾—åˆ°æ¯”è¾ƒä¸é”™çš„logistic regressionåœ¨zç©ºé—´ä¸é”™çš„è§£ã€‚</p><p>ä½†æ˜¯ä¸Šé¢è¿™ä¸ªåŠæ³•ï¼Œä¸èƒ½ä¿è¯è¿™æ˜¯logistic regressionåœ¨Zç©ºé—´ï¼ˆè½¬æ¢ä¹‹åçš„ç©ºé—´ï¼‰çœŸæ­£æœ€å¥½çš„è§£ã€‚</p><h3 id="Kernal-Logistic-Regression"><a href="#Kernal-Logistic-Regression" class="headerlink" title="Kernal Logistic Regression"></a>Kernal Logistic Regression</h3><p>æƒ³è¦æ‰¾åˆ°logistic regressionåœ¨Zç©ºé—´çœŸæ­£æœ€å¥½çš„è§£ï¼Œä¸€ä¸ªåŠæ³•æ˜¯åœ¨Zç©ºé—´åšlogistic regressionã€‚ä½†æ˜¯æˆ‘ä»¬çš„è½¬æ¢å®é™…ä¸Šæ˜¯ç”±kernelæä¾›çš„ï¼Œæ–¹ä¾¿è®¡ç®—$Z_n^TZ_m$ã€‚è€Œlogistic regressionæ ¹æœ¬å°±ä¸æ˜¯äºŒæ¬¡è§„åˆ’é—®é¢˜ï¼Œåˆå¦‚ä½•ç”¨åˆ°kernelï¼Ÿ</p><p>å…¶å®ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨æ±‚çš„ä¸œè¥¿æ˜¯$W$ï¼Œ$W$çš„ç»´åº¦æ˜¯å’Œ$Z$çš„ç»´åº¦ä¸€æ ·ï¼Œé‚£ä¹ˆ$W$æ˜¯ä¸æ˜¯$Z$çš„çº¿æ€§ç»„åˆå‘¢ï¼Ÿ</p><p>åœ¨SVMä¸­ï¼Œæ­£æ˜¯è¿™æ ·ï¼Œè¿˜è®°å¾—$W$æ€ä¹ˆç®—å—ï¼š</p><script type="math/tex; mode=display">W = \sum_{n=1}^N \alpha_n y_nX_n</script><p>åŒæ ·çš„ï¼Œåœ¨PLAï¼ŒLogistic Regressionä¸­ä¹Ÿæ˜¯è¿™æ ·ã€‚å‡å¦‚æˆ‘ä»¬æƒ³æƒ³$W$çš„åˆå§‹å€¼ä¸º0,é‚£ä¹ˆæ¯æ¬¡æ›´æ–°æ­¥éª¤ä¸å°±æ˜¯ä¸€ä¸ªç³»æ•°ä¹˜ä¸Šä¸€ä¸ª$X_i$çš„çº¿æ€§ç»„åˆå—ï¼Ÿ</p><p>å®é™…ä¸Šï¼Œåœ¨L2çš„regularizationä¸­ï¼Œæœ€å¥½çš„Wéƒ½æ˜¯å¯ä»¥è¢«Zçº¿æ€§ç»„åˆè¡¨ç¤ºå‡ºæ¥çš„ã€‚</p><p>å¦‚ä½•è¯æ˜è¿™ä»¶äº‹æƒ…ï¼Ÿ</p><p>æˆ‘ä»¬å°†optimal $W$ å†™ä¸º$W^*$.<br>å…¶ä¸­$W^*$å¯ä»¥è¡¨ç¤ºä¸ºå‚ç›´Xç©ºé—´çš„ä¸å¹³è¡ŒXç©ºé—´ï¼ˆçº¿æ€§ç»„åˆå³ä¸ºå¹³è¡Œï¼‰çš„ã€‚</p><p>$W^* = W_{\Vert} + W_{\perp}$</p><p>æˆ‘ä»¬å¯ä»¥è¯æ˜çš„æ˜¯ï¼Œå½“$W^*$ä¸­$W^{\perp}$ä¸º0.</p><p>å¦‚æœ$W_{\perp}!=0$,åˆ™$W^{*T}X = W_{\Vert}^T X + 0$ã€‚</p><p>ä»å¦ä¸€æ–¹é¢æ¥è¯´ï¼š<br>$W^{ * T}W^* = W_{\Vert} ^T W_{\Vert} + W_{\perp}^TW_{\perp} &gt;W_{\Vert} ^T W_{\Vert} $</p><p>è¿™è¯´æ˜$W_{\Vert}$æ¯”$W^*$æ˜¯æ›´å¥½çš„é€‰æ‹©ï¼Œä¸å‡è®¾çŸ›ç›¾ã€‚</p><p>æ‰€ä»¥ï¼Œ$W^{*} = \sum_{n=1}^N\beta_nZ_n$.</p><p>å› æ­¤ï¼Œæˆ‘ä»¬å°†Wæ¢æˆä¸Šå¼ï¼Œé‚£ä¹ˆå°±ä¼šå‡ºç°äº†æˆ‘ä»¬æƒ³è¦çš„$Z_nZ_m$.</p><p>L2 regularizationå˜æˆä¸‹é¢çš„æ ·å­ï¼š</p><script type="math/tex; mode=display">min_{W} \frac \lambda N \sum_{n=1}^N\sum_{m=1}^N \beta_n\beta_m Z_n^TZ_m + \frac 1 N \sum_{n=1}^N \log \left(1 + \exp\left(-y_n \sum_{m=1}^N \beta_m Z_m Z_n \right) \right)</script><p>æˆ‘ä»¬å¯ä»¥è½»æ˜“å°†ä¸Šå¼ä¸­çš„$Z_n^TZ_m$æ¢æˆ$k(x_n,x_m)$.ä»è€Œå®ç°åœ¨zç©ºé—´ä¸Šlogistic regressionçš„æœ€ä¼˜è§£ã€‚</p><p>ä»å¦ä¸€æ–¹é¢æ¥è¯´ï¼Œ$ \sum_{n}^N\sum_{m}^N \beta_n\beta_m k(x_n,_m) = \beta ^T K \beta$,å¯ä»¥å°†å®ƒçœ‹æˆä¸€ç§ç‰¹æ®Šçš„æ­£åˆ™åŒ–ã€‚æ‰€ä»¥å®é™…ä¸Šæˆ‘ä»¬å¯ä»¥å°†KLRçœ‹æˆå…³äºè½¬æ¢åçš„æ•°æ®åœ¨$\beta$ä¸Šçš„çº¿æ€§æ¨¡å‹ï¼ˆåŸæ¥æ˜¯å…³äºWçš„çº¿æ€§æ¨¡å‹ï¼‰ã€‚</p><p>ä¸€èˆ¬æ¥è¯´$beta_n$éƒ½ä¸ä¸º0,æ‰€ä»¥è¿™ä¸ªå’ŒSVMä¸­çš„$\alpha_n$ä¸ä¸€æ ·ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> regression </tag>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ä¿¡æ¯è®ºâ€”â€”Lossless Encoding</title>
      <link href="/2018/11/02/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94Lossless-Encoding/"/>
      <url>/2018/11/02/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94Lossless-Encoding/</url>
      
        <content type="html"><![CDATA[<p>ä¿¡æ¯è®ºç®—æ˜¯åº”ç”¨æ•°å­¦ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›ç”¨ç†µï¼Œäº’ä¿¡æ¯è¿™äº›ä¸œè¥¿æ¥è§£å†³ä¸€äº›å®é™…çš„é—®é¢˜ã€‚é¦–å…ˆä»‹ç»ä¸‹æ— å¤±çœŸç¼–ç å®šç†ï¼Œå®ƒæ—©å·²ç»è¢«å¹¿æ³›ç”¨åœ¨æˆ‘ä»¬ç”Ÿæ´»å½“ä¸­äº†ã€‚<br><a id="more"></a></p><p>é¦–å…ˆï¼Œè¯´åˆ°æ— å¤±çœŸç¼–ç ï¼Œæˆ‘ä»¬é¦–å…ˆæƒ³åˆ°çš„æ˜¯æ— æŸå‹ç¼©äº†ã€‚æ— æŸå‹ç¼©å®é™…ä¸Šæ˜¯ä¸€ä¸ªæœ€å¤§ç†µçš„é—®é¢˜ã€‚è¿™æ ·çš„æƒ…å†µä¸‹ï¼Œèƒ½åŒ…å«æœ€å¤šçš„ä¿¡æ¯ï¼Œå¦‚æœä¿¡æ¯é‡ä¸€å®šï¼Œä¹Ÿå°±æ˜¯æœ€å¤§ç†µçš„æƒ…å†µä¸‹éœ€è¦å¹³å‡è¾ƒå°çš„æ¯”ç‰¹æ•°ï¼ˆæ¯”å¦‚ä¸€ä¸ªå‡åŒ€åˆ†å¸ƒéšæœºå˜é‡Xçš„ç†µä¸º3,å¦ä¸€ä¸ªéå‡åŒ€åˆ†å¸ƒéšæœºå˜é‡ç†µYä¹Ÿä¸º3ï¼Œé‚£ä¹ˆ|Y|&gt;|X|,å¦‚æœæ™®é€šç¼–ç çš„è¯ï¼ŒYçš„ç¼–ç æ›´é•¿ï¼Œä½†æ˜¯å®ƒä»¬åŒ…å«çš„ä¿¡æ¯é‡å´æ˜¯ä¸€æ ·çš„ï¼‰ã€‚æˆ‘ä»¬éƒ½çŸ¥é“çš„æ˜¯ï¼Œåœ¨å‡åŒ€åˆ†å¸ƒçš„æ—¶å€™ç†µæ˜¯æœ€å¤§çš„ã€‚ä½†æ˜¯æˆ‘ä»¬ä¸èƒ½é€‰æ‹©ä¿¡æºçš„åˆ†å¸ƒï¼Œå› ä¸ºä¿¡æºå°±åœ¨é‚£é‡Œå·²ç»ç¡®å®šäº†ã€‚æˆ‘ä»¬èƒ½å¦é€šè¿‡ä¸€ä¸ªæ˜ å°„ï¼Œä¸€ä¸€å¯¹åº”çš„æ˜ å°„ï¼Œä½¿å¾—ä¸€ä¸ªéç­‰æ¦‚åˆ†å¸ƒé€æ¸èµ°å‘ç­‰æ¦‚å‘¢ï¼Ÿç­”æ¡ˆæ˜¯ï¼Œå¯ä»¥ï¼Œä½†æ˜¯è¿™å¤ªåç›´è§‰äº†ã€‚æ˜¯å•Šè™½ç„¶åç›´è§‰ï¼Œä½†æ˜¯å®ƒä¸åæ•°å­¦ï¼Œæ‰€ä»¥å®ƒå°±æ˜¯å¯¹çš„ã€‚</p><h2 id="æ¸è¿›ç­‰åŒåˆ†å‰²æ€§è´¨ï¼ˆAsymptotic-Equipartition-Propertyï¼‰"><a href="#æ¸è¿›ç­‰åŒåˆ†å‰²æ€§è´¨ï¼ˆAsymptotic-Equipartition-Propertyï¼‰" class="headerlink" title="æ¸è¿›ç­‰åŒåˆ†å‰²æ€§è´¨ï¼ˆAsymptotic Equipartition Propertyï¼‰"></a>æ¸è¿›ç­‰åŒåˆ†å‰²æ€§è´¨ï¼ˆAsymptotic Equipartition Propertyï¼‰</h2><h3 id="å¤§æ•°å®šå¾‹ï¼ˆLaw-of-Large-Numberï¼‰"><a href="#å¤§æ•°å®šå¾‹ï¼ˆLaw-of-Large-Numberï¼‰" class="headerlink" title="å¤§æ•°å®šå¾‹ï¼ˆLaw of Large Numberï¼‰"></a>å¤§æ•°å®šå¾‹ï¼ˆLaw of Large Numberï¼‰</h3><p>è¿™é‡Œå…ˆä¼¯å¥´åˆ©å¤§æ•°å®šå¾‹(å±äºå¼±å¤§æ•°å®šå¾‹)ã€‚å®é™…ä¸Šæ‰€æœ‰çš„å¤§æ•°å®šå¾‹éƒ½åœ¨è¯´ä¸€ä»¶äº‹ï¼šå½“å®éªŒæ¬¡æ•°éå¸¸å¤§çš„æ—¶å€™ï¼Œé¢‘ç‡è¶‹å‘äºæ¦‚ç‡ï¼ˆç»éªŒåˆ†å¸ƒé€¼è¿‘äºç»Ÿè®¡åˆ†å¸ƒï¼‰ã€‚</p><script type="math/tex; mode=display">\frac{S_n}{n} \underrightarrow{p} p</script><p>æ›´ç²¾ç¡®ä¸€ç‚¹çš„è¯´æ³•ï¼š</p><script type="math/tex; mode=display">\forall N, \exists \epsilon >0,\sigma>0, \text{where }n > N,p(\vert \frac{s_n}{n} - p\vert \geq \epsilon) <\sigma.</script><p>å¦å¤–ä¸€ä¸ªå¤§æ•°å®šç†ï¼ˆè¾›é’¦å¤§æ•°å®šå¾‹ï¼‰ï¼š</p><script type="math/tex; mode=display">\begin{align}\frac 1 n \sum_{i=1}^n X_i\underrightarrow{p} EX\end{align}</script><p>æ¸è¿›ç­‰åŒåˆ†å‰²æ€§è´¨å®šä¹‰å¦‚ä¸‹ï¼š</p><p>å¦‚æœ$X_1,X_2,â€¦$æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒçš„ç¦»æ•£éšæœºå˜é‡ï¼Œåˆ†å¸ƒæœä»$p(x)$,åˆ™</p><p>$-\frac 1 n \log p(X_1,X_2,â€¦,X_n) \underrightarrow{p} H(X)$</p><p>ä½¿ç”¨ä¸Šé¢çš„æ›´å‡†ç¡®çš„å†™æ³•å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\forall N, \exists \epsilon>0, \sigma >0, \text{where }n > N,p(\vert - \frac 1 n \log p(X_1,X_2,...,X_n) - H(X)\vert \geq \epsilon) <\sigma.</script><p>å³$p(X_1,X_2,â€¦,X_n)\approx 2^{-nH(X)}$.</p><p>è¿™ä¸ªå®šç†å¯ä»¥ä½¿ç”¨å¼±å¤§æ•°å®šç†åœ°è¯æ˜ï¼š</p><script type="math/tex; mode=display">\begin{aligned}&-\frac 1 n \log p(X_1,X_2,...,X_n)\\& = -\frac 1 n \log p(X_1)p(X_2)...p(X_n)\\&=-\frac 1 n \sum_{i=1}^n \log p(X_i)\end{aligned}</script><p>å› ä¸ºæˆ‘ä»¬çŸ¥é“ï¼Œ$X_1,X_2,â€¦,X_n$æ˜¯äº’ç›¸ç‹¬ç«‹çš„ï¼Œå› æ­¤ï¼Œ$\log X_1,\log X_2,â€¦,\log X_n$ä¹Ÿæ˜¯äº’ç›¸ç‹¬ç«‹åŒåˆ†å¸ƒçš„ã€‚åˆ©ç”¨(1)ï¼š</p><script type="math/tex; mode=display">\begin{aligned}-\frac 1 n \sum_{i=1}^n \log p(X_i) &= -E(\log p(X))\\ &= \sum_{x \in \mathcal{X}} (-p(x)\log p(x)) \\&= H(X)\end{aligned}</script><p>è¿™æ„å‘³ç€ï¼Œå½“nå¾ˆå¤§çš„æ—¶å€™ï¼Œä¸€ä¸ªåºåˆ—å‡ºç°çš„æ¦‚ç‡æ˜¯å‡ ä¹ç›¸ç­‰çš„ï¼Œè¿™ä¸ªæ¦‚ç‡ä¸º$2^{-nH(X)}$.</p><h2 id="ï¼ˆå¼±ï¼‰å…¸å‹åºåˆ—ï¼ˆTypical-Sequenceï¼‰"><a href="#ï¼ˆå¼±ï¼‰å…¸å‹åºåˆ—ï¼ˆTypical-Sequenceï¼‰" class="headerlink" title="ï¼ˆå¼±ï¼‰å…¸å‹åºåˆ—ï¼ˆTypical Sequenceï¼‰"></a>ï¼ˆå¼±ï¼‰å…¸å‹åºåˆ—ï¼ˆTypical Sequenceï¼‰</h2><p>å…¸å‹åºåˆ—å®šä¹‰å¦‚ä¸‹ï¼š<br>ç›¸å¯¹äºåˆ†å¸ƒ$p(x)$å’Œåºåˆ—$(x_1,x_2,â€¦,x_n) \in X_n$,å…¸å‹åºåˆ—é›†åˆ$A_\epsilon ^ {(n)}$å®šä¹‰ä¸ºæ»¡è¶³ä¸‹åˆ—ä¸ç­‰å¼çº¦æŸçš„æ‰€æœ‰åºåˆ—$\mathbf{x}$çš„é›†åˆï¼š</p><script type="math/tex; mode=display">2^{-n(H(X)+\epsilon) \leq p(\mathbf{x}) = p(x_1,x_2,...,x_n) \leq 2^{-n(H(X)-\epsilon)}}</script><p>æ‰€ä»¥å…¸å‹åºåˆ—å…·æœ‰ä»¥ä¸‹æ€§è´¨ï¼š</p><ol><li>è‹¥$\mathbf{x} \in A_\epsilon ^ {(n)}$,åˆ™$H(X)-\epsilon \leq -\frac 1 n \log p(\mathbf{x}) \leq H(X) + \epsilon$.</li><li>è‹¥$n$è¶³å¤Ÿå¤§ï¼Œ$Pr(A_\epsilon ^{(n)}) \geq 1 - \epsilon $</li><li>$\vert A_\epsilon ^{(n)}\vert \leq 2^{n(H(X)+\epsilon)}$</li><li>$\vert A_\epsilon ^{(n)}\vert \geq (1-\epsilon)2^{n(H(X)-\epsilon)}$</li></ol><p>æ€§è´¨1,2å¯ä»¥ç”¨å®šä¹‰å¾—åˆ°ã€‚å› æ­¤è¿™é‡Œè¯æ˜3å’Œ4.</p><p>3.</p><script type="math/tex; mode=display">\begin{aligned}1 &= \sum_{x^n \in \mathcal{X}}P(x^n)\\&\geq \sum_{x^n \in A_\epsilon ^{(n)}} p(x^n)\\&\geq \sum_{x^n \in A_\epsilon ^{(n)}} 2^{-n(H(x)+\epsilon)}\\&= 2^{-n(H(x)+\epsilon)}\vert A_\epsilon ^{(n)} \vert\end{aligned}</script><p>4çš„è¯æ˜é¦–å…ˆè¦ä½¿ç”¨æ€§è´¨2ã€‚</p><script type="math/tex; mode=display">\begin{aligned}1 - \epsilon &\leq Pr\{A_\epsilon ^{(n)}\}\\&\leq \sum_{x^n \in A_\epsilon ^{(n)}} 2 ^{-n(H(X) - \epsilon)}\\&= 2 ^{-n(H(X) - \epsilon)} \vert A_\epsilon ^{(n)}\vert \end{aligned}</script><p>æ‰€æœ‰å¯èƒ½å‡ºç°çš„åºåˆ—ä¸€å…±æœ‰$|X|^n$ç§ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œ$ 2^{m(H(X)+\epsilon)} &lt;&lt; |X|^n$.æ‰€ä»¥å…¸å‹åºåˆ—é›†åˆåªæ˜¯æ‰€æœ‰å¯èƒ½é›†åˆçš„ä¸€ä¸ªå¾ˆå°ï¼ˆå°¤å…¶æ˜¯åŸæ¥åˆ†å¸ƒè¿œç¦»å‡åŒ€åˆ†å¸ƒçš„æ—¶å€™ï¼‰çš„å­é›†ã€‚ä½†æ˜¯å®ƒå‡ ä¹ä¸€å®šä¼šå‡ºç°ï¼Œè€Œä¸”æ¯ä¸ªå…¸å‹åºåˆ—å‡ºç°çš„æ¦‚ç‡å‡ ä¹ä¸€æ ·ã€‚è¿™æ˜¯å¾ˆå¥½çš„æ¶ˆæ¯ï¼Œä¸ºæˆ‘ä»¬åˆšå¼€å§‹æå‡ºæ¥çš„æ˜ å°„æä¾›äº†å¾ˆå¥½çš„æ€è·¯ã€‚</p><h2 id="å®šé•¿ç¼–ç å®šç†-é¦™å†œç¬¬ä¸€å®šç†"><a href="#å®šé•¿ç¼–ç å®šç†-é¦™å†œç¬¬ä¸€å®šç†" class="headerlink" title="å®šé•¿ç¼–ç å®šç†(é¦™å†œç¬¬ä¸€å®šç†)"></a>å®šé•¿ç¼–ç å®šç†(é¦™å†œç¬¬ä¸€å®šç†)</h2><p>å‡è®¾$X^n$æ˜¯ç”±ç‹¬ç«‹åŒåˆ†å¸ƒç¦»æ•£éšæœºå˜é‡$X\tilde{}p(X)$æ„æˆçš„åºåˆ—ã€‚å¯¹äºä»»æ„æ­£æ•°$\epsilon$ï¼Œæ€»æœ‰è¶³å¤Ÿå¤§çš„nï¼Œå¯ä»¥æ‰¾åˆ°ä¸€ä¸ªä¸€ä¸€æ˜ å°„ï¼Œå°†$X^n$æ˜ å°„åˆ°äºŒè¿›åˆ¶åºåˆ—ï¼Œä¸”æ»¡è¶³:</p><script type="math/tex; mode=display">E\left[\frac 1 n l(X^n) \right] \leq H(X)+\epsilon</script><p>ä¸Šå¼ä¸­ï¼Œ$l(X^n)$è¡¨ç¤ºçš„æ˜¯ç¼–ç éœ€è¦çš„bitæ•°ã€‚</p><p>æ¥ä¸‹æ¥æä¾›è¯æ˜ï¼š</p><script type="math/tex; mode=display">\begin{aligned}E(l(X^n)) &= \sum_{x \in \mathcal{X}} p(x) l(x)\\&= \sum_{x \in A_\epsilon ^{(n)}} p(x) l(x) + \sum_{x \in \overline{A_\epsilon ^{(n)}}} p(x) l(x)\\& \leq \sum_{x \in A_\epsilon ^{(n)}} p(x) [ n(H(X)+\epsilon)+1+1 ] + \sum_{x \in \overline{A_\epsilon ^{(n)}}} p(x) [ n\log \vert X\vert +2 ]\\&=[ n(H(X)+\epsilon)+1+1 ] Pr\{ A_\epsilon ^{(n)}\} +  [ n\log \vert X\vert +2] Pr\{\overline{A_\epsilon ^{(n)}}\} \\&\leq n(H(X)+\epsilon)+2 +  n\epsilon \log\vert X\vert+2\epsilon= n(H(X) + \epsilon ')\end{aligned}</script><p>å…¶ä¸­ $\epsilonâ€™ = \epsilon + \epsilon \log \vert X \vert +\frac 2 n + \frac {2\epsilon}{n}$,å¯ä»¥çœ‹åˆ°çš„æ˜¯$n \rightarrow \infty,\epsilonâ€™ \rightarrow 0$.</p><p>ä¸Šé¢è¯æ˜è¿‡ç¨‹ä¸­å€¼å¾—æ³¨æ„çš„äº‹æƒ…æ˜¯ï¼Œä¸ºä»€ä¹ˆè¦åŠ 2ï¼Ÿç¬¬ä¸€ä¸ªåŠ ä¸€æ˜¯ä¸ºäº†å¤„ç†logåä¸ºéæ•´æ•°çš„æƒ…å†µï¼Œç¬¬äºŒä¸ª+1æ˜¯ç•™ä¸€ä¸ªæ¯”ç‰¹ä½ç½®æ¥åŒºåˆ†å…¸å‹åºåˆ—ä¸éå…¸å‹åºåˆ—çš„ç¼–ç ã€‚</p><p>ä½†æ˜¯ï¼Œå®šé•¿ç¼–ç å®šç†æ˜¯æ— æ³•åº”ç”¨åˆ°å·¥ä¸šç•Œçš„ã€‚å› ä¸ºå®ƒéœ€è¦å¯¹åºåˆ—é•¿åº¦ä¸ºnæ¥è¿›è¡Œç¼–ç ï¼Œç”±äºç²¾ç¡®åº¦çš„è¦æ±‚ï¼Œè¿™ä¸ªnå¾€å¾€å¾ˆå¤§ï¼ˆä¸Šäº¿ï¼‰ï¼Œè¿™åœ¨ç°å®ä¸­æ˜¯æ— æ³•å®ç°çš„ã€‚</p><p>é¡ºä¾¿æˆ‘ä»¬è¯æ˜ä¸€ä¸‹ï¼Œå¹³å‡æ¯ä¸ªå­—ç¬¦ç¼–ç æ‰€éœ€è¦çš„bitæ•°ä¸€å®šæ˜¯å¤§äºç­‰äº$H(X)$.</p><p>å‡è®¾$X^n \rightarrow M \in C = \underbrace{\{1,2,3,â€¦,2^{nR}\}}_{need ~nR~ bits} \rightarrow \hat{X}^n$.</p><p>æ„æ€å°±æ˜¯ï¼ŒMä¸ºX^nç¼–ç åçš„ç»“æœï¼Œå®ƒä¸€å®šæ˜¯å±äºåé¢çš„æŸä¸ªæ•°å­—ã€‚</p><p>å¦‚æœæˆ‘ä»¬æƒ³è¦$P_e = 0$,ä¹Ÿå°±æ˜¯æ— å¤±çœŸç¼–ç ï¼Œé‚£ä¹ˆæ ¹æ®Fanoä¸ç­‰å¼ï¼š</p><script type="math/tex; mode=display">H(X^n|M) = 0</script><p>å› æ­¤ï¼š</p><script type="math/tex; mode=display">\begin{aligned}nH(X) &= H(X^n)...(Why?)\\&=H(X^n) - H(X^n|M)\\&=I(X^n,M)\\&=H(M) - H(M|X^n)\\&\leq H(M) \leq nR\end{aligned}</script><p>æ‰€ä»¥å¯ä»¥å¾—åˆ°ï¼š$R\ge H(X)$.</p><p>å¯è§ï¼Œç†µæ˜¯å¹³å‡ç é•¿çš„ä¸‹ç•Œã€‚</p><h2 id="ç çš„ç±»å‹"><a href="#ç çš„ç±»å‹" class="headerlink" title="ç çš„ç±»å‹"></a>ç çš„ç±»å‹</h2><h3 id="éå¥‡å¼‚ç "><a href="#éå¥‡å¼‚ç " class="headerlink" title="éå¥‡å¼‚ç "></a>éå¥‡å¼‚ç </h3><p>è‹¥ä¸€ä¸ªç Cå¯ä»¥å°†ä¸åŒçš„xæ˜ å°„ä¸ºä¸åŒçš„$D^*$ä¸­ çš„åºåˆ—ï¼Œå³ï¼š</p><script type="math/tex; mode=display">x \ne x' \rightarrow C(x) \ne C(x')</script><p>åˆ™è¯¥ç ä¸ºéå¥‡å¼‚ç ã€‚</p><p>ä½†æ˜¯ä»…ä»…æ˜¯éå¥‡å¼‚ç çš„åºåˆ—å¯èƒ½ä¼šæœ‰æ­§ä¹‰ï¼ˆå¾ˆå¥½ç¬‘ï¼‰ã€‚$x_1 \rightarrow 0,x_2 \rightarrow 1,x_3 \rightarrow 01$ï¼Œé‚£ä¹ˆæˆ‘æ”¶åˆ°$01$å°±ä¸çŸ¥é“è¯¥å¦‚ä½•ä»‹ç»å®ƒäº†ã€‚</p><h3 id="å”¯ä¸€å¯è¯‘ç "><a href="#å”¯ä¸€å¯è¯‘ç " class="headerlink" title="å”¯ä¸€å¯è¯‘ç "></a>å”¯ä¸€å¯è¯‘ç </h3><p>å”¯ä¸€å¯è¯‘ç æ˜¯éå¥‡å¼‚ç çš„å­é›†ã€‚ç§°ç $C^<em>$ä¸ºç $C$çš„æ‰©å±•ï¼Œå½“$C^</em>$æ˜¯æœ‰é™é•¿Xçš„åºåˆ—åˆ°æœ‰é™é•¿Dåºåˆ—çš„æ˜ å°„ï¼Œä¸”æ»¡è¶³ï¼š</p><script type="math/tex; mode=display">C(x_1,x_2,...,x_n) = C(x_1)C(x_2)...C(x_n)</script><p>åˆ™è¯¥ç ä¸ºå”¯ä¸€å¯è¯‘ç ã€‚</p><p>ä»å¦ä¸€æ–¹é¢æ¥è¯´ï¼Œå¦‚æœç çš„æ‰©å±•ä¸ºéå¥‡å¼‚ç ï¼Œåˆ™è¯¥ç ä¸ºå”¯ä¸€å¯è¯‘ç ã€‚</p><p>æ¢å¥è¯è¯´ï¼Œæ²¡æœ‰ç å­—æ˜¯ç å­—çš„ç»„åˆã€‚</p><p>è¿™æ ·çš„ä¹Ÿæ˜¯æœ‰ç¼ºç‚¹çš„ï¼Œå› ä¸ºè§£ç å™¨å¤æ‚åº¦è¦æ±‚è¾ƒé«˜ã€‚å¦‚$x_1 = 01,x_2 = 10,x_3 = 0111$,å½“æ”¶åˆ°0110çš„æ—¶å€™ï¼Œåœ¨å‰ä¸‰ä¸ªçš„æ—¶å€™è§£ç å™¨é¢„æµ‹å¯èƒ½æ˜¯ä¸ª$x_3$,ä½†æ˜¯æœ€åä¸€ä¸ªä¸æ»¡è¶³ï¼Œå› æ­¤å°±éœ€è¦å›é€€ã€‚</p><h3 id="å³æ—¶ç ï¼ˆå‰ç¼€ç ï¼‰"><a href="#å³æ—¶ç ï¼ˆå‰ç¼€ç ï¼‰" class="headerlink" title="å³æ—¶ç ï¼ˆå‰ç¼€ç ï¼‰"></a>å³æ—¶ç ï¼ˆå‰ç¼€ç ï¼‰</h3><p>å‰ç¼€ç å¤§å®¶å°±æ¯”è¾ƒç†Ÿæ‚‰äº†ã€‚å‰ç¼€ç æ˜¯å”¯ä¸€å¯è¯‘ç çš„å­é›†ã€‚å¦‚å¸¸ç”¨çš„éœå¤«æ›¼ç¼–ç ã€‚å®ƒçš„æ„æ€æ˜¯æ²¡æœ‰ä»€ä¹ˆç å­—æ˜¯å¦ä¸€ä¸ªç å­—çš„å‰ç¼€ã€‚æ‰€ä»¥è§£ç å™¨åªè¦å‘ç°æœ‰è®¤è¯†çš„ï¼Œç«‹é©¬å°±å¯ä»¥è§£ç äº†ï¼Œæ‰€ä»¥å«å³æ—¶ç ã€‚</p><p>è«å°”æ–¯ç”µç æ˜¯éå¥‡å¼‚ç ï¼Œä½†ä¸æ˜¯å”¯ä¸€å¯è¯‘ç ã€‚æ±‰è¯­ä¹Ÿä¸æ˜¯å”¯ä¸€å¯è¯‘ç ï¼Œå› ä¸ºæ–­å¥ä¸å¯¹å°±ä¼šå¼•èµ·æ­§ä¹‰ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> ä¿¡æ¯è®º </category>
          
      </categories>
      
      
        <tags>
            
            <tag> information theory </tag>
            
            <tag> lossless encoding </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ä¿¡æ¯è®ºâ€”â€”è¿ç»­éšæœºå˜é‡çš„ç†µå’Œäº’ä¿¡æ¯</title>
      <link href="/2018/11/01/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94%E8%BF%9E%E7%BB%AD%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E7%86%B5%E5%92%8C%E4%BA%92%E4%BF%A1%E6%81%AF/"/>
      <url>/2018/11/01/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94%E8%BF%9E%E7%BB%AD%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E7%86%B5%E5%92%8C%E4%BA%92%E4%BF%A1%E6%81%AF/</url>
      
        <content type="html"><![CDATA[<p>å¦‚ä½•å°†ç¦»æ•£éšæœºå˜é‡çš„è¿™äº›æ¦‚å¿µæ¨å¹¿åˆ°è¿ç»­éšæœºå˜é‡ï¼Ÿ<br><a id="more"></a></p><p>ä½¿ç”¨é»æ›¼ç§¯åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">\begin{align}H(X) &= -\sum_{x} p(x)\Delta x \log p(x) \Delta x\\&= -\sum_{x}p(x)\log p(x)\Delta x - -\sum_{x}p(x)\log \Delta x\Delta x \end{align}</script><p>ä¸Šå¼ä¸­ï¼Œæœ€åä¸€é¡¹æ˜¯è¶‹äºè´Ÿæ— ç©·çš„ã€‚</p><p>è¿™æ„å‘³ç€è¿ç»­éšæœºå˜é‡åŒ…å«çš„ä¿¡æ¯æ˜¯æ— ç©·çš„ã€‚ä½†æ˜¯æ— ç©·çš„æ˜¯æ— æ³•ç ”ç©¶çš„ï¼Œå› æ­¤é¦™å†œé‡æ–°ç»™äº†ä¸€ä¸ªå¾®åˆ†ç†µçš„å®šä¹‰ï¼Œå®ƒåœ¨æ•°å­¦ä¸Šä¸å¤Ÿä¸¥æ ¼ï¼Œä½†æ˜¯åœ¨å®é™…ä¸Šå´éå¸¸æœ‰ç”¨ã€‚</p><script type="math/tex; mode=display">h(X) = \int _{-\infty }^{+\infty} p(x)\log p(x) dx</script><p>å¯ä»¥çœ‹åˆ°å®ƒåœ¨å½¢å¼ä¸Šä¸ç¦»æ•£å½¢å¼çš„ç†µæ˜¯éå¸¸ç›¸ä¼¼çš„ã€‚</p><p>åŒæ—¶ä¹Ÿæœ‰è”åˆç†µï¼š</p><script type="math/tex; mode=display">h(X,Y) = -\int p(x,y)\log p(x,y) dxdy</script><p>æ¡ä»¶ç†µï¼š</p><script type="math/tex; mode=display">h(X|Y) = -\iint p(x,y) \log p(x|y) dxdy = -\int p(y) \int p(x|y) \log p(x|y) dx dy</script><p>ä¸ç­‰å¼å…³ç³»ï¼š<br>$<br>h(X,Y) = h(X) + h(Y|X) = h(Y) + h(X|Y)<br>$</p><p>$<br>h(X|Y) \leq h(X), h(Y|X) \leq h(Y)<br>$</p><p>$<br>h(X,Y)\leq h(X) + h(Y)<br>$</p><p>è¿™äº›ä¸ç­‰å¼éƒ½æ˜¯å­˜åœ¨çš„ï¼Œä¸ç¦»æ•£å½¢å¼ä¸€è‡´ï¼Œä½†æ˜¯è¦æ³¨æ„çš„æ˜¯h(X)ä¸ä¸€å®šæ˜¯éè´Ÿçš„äº†ã€‚</p><p>ä¾‹å¦‚ï¼š</p><script type="math/tex; mode=display">X:p(x) = \left \{ \begin{array}{c}     \frac 1 {b-a} , a \leq x \leq b;\\    0, otherwise;    \end{array}    \right.</script><p>é‚£ä¹ˆå®ƒçš„å¾®åˆ†ç†µå®é™…ä¸Šç­‰äº$\log(b-a)$.å½“$b-a&lt;1$çš„æ—¶å€™ï¼Œè¿™ä¸ªç†µæ˜¯å°äº0çš„ã€‚</p><h3 id="é«˜æ–¯åˆ†å¸ƒçš„å¾®åˆ†ç†µ"><a href="#é«˜æ–¯åˆ†å¸ƒçš„å¾®åˆ†ç†µ" class="headerlink" title="é«˜æ–¯åˆ†å¸ƒçš„å¾®åˆ†ç†µ"></a>é«˜æ–¯åˆ†å¸ƒçš„å¾®åˆ†ç†µ</h3><p>é«˜æ–¯åˆ†å¸ƒæ¦‚ç‡å¯†åº¦å¦‚ä¸‹ï¼š</p><p>$X:p(x) = \frac{1}{\sqrt {2 \pi \sigma}} exp [-\frac{(x-m)^2}{2\sigma ^2}]$</p><p>è€Œå®ƒçš„å¾®åˆ†ç†µä¸º$h(x) = \frac 1 2 \log 2 \pi e \sigma^2$.</p><p>è¿™ä¸ªéœ€è¦è®°ä½ã€‚å½“ç„¶åªè¦å¸¦è¿›å®šä¹‰å°±å¯ä»¥æ¨ç®—å‡ºæ¥çš„ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒçš„å¾®åˆ†ç†µå’Œmï¼ˆæœŸæœ›ï¼‰æ˜¯æ— å…³çš„</p><p>ç»™å®šmå’Œ$\sigma$çš„æƒ…å†µä¸‹ï¼Œå½“è¿ç»­å˜é‡æœä»é«˜æ–¯åˆ†å¸ƒçš„æ—¶å€™ï¼Œå¾®åˆ†ç†µæœ€å¤§ã€‚</p><h3 id="äº’ä¿¡æ¯"><a href="#äº’ä¿¡æ¯" class="headerlink" title="äº’ä¿¡æ¯"></a>äº’ä¿¡æ¯</h3><p>$I(X;Y) = \iint p(x,y) \log \frac{p(x,y)}{p(x)p(y)}dxdy$</p><p>å¯ä»¥ç›´æ¥ä½¿ç”¨é»æ›¼ç§¯åˆ†å¾—åˆ°ï¼Œä¸ç¦»æ•£çš„æƒ…å†µä¹Ÿéå¸¸ä¸€è‡´ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> ä¿¡æ¯è®º </category>
          
      </categories>
      
      
        <tags>
            
            <tag> information theory </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ä¿¡æ¯è®ºâ€”â€”Fanoä¸ç­‰å¼</title>
      <link href="/2018/11/01/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94Fano%E4%B8%8D%E7%AD%89%E5%BC%8F/"/>
      <url>/2018/11/01/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94Fano%E4%B8%8D%E7%AD%89%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>ä»$X$åˆ°$Y$åˆ°$\hat{X}$ï¼Œå…¶ä¸­$\hat {X}$æ˜¯å¯¹åŸæœ‰çš„$X$çš„ä¼°è®¡,è€ŒYå¯ä»¥çœ‹ä½œæ˜¯ä¸€ä¸ªä¸­é—´è¿‡ç¨‹ï¼Œå¯ä»¥æƒ³è±¡ç±»ä¼¼äºç¼–ç è§£ç çš„è¿‡ç¨‹ã€‚è¿™ä¹‹é—´æœ‰ä»€ä¹ˆè”ç³»ï¼Ÿ<br><a id="more"></a></p><p>Fanoä¸ç­‰å¼å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">P_e = Pr(X \ne \hat{X})\\H(P_e) + P_e log|X| \geq H(X|\hat{X}) \geq H(X|Y)</script><p>è¯æ˜å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">E =\left \{    \begin{array}{c}    0,X = \hat{X}\\    1,X \ne \hat{X}    \end{array} \right.</script><p>åˆ™ï¼š$H(E) = H(P_e)$.æ³¨æ„ï¼šå½“ä¸ç­‰çš„æ—¶å€™ä¸º$1$,è¿™ä¸ªEå¯ä»¥çœ‹ä¸ºé”™è¯¯ã€‚</p><p>ç°åœ¨è€ƒè™‘$H(E,X|\hat{X})$.ç”±ä¹‹å‰çš„æ¡ä»¶ç†µå¯ä»¥çŸ¥é“ï¼š</p><script type="math/tex; mode=display">\begin{align}H(E,X|\hat {X}) &= H(X|\hat{X}) + H(E|X,\hat{X}) &-----(1)\\&=H(E|\hat{X})+H(X|E,\hat{X}) &-----(2)\end{align}</script><p>å¾ˆå®¹æ˜“çŸ¥é“(1)ä¸­ï¼Œ$H(E|X,\hat{X}) = 0$,å› ä¸ºEçš„å€¼å°±æ˜¯ç”±$X,\hat{X}$ç¡®å®šçš„ã€‚</p><p>(2)ä¸­ï¼Œ$H(E|\hat{X}) \leq H(P_e)$.è€Œç¬¬äºŒé¡¹$H(X|E,\hat{X})$æ»¡è¶³ï¼š</p><script type="math/tex; mode=display">H(X|E,\hat{X}) = P_e H(X|X \ne \hat{X},\hat{X}) + (1-P_e) H(X|X = \hat{X},\hat{x}).</script><p>è¿™æ­¥è¿˜æ˜¯æ¯”è¾ƒéš¾æ‡‚çš„ã€‚</p><p>ä¸Šå¼ä¸­ï¼Œ$P_e H(X|X \ne \hat{X},\hat{X}) \leq P_e \log |X|,H(X|X = \hat{X},\hat{x}) = 0 $.</p><p>å› æ­¤ç»„åˆèµ·æ¥ï¼š $H(X|\hat{X}) \leq  H(P_e) + \log |X|$.<br>è¿™å°±å¾—åˆ°äº†Fanoä¸ç­‰å¼çš„ç¬¬ä¸€ä¸ªä¸ç­‰å·ã€‚</p><p>æ¥ä¸‹æ¥è¯æ˜$I(X;Y)\geq I(X;\hat{X})$.</p><script type="math/tex; mode=display">\begin{align}I(X;Y,\hat{X}) &= I(X;Y) + I(X;\hat{X}|Y)\\&= I(X;\hat{X}) + I(X; Y|\hat{X})\end{align}</script><p>ä¸Šå¼ä¸­ï¼Œ $I(X;\hat{X}|Y) = 0$.é™¤å»Yçš„ä¿¡æ¯ï¼Œ$X,\hat{X}$æ˜¯ç»Ÿè®¡ç‹¬ç«‹çš„,è€Œä¸¤ä¸ªç³»ç»Ÿçš„äº’ä¿¡æ¯æ˜¯å¤§äºç­‰äº0çš„ã€‚æ‰€ä»¥å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">I(X,Y) \geq I(X;\hat{X}).</script><p>è¿™å‘Šè¯‰æˆ‘ä»¬ç¼–ç è¿‡ç¨‹ä¸­ï¼ŒåŸæœ‰ä¿¡æ¯åªä¼šè¡°å‡ï¼Œè€Œä¸å¯èƒ½å¢å¤§ã€‚</p><p>å¾—åˆ°ä¸Šå¼åï¼Œç¬¬äºŒä¸ªä¸ç­‰å¼å¾ˆå¿«å°±å¯ä»¥å¾—å‡ºï¼š</p><script type="math/tex; mode=display">H(X) - H(X|Y) \geq H(X) - H(X|\hat {X})\\H(X|\hat{X})\geq H(X|Y)</script><p>æ‰€ä»¥æ•´ä¸ªä¸ç­‰å¼å°±å¾—åˆ°äº†ï¼š</p><p>$H(P_e) + P_e \log|X| \geq  H(X|\hat {X}) \geq H(X|Y)$.</p>]]></content>
      
      
      <categories>
          
          <category> ä¿¡æ¯è®º </category>
          
      </categories>
      
      
        <tags>
            
            <tag> information theory </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ä¿¡æ¯è®ºâ€”â€”the Convexity</title>
      <link href="/2018/10/31/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94the-Convexity/"/>
      <url>/2018/10/31/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94the-Convexity/</url>
      
        <content type="html"><![CDATA[<p>è¿™ç¯‡åšå®¢æ¥ä»‹ç»ç†µï¼Œäº’ä¿¡æ¯ï¼Œé‰´åˆ«ä¿¡æ¯çš„å‡¸æ€§ï¼Œä¸ä¼˜åŒ–æœ‰ç€é‡è¦çš„å…³ç³»ã€‚<a id="more"></a></p><h2 id="å‡¸é›†-Convex-Set"><a href="#å‡¸é›†-Convex-Set" class="headerlink" title="å‡¸é›†(Convex Set)"></a>å‡¸é›†(Convex Set)</h2><p>å‡¸é›†ï¼šåœ¨æ¬§æ°ç©ºé—´ä¸­ï¼Œå‡¸é›†æ˜¯å¯¹äºé›†åˆå†…çš„æ¯ä¸€å¯¹ç‚¹ï¼Œè¿æ¥è¯¥å¯¹ç‚¹çš„ç›´çº¿æ®µä¸Šçš„æ¯ä¸ªç‚¹ä¹Ÿåœ¨è¯¥é›†åˆå†…çš„é›†åˆã€‚</p><p>å‡¸é›†æœ‰ï¼šå®æ•°ï¼Œæ¦‚ç‡çŸ¢é‡é›†åˆç­‰ã€‚æ•´æ•°ï¼Œæœ‰ç†æ•°ç­‰ä¸æ˜¯å‡¸é›†ã€‚</p><p>æƒ³è¦ç ”ç©¶å‡¸å‡½æ•°ï¼Œé¦–å…ˆå‡¸å‡½æ•°ä¸€å®šè¦å®šä¹‰åœ¨å‡¸é›†ä¸Šã€‚è€Œæ¦‚ç‡çŸ¢é‡é›†åˆä¸ºå‡¸é›†ï¼Œæ˜¯ä¸€ä¸ªå¥½æ¶ˆæ¯ã€‚</p><p>å‡¸å‡½æ•°æœ‰ä¸ªå‘ã€‚å°±æ˜¯ä¸­å›½æ•™ææ€»æ˜¯å«å‡¸å‡½æ•°å’Œå‡¹å‡½æ•°ï¼Œä½†æ˜¯å®é™…ä¸Šä¸­å›½çš„å‡¸å‡½æ•°ï¼Œæ˜¯æœ‰æœ€å¤§å€¼çš„å‡½æ•°ï¼Œè€Œéå›½å¤–çš„convex functionï¼ˆæœ€å°å€¼ï¼‰ã€‚å¦å¤–ä¸€ç§æ¯”è¾ƒå¥½çš„å«æ³•æ˜¯ä¸Šå‡¸å’Œä¸‹å‡¸ï¼Œè¿™ä¸ªå°±å®¹æ˜“åŒºåˆ†äº†ï¼Œä¸Šå‡¸å‡½æ•°æœ‰æœ€å°å€¼ï¼Œä¸‹å›¾å‡½æ•°æœ‰æœ€å¤§å€¼ã€‚</p><p>ä¸¥æ ¼çš„æ•°å­¦å®šä¹‰ï¼š</p><ul><li>å®šä¹‰åœ¨å‡¸é›†Dä¸Šçš„å‡½æ•°f(x)å¦‚æœæ»¡è¶³$f(\lambda \alpha + (1-\lambda)\beta) \leq \lambda f(\alpha) + (1-\lambda) f(\beta)$,åˆ™ä¸ºä¸‹å‡¸å‡½æ•°ã€‚</li></ul><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/ConvexFunction.svg/768px-ConvexFunction.svg.png" alt=""></p><ul><li>å®šä¹‰åœ¨å‡¸é›†Dä¸Šçš„å‡½æ•°f(x)å¦‚æœæ»¡è¶³$f(\lambda \alpha + (1-\lambda)\beta) \geq \lambda f(\alpha) + (1-\lambda) f(\beta)$,åˆ™ä¸ºä¸Šå‡¸å‡½æ•°ã€‚</li></ul><h2 id="Jensonä¸ç­‰å¼"><a href="#Jensonä¸ç­‰å¼" class="headerlink" title="Jensonä¸ç­‰å¼"></a>Jensonä¸ç­‰å¼</h2><p>å¦‚æœfæ˜¯ä¸‹å‡¸å‡½æ•°ï¼Œä¸”Xæ˜¯ç¦»æ•£éšæœºå˜é‡ï¼Œåˆ™$Ef(X)\geq f(EX)$,å¹¶ä¸”å¦‚æœfæ˜¯ä¸¥æ ¼ä¸‹å‡¸å‡½æ•°ï¼Œåˆ™ä¸Šå¼ä¸­ç­‰å·è¯´æ˜Xä¸ºå¸¸æ•°ï¼ŒåŠXä¸EXä»¥æ¦‚ç‡1ç›¸ç­‰ã€‚ï¼ˆå…¶ä¸­Eä¸ºå¹³å‡å–å€¼ï¼‰ã€‚</p><p>ç”±Jensonä¸ç­‰å¼å¯ä»¥æ¨å‡º<strong>å¯¹æ•°æ±‚å’Œä¸ç­‰å¼</strong>ï¼š<br>å¯¹äºéè´Ÿå®æ•°$a_1,a_2,â€¦,a_n;b_1,b_2,â€¦,b_n$æœ‰</p><script type="math/tex; mode=display">\sum _{i=1}^n a_i \log \frac{a_i}{b_i} \geq \left(\sum_{i=1}^na_i \right) \log \frac{\sum _{i=1}^n a_i}{\sum _{i=1}^n b_i}</script><p>è¿™ä¸ªå¼å­çš„è¯æ˜å¦‚ä¸‹ï¼š</p><p>é¦–å…ˆï¼Œå½“$t&gt;0$æ—¶ï¼Œ$t \log t$ä¸ºä¸€ä¸ªä¸¥æ ¼ä¸‹å‡¸å‡½æ•°.å¯è‡ªè¡Œç”¨å¯¼æ•°è¯æ˜ã€‚<br>ç”±Jensonä¸ç­‰å¼å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display"> Et \log Et \leq \ E(t \log t)\\i.e. \sum \alpha_i f(t_i) \geq  f(\sum \alpha _i t_i)\\</script><p>ä»¤$\alpha _i = \frac {b_i}{B},t_i=\frac {a_i}{b_i}, B = \sum_{b_i}$ï¼Œä»£å…¥ä¸Šå¼å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display"> \sum \frac{b_i}{B}\frac{a_i}{b_i} \log (\frac{a_i}{b_i}) \geq(\sum \frac{b_i}{B}\frac{a_i}{b_i}) \log(\sum \frac{b_i}{B}\frac{a_i}{b_i})\\ \sum \frac{a_i}{B} \log (\frac{a_i}{b_i}) \geq (\sum \frac{a_i}{B}) \log{\sum \frac {a_i}{B}}\\ \sum a_i \log {\frac {a_i}{b_i}} \geq (\sum a_i) \log {\frac {\sum a_i}{\sum b_i}}</script><p>åœ¨è¿™é‡Œè¦å­¦ä¼šå¦‚ä½•æ„é€ å»è¯æ˜è¿™ä¸ªä¸ç­‰å¼ã€‚</p><h2 id="å‡¸æ€§"><a href="#å‡¸æ€§" class="headerlink" title="å‡¸æ€§"></a>å‡¸æ€§</h2><h3 id="é‰´åˆ«ä¿¡æ¯çš„å‡¸æ€§"><a href="#é‰´åˆ«ä¿¡æ¯çš„å‡¸æ€§" class="headerlink" title="é‰´åˆ«ä¿¡æ¯çš„å‡¸æ€§"></a>é‰´åˆ«ä¿¡æ¯çš„å‡¸æ€§</h3><p>$D(p\Vert q)$æ˜¯$(p,q)$çš„ä¸‹å‡¸å‡½æ•°ã€‚å³è‹¥å­˜åœ¨$(p_1,q_1)$å’Œ$(p_2,q_2)$,åˆ™$\lambda D(p_1\Vert q_1) + (1 - \lambda) D(p_2\Vert q_2) \geq D(\lambda p_1 + (1-\lambda)p_2 \Vert \lambda q_1+(1 - \lambda)q_2)$å¯¹æ‰€æœ‰çš„$0\leq \lambda \1$æˆç«‹ã€‚</p><p>è¯æ˜å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\begin{align}D(\lambda p_1 + (1-\lambda)p_2 \Vert \lambda q_1+(1 - \lambda)q_2)&=\sum_{x \in \mathcal{X}} \left [ \lambda p_1(x)+ \overline {\lambda} p_2(x) \right] \log \frac{\lambda p_1(x)+ \overline {\lambda} p_2(x)}{\lambda q_1(x)+ \overline {\lambda} q_2(x)}\\& \leq \sum_{x \in \mathcal{X}} \lambda p_1(x) \log {\frac{p_1(x)}{q_1(x)}} +  \overline {\lambda} p_2(x) \log \frac{p_2}{q_2}\\&= \sum_{x \in \mathcal{X}} \lambda D(p_1 \Vert q_1) + \overline {\lambda} D(p_2 \Vert q_2)\end{align}</script><p>å¯ä»¥çœ‹åˆ°ä¸Šå¼çš„è¯æ˜åˆ©ç”¨åˆ°äº†ä¹‹å‰çš„å¯¹æ•°æ±‚å’Œä¸ç­‰å¼ã€‚</p><h3 id="ç†µçš„å‡¸æ€§"><a href="#ç†µçš„å‡¸æ€§" class="headerlink" title="ç†µçš„å‡¸æ€§"></a>ç†µçš„å‡¸æ€§</h3><p>çŸ¥é“äº†é‰´åˆ«ä¿¡æ¯çš„ä¸‹å‡¸æ€§è´¨ï¼Œè¯æ˜ç†µçš„å‡¸æ€§å°±éå¸¸å®¹æ˜“ã€‚</p><p>$H(X) = \log |X| - D(p\Vert u)$</p><p>ä¸Šå¼ä¸­ï¼Œuä¸å˜ï¼Œæ˜¯å‡åŒ€åˆ†å¸ƒçš„æƒ…å†µï¼Œè¿™æ—¶å€™Dæ˜¯ä¸€ä¸ªä¸‹å‡¸å‡½æ•°ï¼Œè€Œå¾ˆæ˜æ˜¾$\log |X|$ä¸å˜ï¼Œå› æ­¤$H(X)$æ˜¯ä¸€ä¸ªä¸Šå‡¸å‡½æ•°ã€‚å…¶å®å¤§å®¶ä¹Ÿå¾ˆå®¹æ˜“ç†è§£ã€‚å› ä¸ºå‡åŒ€åˆ†å¸ƒå¼çš„ç†µæœ€å¤§ï¼Œä¹Ÿå°±æ˜¯æœ‰æœ€å¤§å€¼ã€‚</p><h3 id="äº’ä¿¡æ¯çš„å‡¸æ€§"><a href="#äº’ä¿¡æ¯çš„å‡¸æ€§" class="headerlink" title="äº’ä¿¡æ¯çš„å‡¸æ€§"></a>äº’ä¿¡æ¯çš„å‡¸æ€§</h3><p>ä¸‹é¢æ¥è¯æ˜äº’ä¿¡æ¯çš„å‡¸æ€§ã€‚</p><p>äº’ä¿¡æ¯å®šä¹‰ä¸ºä¸‹ï¼š</p><script type="math/tex; mode=display">I(X;Y) = I(p;Q) =  \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x)q(y|x)\log \frac{q(y|x)}{\sum _{x \in \mathcal{X}}p(x)q(y|x)}</script><p>è¿™æ ·çš„å†™æ³•ï¼Œå¯¹äºä¿¡é“ä¼ è¾“çš„æ¨¡å‹æ›´æœ‰å¸®åŠ©ã€‚</p><p>é¦–å…ˆå¦‚æœç»™å®šQï¼Œè¿™æ„å‘³ç€ç»™å®šäº†ä¿¡é“ï¼š</p><p>Fix Q =[g(y|x)]</p><script type="math/tex; mode=display">\begin{align}I(X;Y) &= H(Y) - H(Y|X)\\&= H(Y) - \sum_{x \in \mathcal{X}} p(x)H(Y|X=x)\\&= H(Y) - \sum_{x \in \mathcal{X}} p(x) \sum_{y in \mathcal{Y}} q(y|x) \log \frac{q(y|x)}\end{align}</script><p>ä¸Šå¼ä¸­æ—¢ç„¶Qå·²ç»ç»™å®šï¼Œå› æ­¤H(Y|X)ä¹Ÿå°±æ˜¯p(x)çº¿æ€§ç»„åˆã€‚å› æ­¤æ•´ä¸ªå‡½æ•°ä¸ºä¸Šå‡¸å‡å»çº¿æ€§ï¼Œä¾ç„¶ä¸ºä¸Šå‡¸å‡½æ•°ã€‚</p><p>å¦‚æœç»™å®špï¼š</p><p>Fix p(x):</p><script type="math/tex; mode=display">\begin{align}I(X;Y) = D(p(x,y)\Vert p(x)(y))\\p(x,y) = p(x)q(y|x)\\p(y) = \sum_{x \in \mathcal{X}} p(x)q(y|x)\end{align}</script><p>å› æ­¤p(x,y),p(y),p(x)p(y)éƒ½æ˜¯q(y|x)çš„çº¿æ€§ç»„åˆã€‚è€ŒDæœ¬èº«æ˜¯ä¸‹å‡¸å‡½æ•°ã€‚æ‰€ä»¥äº’ä¿¡æ¯å›ºå®šp(x)æ—¶å€™ä¸ºä¸‹å‡¸å‡½æ•°ã€‚å¯ç”¨äºæœ‰å¤±çœŸç¼–ç ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> ä¿¡æ¯è®º </category>
          
      </categories>
      
      
        <tags>
            
            <tag> information theory </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Learning From Dataâ€”â€”Generative Learning Algorithm</title>
      <link href="/2018/10/29/Learning-From-Data%E2%80%94%E2%80%94Generative-Learning-Algorithm/"/>
      <url>/2018/10/29/Learning-From-Data%E2%80%94%E2%80%94Generative-Learning-Algorithm/</url>
      
        <content type="html"><![CDATA[<p>ä¹‹å‰æˆ‘ä»¬ä»‹ç»çš„åˆ†ç±»ç®—æ³•ï¼ŒåŒ…æ‹¬ï¼Œlogistic regressionï¼ŒPLAç”šè‡³åŠ ä¸Šlinear regressionï¼Œä»–ä»¬éƒ½æ˜¯è¯•å›¾æ‰¾åˆ°ä¸€æ¡çº¿ç„¶åæ¥å°†ä¸¤ç§ç±»åˆ«åˆ†å¼€ã€‚è¿™ç§ç®—æ³•å«åšDiscriminative Learning Algorithmï¼Œä»–ä»¬çš„ç”±æ¥ï¼Œéƒ½æ˜¯ç›´æ¥å»ä¼°è®¡$P(Y|X)$,è¿™æ ·çš„è¯æ ¹æ®æ–°æ ·æœ¬çš„Xï¼Œä»è€Œé¢„æµ‹Yã€‚<a id="more"></a></p><p>æ¥ä¸‹æ¥æˆ‘ä»¬æƒ³ä»‹ç»çš„æ˜¯å¦å¤–ä¸€ç§æ€è·¯æ¥è§£å†³åˆ†ç±»é—®é¢˜ã€‚æˆ‘ä»¬ä¸å†ç›´æ¥ä¼°è®¡$P(Y|X)$,è€Œæ˜¯ä¼°è®¡$P(X|Y)$.ä¹Ÿå°±æ˜¯ï¼Œæˆ‘ä»¬å¸Œæœ›ä»å½“å‰çš„æ ·æœ¬ä¸­å­¦åˆ°Xçš„ç‰¹å¾çœ‹ä¸Šå»åº”è¯¥æ˜¯ä»€ä¹ˆæ ·å­ï¼Œä»é¸¡çš„æ ·æœ¬ä¸­å­¦åˆ°é¸¡çš„æ ·å­ï¼Œä»ç‹—çš„æ ·æœ¬ä¸­å­¦åˆ°ç‹—çš„æ ·å­ã€‚è¿™æ ·çš„ç®—æ³•å«åšGenerative Learning Algorithmï¼ˆç”Ÿæˆå­¦ä¹ ç®—æ³•ï¼‰ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬æœ€åè¦çŸ¥é“çš„è¿˜æ˜¯$P(Y|X)$,ä¸è¿‡æ ¹æ®Bayesç†è®ºå¯ä»¥çŸ¥é“ï¼š</p><script type="math/tex; mode=display">P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}</script><p>æˆ‘ä»¬çŸ¥é“ï¼š</p><p>$argmax_yp(y|x) = argmax_y \frac{p(x|y)p(y)}{p(x)} = argmax_y p(x|y)p(y) = WhatWePredict$</p><p>æ‰€ä»¥æˆ‘ä»¬çœŸæ­£éœ€è¦è§£å†³çš„æ˜¯$P(x|y)P(y)$.</p><p>å½“ç„¶ï¼Œå¦‚æœæƒ³è¦æ±‚å¾—P(X),å¯ä»¥é€šè¿‡å…¨æ¦‚ç‡å…¬å¼ï¼š$P(X) = P(y=1)\cdot P(X|y=1) +P(y=0)\cdot P(X|y=0)$.</p><p>ä¸‹é¢ä»‹ç»ä¸¤ä¸ªæœ€å¸¸è§çš„ç”Ÿæˆå­¦ä¹ ç®—æ³•ï¼šGaussian Discriminant Analysis(é«˜æ–¯åˆ¤åˆ«åˆ†æ)ä¸Naive Beyas(æœ´ç´ è´å¶æ–¯æ¨¡å‹)ã€‚</p><h3 id="Gaussian-Discriminate-Analysis"><a href="#Gaussian-Discriminate-Analysis" class="headerlink" title="Gaussian Discriminate Analysis"></a>Gaussian Discriminate Analysis</h3><p>é«˜æ–¯åˆ¤åˆ«æ¨¡å‹ä¸»è¦ç”¨äºè¾“å…¥æ˜¯è¿ç»­çš„æ—¶å€™ï¼Œä¹Ÿå°±æ˜¯Xçš„ç‰¹å¾å€¼æ˜¯å±äºå®æ•°é›†çš„ã€‚é¦–å…ˆæ¥å¤ä¹ ä¸€ä¸‹å¤šç»´é«˜æ–¯åˆ†å¸ƒæ¨¡å‹ï¼Œå®ƒæ˜¯é«˜æ–¯åˆ¤åˆ«æ¨¡å‹çš„åŸºç¡€ï¼š</p><p>ä¸€èˆ¬æ¥è¯´ï¼Œå¤šç»´é«˜æ–¯åˆ†å¸ƒç®€å†™ä¸ºï¼šX \tilde{} N(\mu,\Sigma).</p><ul><li>$\mu \in \mathbb{R}^n$ æ˜¯å¹³å‡å‘é‡ã€‚</li><li>$\Sigma \in \mathbb{R}^{n \times n}$æ˜¯åæ–¹å·®çŸ©é˜µã€‚$\Sigma$æ˜¯å¯¹ç§°çš„SPDï¼ˆ symmetric and positive definite matrixï¼‰.</li></ul><p>å®ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">p(x;\mu,\Sigma) = \frac{1}{(2\pi)^{\frac n 2}\vert \Sigma\vert ^{\frac 1 2}} e^{-\frac 1 2(x-u)^T\Sigma^{-1}(x-u)}</script><p>å‡å€¼å’Œåæ–¹å·®å¯¹åˆ†å¸ƒæœ‰ä»€ä¹ˆå½±å“ï¼Ÿåˆ©ç”¨ä¸€ä¸ªäºŒç»´çš„å‘é‡æ¥è¯´çš„è¯ï¼Œæœ‰ä¸‹é¢å‡ å¼ å›¾å¯ä»¥çœ‹çœ‹ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/mg1.png" alt=""></p><p>è¿™ä¸‰å¼ å›¾åˆ†åˆ«å¯¹åº”ç€åæ–¹å·®çŸ©é˜µä¸º$Iï¼Œ2I,\frac 1 2 I$çš„æƒ…å†µã€‚å…¶ä¸­<br>$I = \begin{bmatrix}<br>1&amp;0\\<br>0&amp;1<br>\end{bmatrix}$.</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/mg2.png" alt=""></p><p>ä»ä¸Šå›¾åˆå¯ä»¥çœ‹å‡ºæ¥ï¼Œåæ–¹å·®çŸ©é˜µéå¯¹è§’çº¿çš„æ•°å­—å˜åŒ–çš„æ—¶å€™ï¼Œå¯¹å®ƒçš„å›¾å½¢ä¼¼ä¹æ‰­å‘ä¸€è¾¹äº†ã€‚å®é™…ä¸Šè¿™ä»£è¡¨äº†ä¸¤ä¸ªç‰¹å¾é—´çš„ç›¸å…³ç¨‹åº¦ã€‚</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/mg3.png" alt=""></p><p>æœ€åä¸€ä¸ªå°±æ˜¯$mu$çš„å˜åŒ–ï¼Œå¾ˆæ˜¾è€Œæ˜“è§ï¼Œå›¾å½¢çš„ä¸­å¿ƒæ”¹å˜äº†ã€‚</p><p>ç°åœ¨å‡å¦‚æœ‰ä¸€ä¸ªåˆ†ç±»é—®é¢˜ï¼Œå…¶ä¸­$X \in \mathbb{R}^n$,åˆ©ç”¨é«˜æ–¯åˆ¤åˆ«æ¨¡å‹çš„è¯ï¼Œæˆ‘ä»¬ä¼šæœ‰ä»¥ä¸‹å‡è®¾ï¼š</p><script type="math/tex; mode=display">y\tilde{} Bernuolli(\Phi)\\p(x|y=0)\tilde{}N(\mu_0,\Sigma)\\p(x|y=1)\tilde{}N(\mu_1,\Sigma)</script><p>ä»ä¸Šé¢çœ‹å‡ºæ¥ï¼Œæˆ‘ä»¬å¯¹äºä¸¤ä¸ªæ¨¡å‹çš„ç”Ÿæˆé‡‡ç”¨çš„$\mu$ä¸ä¸€æ ·ï¼Œä½†æ˜¯$\Sigma$ä¸€æ ·ï¼Œè¿™æ ·ä¸å…‰ä¿è¯äº†ä¸¤ä¸ªæ¨¡å‹çš„å½¢çŠ¶ä¸€æ ·ï¼Œç®€åŒ–äº†è®¡ç®—è¿‡ç¨‹ï¼Œæœ€åå¯ä»¥ç”¨å®ƒä»¬æ¥è§¦ç‚¹çš„åˆ‡çº¿æ¥å½“åšåˆ†å‰²çº¿ï¼Œä»è€Œå®ç°ä¸ä¹‹å‰Discriminativeå­¦ä¹ ç®—æ³•çš„æ¯”è¾ƒã€‚</p><p>Note:ç°åœ¨nä¸ºå‘é‡çš„ç»´åº¦ï¼Œè€Œmä¸ºæ ·æœ¬ä¸ªæ•°ã€‚</p><p>å’Œå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬æ±‚å®ƒçš„logæå¤§ä¼¼ç„¶ä¼°è®¡ï¼š</p><script type="math/tex; mode=display">\begin{align}l(\Phi,\mu_0,\mu_1,\Sigma)&= \log \prod_{i=1}^{m}p(X_i,y_i;\Phi,\mu_0,\mu_1,\Sigma)\\&= \log \prod_{i=1}^m p(X_i|y_i;\mu_0,\mu_1,\Sigma)p(y_i;\Phi) \end{align}</script><p>ä¸Šè¿°çš„å¼å­ä¸­çš„å‚æ•°å–å€¼å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\Phi = \frac 1 m \sum_{i=1}^m \mathbf{1}\{ y_i = 1\}\\\mu_b = \frac{\sum_{i=1}^m \mathbf{1}\{y_i=b\}X_i}{\sum_{i=1}^m \mathbf{1}\{y_i=b\}},b=0,1\\\Sigma = \frac 1 m \sum_{i=1}^m(X_i - \mu_{y_i})(X_i - \mu_{y_i})^T</script><p>æˆ‘ä»¬å¸Œæœ›ç”Ÿæˆçš„æ¨¡å‹å¦‚ä¸‹ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/IMG_0366.JPG" alt=""></p><p>å¦‚æœç”»ç­‰é«˜çº¿å›¾ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/mg4.png" alt=""> </p><p>æˆ‘ä»¬ä¼šå‘ç°æ‰¾åˆ°ä¸€æ¡çº¿ï¼Œå®ƒå±äº0æˆ–è€…1çš„æ¦‚ç‡æ˜¯ç›¸ç­‰çš„ã€‚å› æ­¤æˆ‘ä»¬å°±æ‰¾åˆ°äº†è¿™æ¡çº¿ã€‚</p><h4 id="GDAä¸Logistic-Regression"><a href="#GDAä¸Logistic-Regression" class="headerlink" title="GDAä¸Logistic Regression"></a>GDAä¸Logistic Regression</h4><p>åœ¨ä¸Šé¢çš„æ¨å¯¼è¿‡ç¨‹ä¸­æˆ‘ä»¬å‘ç°ï¼š</p><script type="math/tex; mode=display">\begin{align}P(y=1|X;\Phi,\mu_0,\mu_1,\Sigma) &=  \frac{p(X|y=1)p(y=1)}{p(X|y=1)p(y=1)+p(X|y=0)p(y=0)}\\&=\frac {1}{1+e^{-\theta ^TX}}\end{align}</script><p>ä¸Šå¼ä¸­ï¼š$\theta = \begin{bmatrix} \log \frac{1-\Phi}{\Phi} - \frac 1 2(\mu_0^T\Sigma^{-1}\mu_0 - \mu_1^T \Sigma^{-1}\mu_1) \ \Sigma ^{-1}(\mu_0 - \mu_1)\end{bmatrix}$.</p><p>å¦‚æœè¿˜è®°å¾—ä¸Šæ¬¡è®²è¿‡çš„exponential familyï¼Œæˆ‘ä»¬åº”è¯¥çŸ¥é“å¯¹äºä¼¯å¥´åˆ©åˆ†å¸ƒ$(y|x \tilde{} B(\psi))$çš„canonical link funtionæ˜¯ $\log \frac{\psi}{1-\psi}$.</p><p>å› æ­¤$\theta ^X = \log \frac{\psi}{1-psi} = \log \frac{p(y=1|X)}{p(y=0|X)} = \log \frac {p(X|y=1)p(y=1)}{p(X|y=0)p(y=0)}$</p><p>è¿™æ„å‘³ç€ï¼šå¦‚æœ$p(x|y)\tilde{}N(\mu,\Sigma)$,åˆ™$p(y|x)$æ˜¯logistic function.ä½†æ˜¯ä»logistic regressionæ˜¯æ— æ³•æ¨å‡ºæ¥GDAçš„ã€‚</p><p>å¦‚æœæˆ‘ä»¬çš„å‡è®¾æ­£ç¡®ï¼ŒGDAæ¨¡å‹æ›´é«˜æ•ˆï¼Œæ•ˆæœä¹Ÿæ›´å¥½ï¼Œä½†æ˜¯logistic regressionå› ä¸ºæ¯”è¾ƒç®€å•ï¼Œå¯¹äºå³ä½¿å‡è®¾é”™è¯¯äº†ä¾ç„¶æœ‰å¾ˆå¥½çš„é²æ£’æ€§ã€‚GDAæœ€å¤§åŒ–è”åˆåˆ†å¸ƒï¼Œè€ŒLRæœ€å¤§åŒ–çš„æ˜¯æ¦‚ç‡åˆ†å¸ƒã€‚</p><p>æ‰€ä»¥æˆ‘ä»¬å‘ç°ï¼ŒGDAå¹¶ä¸åƒæ™®é€šçš„å­¦ä¹ ç®—æ³•é‚£æ ·éœ€è¦å»è¿›è¡Œcost funtionçš„ä¼˜åŒ–ã€‚è¿™å¾—ç›Šäºæˆ‘ä»¬å‡è®¾æ•´ä¸ªæ­£æ ·æœ¬çš„Xæœä»ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒã€‚è€Œä¹‹å‰çš„å­¦ä¹ æ–¹æ³•ï¼Œæ¯ä¸ªæ ·æœ¬å› ä¸ºXä¸åŒå„ä¸ªæ ·æœ¬ä¹‹é—´æœä»åˆ†å¸ƒçš„å‚æ•°éƒ½ä¼šä¸ä¸€è‡´ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªæ ·æœ¬åœ¨ç»™å®šyçš„æ—¶å€™æœ‰ä¸€ä¸ªè‡ªå·±çš„åˆ†å¸ƒï¼ˆå¦‚é€»è¾‘å›å½’ï¼Œæ¯ä¸ªæ ·æœ¬éƒ½æœ‰ä¸åŒçš„æ¦‚ç‡ï¼Œè€Œå®ƒé¢„æµ‹çš„æ˜¯ä¼¯å¥´åˆ©åˆ†å¸ƒï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªæ ·æœ¬æœä»ä¸åŒçš„ä¼¯å¥´åˆ©åˆ†å¸ƒï¼›åˆå¦‚çº¿æ€§å›å½’ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰ä¸ä¸€æ ·çš„$\mu$ï¼‰ã€‚</p><h3 id="Naive-Beyas"><a href="#Naive-Beyas" class="headerlink" title="Naive Beyas"></a>Naive Beyas</h3><p>ä¸‹é¢ä»‹ç»æœ´ç´ è´å¶æ–¯æ¨¡å‹ã€‚å®ƒç”¨æ¥å¤„ç†è¾“å…¥ä¸ºç¦»æ•£æ•°æ®æ—¶å€™çš„æƒ…å†µã€‚å‡å¦‚æœ‰è¿™ä¹ˆä¸€ä¸ªä¾‹å­ï¼šåƒåœ¾é‚®ä»¶åˆ†ç±»ã€‚å¦‚ä½•å®šä¹‰é‚®ä»¶çš„ç‰¹å¾ï¼Ÿå½“ç„¶å®šä¹‰çš„æ˜¯å…¶ä¸­çš„å•è¯äº†ã€‚ä½†æ˜¯è¿™ä¸ªä¸–ç•Œä¸Šå•è¯æœ‰å¾ˆå¤šå¾ˆå¤šï¼Œè€Œåƒåœ¾é‚®ä»¶å¾ˆå¯èƒ½æ›´åŒ…å«äº†å¾ˆå¤šæ— æ„ä¹‰çš„å­—ç¬¦ç»„åˆï¼Œè¿™æ ·ç‰¹å¾å°±æ›´å¤šäº†ã€‚</p><p>å‡è®¾ç»™ä¸€ä¸ªå¤§å°ä¸ºnçš„å­—å…¸ï¼ˆè¿™ä¸ªnå¯èƒ½å¾ˆå¤§ï¼Œ50000æˆ–è€…100000ï¼‰ï¼Œè€Œä¸€ä¸ªé‚®ä»¶çš„ç‰¹å¾å€¼ä¼šåƒä¸‹é¢çš„æ ·å­ï¼š</p><script type="math/tex; mode=display">\begin{bmatrix}1\\0\\.\\.\\.\\1\\.\\.\\.\end{bmatrix} \begin{matrix}a\\aadjsa\\.\\.\\.\\b\\.\\.\\.\end{matrix}</script><p>1æˆ–è€…0ä»£è¡¨äº†åœ¨è¿™ä¸ªé‚®ä»¶ä¸­æ˜¯å¦å‡ºç°äº†ã€‚</p><p>ç°åœ¨å¸Œæœ›å¯¹$P(X|Y)$å»ºæ¨¡ã€‚å¯¹äºä¸€å°åƒåœ¾é‚®ä»¶ï¼š</p><script type="math/tex; mode=display">p(x_1,x_2,...,x_n|y) = p(x_1|y)p(x_2|y,x_1),...,p(x_n|y,x_1,...,x_{n-1})</script><p><em>ä½¿ç”¨äº†æ¦‚ç‡è®ºä¸­çš„é“¾å¼æ³•åˆ™ã€‚è¿™ä¸ªè§„åˆ™æˆ‘ä¹Ÿä¸äº†è§£ï¼Œæ¦‚ç‡è®ºè¿˜éœ€è¦å­¦ä¹ </em></p><p>è¿™æ ·çš„è®¡ç®—æ˜¯éå¸¸å¤æ‚çš„ã€‚å› æ­¤æœ´ç´ è´å¶æ–¯æ¨¡å‹ä¸­æœ‰ä¸€ä¸ªå‡è®¾ï¼š$x_i$åœ¨ç»™å®šyä¹‹åæ˜¯æ¡ä»¶ç‹¬ç«‹çš„ã€‚è¿™æ„å‘³ç€ï¼š$p(x_n|y,x_1,â€¦,x_{n-1}) = p(x_n|y)$.</p><p>æ‰€ä»¥$p(x_1,x_2,â€¦,x_n|y) = \prod _{i=1}^np(x_i|y)$.</p><h4 id="å¤šå˜é‡ä¼¯å¥´åˆ©äº‹ä»¶æ¨¡å‹"><a href="#å¤šå˜é‡ä¼¯å¥´åˆ©äº‹ä»¶æ¨¡å‹" class="headerlink" title="å¤šå˜é‡ä¼¯å¥´åˆ©äº‹ä»¶æ¨¡å‹"></a>å¤šå˜é‡ä¼¯å¥´åˆ©äº‹ä»¶æ¨¡å‹</h4><p>$p(x,y) = p(y)p(x|y) = p(y) \prod _{i=1}^n p(x_i|y)$.</p><p>è¿™ä¸ªæ¨¡å‹å‡è®¾äº†æ¯å°é‚®ä»¶æ˜¯ä»¥$\Phi_y$éšæœºç”Ÿæˆçš„ã€‚å¦‚æœç»™å®šyï¼Œæ¯ä¸ªå•è¯æ˜¯ç‹¬ç«‹çš„åŒ…å«åœ¨ä¿¡æ¯é‡Œçš„ï¼Œè€Œè¿™ä¸ªæ¦‚ç‡ä¸º$p(x_i=1|y) = \Phi_{i|y}$.</p><p>è¿™ä¸ªæ¨¡å‹å‚æ•°å¦‚ä¸‹ï¼š</p><ul><li>$\Phi_y = p(y=1)$</li><li>$\Phi_{i|y=0} = p(x=1|y=0)$</li><li>$\Phi_{i|y=1} = p(x=1|y=1)$</li></ul><p>åŒæ ·çš„ï¼Œæ¥ä¸‹æ¥è¦åšçš„ä¾ç„¶æ˜¯æ±‚logæå¤§ä¼¼ç„¶ä¼°è®¡ï¼š</p><script type="math/tex; mode=display">\begin{align}L(\Phi_y \Phi_{i|y=1},\Phi_{i|y=0}) &=\log \prod_{i=1}^m p(X_i,y_i)\\ &= \log \prod_{i=1}^m p(X_i|y_i)p(y_i)\\ &= \log \prod_{i=1}^m \prod_{j=1}^np(x_j|y_i)\end{align}</script><p>ä¸éš¾æƒ³è±¡ï¼Œå„ä¸ªå‚æ•°çš„å–å€¼å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\Phi_y = p(y=1) = \frac 1 m \sum_{i=1}^m \mathbf{1}\{y_i=1\}\\\Phi_{j|y=b} = \frac {\sum_{i=1}^m \mathbf{1}\{y_i = b\} \cdot \mathbf{1}\{ x_j=1\} }{\sum_{i=1}^m \mathbf{1}\{y_i = b\} },b=0,1</script><p>å½“ç»™äº†ä¸€ä¸ªæ–°çš„æ ·æœ¬çš„æ—¶å€™ï¼Œæˆ‘ä»¬è®¡ç®—$p(y=1|x)$:</p><script type="math/tex; mode=display">\begin{align}p(y=1|X) &= \frac{p(X|y=1)p(y=1)}{p(X)}\\&= \frac{p(y=1)\prod _{i=1}^n p(x_i|y=1)}{p(X|y=1)+p(X|y=0)}\\&=\frac{p(y=1)\prod _{i=1}^n p(x_i|y=1)}{p(y=0)\prod _{i=1}^n p(x_i|y=0)+ p(y=1)\prod _{i=1}^n p(x_i|y=1)}\end{align}</script><p>ç„¶åæ ¹æ®æ¦‚ç‡æ˜¯å¦å¤§äº0.5æ¥è¿›è¡Œé¢„æµ‹ã€‚</p><h4 id="Laplace-Smoothing"><a href="#Laplace-Smoothing" class="headerlink" title="Laplace Smoothing"></a>Laplace Smoothing</h4><p>è¿™ä¸ªæ¨¡å‹æ˜¯æœ‰ä¸€ä¸ªç¼ºç‚¹çš„ï¼šå¦‚æœæ–°çš„æ ·æœ¬çš„å•è¯ä»æ¥æ²¡æœ‰åœ¨è®­ç»ƒé›†åˆé‡Œå‡ºç°è¿‡ï¼Œé‚£ä¹ˆï¼š</p><script type="math/tex; mode=display">\Phi_{j|y=b} = \frac {\sum_{i=1}^m \mathbf{1}\{y_i = b\} \cdot \mathbf{1}\{ x_{i,j}=1\} }{\sum_{i=1}^m \mathbf{1}\{y_i = b\} } = 0,b=0,1</script><p>ä¹Ÿå°±æ˜¯åƒåœ¾é‚®ä»¶å’Œéåƒåœ¾é‚®ä»¶é‡Œå‡ºç°å®ƒçš„æ¦‚ç‡éƒ½ä¸º0.é‚£ä¹ˆæœ€åé¢„æµ‹ç»“æœä¸º$\frac 0 0$,è¿™å°±æ²¡æ³•è®¡ç®—äº†ã€‚</p><p>æ‰€ä»¥æˆ‘ä»¬éœ€è¦è¿›è¡ŒLaplaceå¹³æ»‘ã€‚å¯¹äºæ²¡æœ‰å‡ºç°è¿‡çš„è¯ï¼Œæˆ‘ä»¬ç»™ä»–èµ‹ä¸€ä¸ªè¾ƒå°å€¼ï¼Œè€Œä¸æ˜¯è®©ä»–ä¸º0ï¼š</p><script type="math/tex; mode=display">\Phi_j = \frac{\sum_{i=1}^m \mathbf{1}\{z_i = j\}+1}{m+k}</script><p>ä¸ºäº†æœ€åä½¿å¾—$\Phi_j$çš„å’Œä¸º0,æ‰€ä»¥kä¸ºç±»æ¯”çš„ä¸ªæ•°ï¼Œå³$\sum_{i=1}^k\Phi_i = 1$ã€‚</p><p>æ‰€ä»¥æœ€åçš„ä¼°è®¡å°±æˆäº†ä¸‹é¢çš„æ ·å­ï¼š</p><script type="math/tex; mode=display">\Phi_{j|y=b} = \frac {\sum_{i=1}^m \mathbf{1}\{y_i = b\} \cdot \mathbf{1}\{ x_{i,j} = 1\} +1}{\sum_{i=1}^m \mathbf{1}\{y_i = b\} +2},b=0,1</script><p>é™¤æ­¤ä¹‹å¤–å…¶ä»–åœ°æ–¹æ˜¯ä¸€æ ·çš„ã€‚</p><h4 id="Naive-Bayes-and-Multinomial-Event-Model"><a href="#Naive-Bayes-and-Multinomial-Event-Model" class="headerlink" title="Naive Bayes and Multinomial Event Model"></a>Naive Bayes and Multinomial Event Model</h4><p>ç°åœ¨æœ‰ä¸€ä¸ªæ–°çš„æ–¹æ³•ï¼Œå°±æ˜¯å¤šé¡¹å¼æ¨¡å‹ã€‚</p><p>ç°åœ¨$x_i \in {1,â€¦,\vert V \vert}$,å…¶ä¸­|V|ä¸ºå­—å…¸çš„é•¿åº¦ã€‚è€Œnä¸ºä¿¡æ¯çš„é•¿åº¦ï¼Œä¹Ÿå°±æ˜¯ç°åœ¨æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾æ•°å·²ç»ä¸ä¸€æ ·äº†ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æ³•ä¿è¯æ¯å°ä¿¡é•¿åº¦ä¸€æ ·ã€‚</p><p>å¯¹å­—å…¸ä¸­æ¯ä¸ªè¯éƒ½è¿›è¡Œç¼–å·ï¼š</p><div class="table-container"><table><thead><tr><th>dictionary id</th><th>1</th><th>2</th><th>â€¦</th><th>1300</th><th>â€¦</th><th>2400</th><th>â€¦</th></tr></thead><tbody><tr><td>word</td><td>a</td><td>aa</td><td>â€¦</td><td>free</td><td>â€¦</td><td>gift</td><td>â€¦</td></tr></tbody></table></div><p>å¤šé¡¹å¼æ¨¡å‹ä¸­æœ‰ä¸‹é¢å‡ ä¸ªå‡è®¾ï¼š</p><ul><li>ä¿¡æ¯éšæœºåœ°è¢«ä»¥æ¦‚ç‡$p(y)$ç”Ÿæˆ</li><li>$x_1,x_2,â€¦,x_n$ç‹¬ç«‹åŒåˆ†å¸ƒ</li><li>$p(x_1,x_2,â€¦,x_n,y) = p(y)\prod _{i=1}^n p(x_i|y)$</li></ul><p>å‡è®¾$p(x_i=k|y)$å¯¹æ‰€æœ‰çš„$k$æ¥è¯´éƒ½ç›¸ç­‰ã€‚<br>åˆ™è¯¥æ¨¡å‹å‚æ•°å¦‚ä¸‹ï¼š</p><ul><li>$\Phi_y = p(y)$ </li><li>$\Phi_{k|y=1} = p(x_j = k|y=1)$ for any $j \in \{1,â€¦,n\}$</li><li>$\Phi_{k|y=1} = p(x_j = k|y=0)$ for any $j \in \{1,â€¦,n\}$</li></ul><p>åˆ™å‡ºç°è®­ç»ƒæ ·æœ¬çš„æ¦‚ç‡ä¸º:</p><script type="math/tex; mode=display">\begin{align}L(\Phi_y,\Phi_{k|y=0},\Phi_{k|y=1}) &= \prod_{i=1}^m p(x_i,y_i)\\&= \prod_{i=1}^m p(y_i)p(x_i|y_i)\\&=\prod_{i=1}^m p(y_i) \prod_{j=1}^{n_i} p(x_{i,j}|y_i;\Phi_y,\Phi_{k|y=1},\Phi_{k|y=0})\end{align}</script><p>å„ä¸ªå‚æ•°å–å€¼å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\Phi_y = \frac 1 m \sum_{i=1}^m \mathbf{1}\{y_i=1\}\\\Phi_{k|y=1} = \frac{\sum_{i=1}^m \mathbf{1}\{y_i=1\}\cdot (\sum_{j=1}^{n_i}\mathbf{1}\{x_i=k\}) +1}{\sum_{i=1}^m \mathbf{1}\{y_i=1\}+|V|}\\\Phi_{k|y=0} = \frac{\sum_{i=1}^m \mathbf{1}\{y_i=0\}\cdot (\sum_{j=1}^{n_i}\mathbf{1}\{x_i=k\}) +1}{\sum_{i=1}^m \mathbf{1}\{y_i=0\}+|V|}</script><p>æœ€åï¼Œé¢„æµ‹å€¼ä¸ºï¼š$p(y=1|x) = \frac{p(x|y=1)p(y)} {p(x|y=1)+p(x|y=0)}$</p>]]></content>
      
      
      <categories>
          
          <category> æ•°æ®å­¦ä¹ è¯¾ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LFD class </tag>
            
            <tag> mathematics </tag>
            
            <tag> classification </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>ä¿¡æ¯è®ºâ€”â€”Basic Conception</title>
      <link href="/2018/10/29/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94Basic-Conception/"/>
      <url>/2018/10/29/%E4%BF%A1%E6%81%AF%E8%AE%BA%E2%80%94%E2%80%94Basic-Conception/</url>
      
        <content type="html"><![CDATA[<p>å¼€ä¸€ä¸ªæ ç›®æ¥è®°å½•ä¿¡æ¯è®ºçš„å­¦ä¹ ã€‚å¸Œæœ›ä»Šå¹´è¿™é—¨è¯¾ä¸è¦æŒ‚æ‰ã€‚<a id="more"></a></p><h2 id="ä¿¡æ¯ç†µ-Entropy"><a href="#ä¿¡æ¯ç†µ-Entropy" class="headerlink" title="ä¿¡æ¯ç†µ(Entropy)"></a>ä¿¡æ¯ç†µ(Entropy)</h2><p>é¦™å†œå®šä¹‰äº†ä¿¡æ¯ç†µï¼Œä»è€Œä¸ºé€šä¿¡æ—¶ä»£å¥ å®šäº†æ•°å­¦åŸºç¡€ã€‚é¦–å…ˆç›´è§‚çš„è¯´æ˜ä¸€ä¸‹ä»€ä¹ˆæ˜¯ä¿¡æ¯ï¼Ÿé¦™å†œè¯´ï¼šä¿¡æ¯æ˜¯ç”¨æ¥æ¶ˆé™¤éšæœºä¸ç¡®å®šæ€§çš„ä¸œè¥¿ã€‚è¿™ä¸ªè¯´æ³•å®åœ¨æ˜¯å¤ªæŠ½è±¡äº†ã€‚</p><p>è€ƒè™‘ä¸¤ä¸ªç®€å•çš„ä¾‹å­ï¼š1,æ˜å¤©å¤ªé˜³ä»ä¸œæ–¹å‡èµ·ï¼›2ï¼Œæ˜å¤©å¤ªé˜³è¦çˆ†ç‚¸ã€‚å“ªå¥è¯åŒ…å«äº†æ›´å¤šçš„ä¿¡æ¯ï¼Ÿ</p><p>ç›´è§‚ä¸Šæ¥è®²ï¼Œæ˜æ˜¾æ˜¯ç¬¬äºŒå¥ã€‚å› ä¸ºç¬¬ä¸€å¥æ˜¯å¥â€œåºŸè¯â€ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¿™æ ·ç†è§£ï¼Œæ¦‚ç‡è¶Šå°çš„äº‹å®ƒçš„ä¿¡æ¯é‡è¶Šå¤§ã€‚å¦‚æœä¸€ä»¶äº‹å‘ç”Ÿæ¦‚ç‡ä¸º1ï¼Œé‚£å®ƒåŒ…å«çš„ä¿¡æ¯ä¸º0.å¦‚æœä¸€ä»¶äº‹å‘ç”Ÿæ¦‚ç‡ä¸º0ï¼Œå®ƒçš„ä¿¡æ¯é‡ä¸º$\infty$.å¦å¤–ï¼Œå¦‚æœä¸¤ä¸ªäº‹ä»¶æ˜¯ç‹¬ç«‹çš„ï¼Œä»–ä»¬ä¸¤ä¸ªå‘ç”Ÿçš„ä¿¡æ¯é‡æ˜¯äºŒè€…ä¹‹å’Œã€‚æ‰€ä»¥å†™æˆå‡½æ•°å½¢å¼ï¼š<br>å¦‚æœæœ‰ä¸¤ä¸ªç‹¬ç«‹äº‹ä»¶a,b,å‡å¦‚f(x)è¡¨ç¤ºxçš„ä¿¡æ¯é‡ï¼Œåˆ™ï¼š</p><script type="math/tex; mode=display">If p(a)>p(b), f(a)< f(b);\\f(0) = \infty, f(1) = 0;f(a,b) = f(a)+f(b)</script><p>é€šè¿‡ä¸Šé¢çš„æ€§è´¨ï¼Œç»“åˆæ¦‚ç‡ï¼Œå…¶å®æˆ‘ä»¬è¿˜æ˜¯æ¯”è¾ƒå®¹æ˜“æƒ³åˆ°çš„æ˜¯logå‡½æ•°ã€‚ f(x) = -log p(x),æ»¡è¶³ä¸Šé¢çš„æ‰€æœ‰æ¡ä»¶ã€‚ä¸è¿‡è¿™æ˜¯ç›´è§‚çŒœæµ‹ï¼Œåé¢å»è¯æ˜ä¸€ä¸‹logå‡½æ•°æ˜¯å”¯ä¸€æ»¡è¶³ä¿¡æ¯çš„å®šä¹‰çš„å‡½æ•°ã€‚</p><p>ç°åœ¨è¯´æ˜ä¸€ä¸‹ä¿¡æ¯ç†µçš„å®šä¹‰ï¼š$H(X) =- \sum _{x \in \mathcal{X}} p(x) \log p(X)$.ä¿¡æ¯ç†µæ˜¯ç”¨æ¥è¡¡é‡ä¸€ä¸ªç³»ç»Ÿçš„ä¿¡æ¯é‡ï¼Œå¦‚æœåŸºäºä¹‹å‰å¯¹äº‹ä»¶ä¿¡æ¯çš„å®šä¹‰çš„è¯ï¼Œä»ç›´è§‚æ¥è®²è¿™ä¸ªå®šä¹‰æ˜¯åˆç†çš„ã€‚ç°åœ¨è¯æ˜å®ƒä¸å…‰æ˜¯åˆç†çš„ï¼Œè€Œä¸”æ˜¯å”¯ä¸€çš„ã€‚</p><p>é¦™å†œç»™å‡ºä¿¡æ¯ç†µå‡½æ•°æ»¡è¶³çš„ä¸‰ä¸ªæ¡ä»¶ï¼š</p><ul><li>è¿ç»­æ€§</li><li>ç­‰æ¦‚æ—¶çš„å•è°ƒé€’å¢ç‰¹æ€§</li><li>å¯åŠ æ€§</li></ul><p>ç”¨æ•°å­¦è¯­è¨€æè¿°å¦‚ä¸‹ï¼š</p><ul><li>$H(X) = H(P_x)$ is continuous on $P_x$.</li><li>$g(N) - f(\frac 1 N,\frac 1 N,â€¦,\frac 1 N)$. If $M &gt; N, g(M)&gt;g(N).$</li><li>$f(P_1,p_2,â€¦,P_N) = f(P_1+P_2+â€¦+P_M,P_{M+1},â€¦,P_N) + (P_1+â€¦+P_M)f(P_1â€™+P_2â€™+â€¦+P_Mâ€™),P_iâ€™ = \frac{P_i}{\sum_{j=1} ^M P_j}$</li></ul><p>ç°åœ¨æ ¹æ®ä¸Šé¢ä¸‰ä¸ªå‡è®¾æ¥è¯æ˜æˆ‘ä»¬ä¹‹å‰çŒœæµ‹çš„ä¿¡æ¯åº¦é‡å‡½æ•°ä¸å…‰æ˜¯æ­£ç¡®çš„è€Œä¸”æ˜¯å”¯ä¸€çš„ã€‚</p><p>é¦–å…ˆè€ƒè™‘å‡åŒ€åˆ†å¸ƒçš„æƒ…å†µï¼š</p><p>$X~Unif:$</p><script type="math/tex; mode=display">\begin{align}g(MN) &= f(\frac 1 {MN},...,\frac{1}{MN})\\&=f(\frac 1 M,\frac 1 {MN},...,\frac 1 {MN}) + \frac 1 M (\frac 1 N,...,\frac 1 N)\\ &= f(\frac 1 M,...,\frac 1 M) +\sum_{i=1}^M \frac 1 M ( \frac 1 N ,...,\frac 1 N)\\ &= g(M) + g(N)\end{align}</script><p>ç”±ä»¥ä¸Šçš„æ¨è®ºï¼š</p><script type="math/tex; mode=display">g(S^m) = g(S) \cdot g(S^{m-1}) = mg(s) ---------------------(1)</script><p>å–å››ä¸ªæ­£æ•´æ•°$s,m,t,n \in N$,ä½¿å¾—$s^m \leq t^n &lt; s^{m+1}$,ç”±äºå•è°ƒå¢å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">g(s^m) \leq g(t^n) < g(s^{m+1}) \\mg(s) \leq ng(t) < (m+1)g(s) \\\frac m n \leq \frac {g(t)}{g(s)} < \frac {m+1}{n}</script><p>å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">\left\vert \frac m n - \frac {g(t)}{g(s)} \right\vert \leq \frac 1 n ---------(2)</script><p>å¦‚æœè®²ä¸Šé¢çš„4ä¸ªæ•´æ•°åº”ç”¨åˆ°logå‡½æ•°ä¸Šï¼Œå¯ä»¥å¾—åˆ°:</p><script type="math/tex; mode=display">m\log s\leq n\log t < (m+1) logs\\\left\vert \frac m n - \frac {\log t }{\log s} \right\vert \leq \frac 1 n -------(3)</script><p>åˆ©ç”¨$|a Â± b| \leq |a|+ |b|$,ç»“åˆä¸ç­‰å¼(2),(3)ï¼š</p><script type="math/tex; mode=display">\left \vert \frac {g(t)}{g(s)} - \frac {\log t}{\log s} \right \vert \leq \frac 2 n</script><p>å½“Nå–è¶³å¤Ÿå¤§æ—¶ï¼Œ$\frac {g(t)}{g(s)} -&gt; \frac {\log t}{\log s}$</p><p>å› æ­¤æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š$g(t) = C \log t = -\sum _{i=1}^T \frac 1 t \log \frac 1 t$.</p><p>ä¸‹æ¥æˆ‘ä»¬è¦è€ƒè™‘éå‡åŒ€åˆ†å¸ƒçš„æƒ…å†µã€‚</p><p>å‡è®¾$X~P_x(x)$,ä»¤$P(n) = \frac{m_n}{\sum_{i=1}^N m_i} = \frac{m_n}{M}$.</p><p>å®é™…ä¸Šä¸Šé¢å„ä¸ªå­—æ¯æ„æ€å¯ä»¥ç”¨æ‘¸çƒæ¥è€ƒè™‘ï¼š$m_1$ä¸ªçº¢çƒï¼Œ$m_2$ä¸ªç»¿çƒï¼Œâ€¦,å…±Mä¸ªçƒ,Nç§é¢œè‰²ï¼Œè€Œ$P(1)$ä¹Ÿå°±æ˜¯æ‘¸åˆ°çº¢çƒçš„æ¦‚ç‡ã€‚è€ƒè™‘ï¼š</p><script type="math/tex; mode=display">\begin{align}g(M) &= f(\frac 1 M,...,\frac 1 M)\\&=f(\frac {m_1} {M},\frac {m_2}{M},...,\frac{m_N}{M}) +\sum _{i=1}^N \frac {m_i} {M}f(\frac {1}{m_i},...,\frac {1}{m_i})\\&= f(P_1,P_2,...,P_N) + \sum_{i=1}^N P_ig(m_i)\end{align}</script><p>å¯¹ä¸Šé¢ç­‰å¼ç¨åŠ å˜å½¢ï¼š</p><script type="math/tex; mode=display">\begin{align}f(P_1,P_2,...,P_N) &= g(M) - \sum_{i=1}^N P_ig(m_i)\\&= C \log M (\sum_{i=1}^N P_i)- \sum_{i=1}^N P_ig(m_i)\\&= C\sum _{i=1}^N P_i(\log M - \log m_i )\\&= -C\sum_{i=1}^N P_i \log P_i\end{align}</script><p>è¿™å°±å¾—åˆ°äº†æˆ‘ä»¬å¯¹ç†µçš„åº¦é‡å‡½æ•°çš„å½¢å¼ã€‚åœ¨ä¸åŒçš„ä¿¡æ¯åº¦é‡ä¸­å¸¸æ•°Cä»¥åŠlogçš„åº•æ•°æ˜¯ä¸åŒçš„ï¼Œè€Œæœ€å¸¸ç”¨çš„logåº•æ•°ä¸º2ï¼Œä¹Ÿå°±äº§ç”Ÿäº†bitã€‚</p><p>ç°åœ¨Xçš„åˆ†å¸ƒæ˜¯æœ‰ç†æ•°ï¼Œä½†æ˜¯å³ä½¿æ˜¯æ— ç†æ•°ä¹Ÿæ˜¯æˆç«‹çš„ã€‚å…·ä½“çš„è¯æ˜å°±ä¸å±•å¼€äº†ã€‚</p><p>å¯¹äºä¿¡æ¯åº¦é‡çš„å‡è®¾æ¡ä»¶ï¼Œå®é™…ä¸Šé¦™å†œçš„å®šä¹‰ä¸æ˜¯å”¯ä¸€çš„ã€‚æ•°å­¦å®¶A.I.Khinchinæ›¾ç»ç»™å‡ºè¿™æ ·çš„å‡è®¾ï¼š</p><ul><li>è¿ç»­æ€§</li><li>å¯åŠ æ€§</li><li>æå€¼æ¡ä»¶<script type="math/tex; mode=display">max f(p_1,p_2,...,p_N) = f(\frac 1 N,...,\frac 1 N)</script></li><li>é›¶æ¦‚ç‡äº‹ä»¶ä¸å½±å“ä¸ç¡®å®šæ€§<script type="math/tex; mode=display">f(p_1,p_2,...,p_N) = f(p_1,p_2,...,p_N,0)</script></li></ul><p>å®é™…ä¸Šè¿™ä¸ªæ¡ä»¶ä¸­çš„3,4æ¡ä»¶ç­‰ä»·äºé¦™å†œçš„ç¬¬äºŒä¸ªæ¡ä»¶ã€‚è¯æ˜å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display"> f(\frac 1 N,...,\frac 1 N)  =  f(\frac 1 N,...,\frac 1 N,0)< f(\frac 1 {N+1},\frac 1 {N+1},...,\frac 1 {N+1})</script><p>è€Œä¸Šå¼å®é™…ä¸Šå°±æ˜¯ç­‰æ¦‚æ—¶å€™çš„å•è°ƒæ€§ã€‚</p><h3 id="è”åˆç†µ"><a href="#è”åˆç†µ" class="headerlink" title="è”åˆç†µ"></a>è”åˆç†µ</h3><p>Definition:<br>$H(X,Y) = -\sum _{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} P(x,y) \log P(x,y)$</p><p>$H(X,Y)$å®é™…ä¸Šæ˜¯Xä¸Yç³»ç»Ÿçš„è”åˆåŒ…å«çš„ä¿¡æ¯é‡ã€‚éœ€è¦è€ƒè™‘çš„é—®é¢˜ï¼š$H(X,Y) ? H(X)+H(Y)$ã€‚</p><h3 id="æ¡ä»¶ç†µ"><a href="#æ¡ä»¶ç†µ" class="headerlink" title="æ¡ä»¶ç†µ"></a>æ¡ä»¶ç†µ</h3><p>Definition:</p><script type="math/tex; mode=display">\begin{align}H(Y|X) &= \sum_{x \in \mathcal{X}} p(x)H(Y|X=x)\\ &= - \sum_{x \in \mathcal{X}}p(x) \sum_{y \in \mathcal{Y}} p(y|x)\log p(y|x)\\ &= - \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x,y) \log p(y|x)\end{align}</script><p>è¯·ä¸è¦ä»¥ä¸º$H(Y|X) = -\sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(y|x) \log p(y|x)$.</p><p>æ¡ä»¶ç†µçš„å®é™…ä¸Šæ˜¯çŸ¥é“Xçš„ä¿¡æ¯ä¹‹åï¼ŒYçš„å‰©ä½™ä¿¡æ¯é‡ã€‚</p><p>å€¼å¾—æ³¨æ„çš„æ˜¯H(Y|X=x)ä¸æ˜¯ä¸€ä¸ªæ¡ä»¶ç†µï¼Œæ¡ä»¶ç†µæ˜¯æ ¹æ®ä¸¤ä¸ªç³»ç»Ÿè€Œè¨€çš„ï¼Œè€Œåœ¨è¿™ä¸ªä¸­ï¼Œå®é™…ä¸Šåªæœ‰ä¸€ä¸ªåŠ äº†æ¡ä»¶çš„ç³»ç»Ÿï¼š$Y|X=x$.</p><p>å½“X,Yç‹¬ç«‹æ—¶ï¼Œ$H(Y|X) = H(Y),H(X|Y) = H(X)$.åœ¨ç›´è§‰ä¸Šè¿™ä¸ªä¹Ÿæ˜¯éå¸¸æ­£ç¡®çš„ã€‚åŒæ—¶æˆ‘ä»¬ç”¨ç‰©ç†æ€ç»´ç†è§£è¿™ä»¶äº‹ï¼Œåº”è¯¥å¯ä»¥å¾—åˆ°ï¼š$H(X,Y) = H(X) + H(Y|X)$.</p><p>ç°åœ¨ç”¨ä¸¥æ ¼çš„æ•°å­¦è¯æ˜æ¥è¯æ˜ä¸Šé¢çš„å¼å­æ˜¯æˆç«‹çš„ï¼š</p><p>é¦–å…ˆæˆ‘ä»¬å¸Œæœ›ç®€åŒ–ä¸€ä¸‹ç†µçš„å†™æ³•ï¼š$H(X) = \mathbb{E}_x \log \frac 1 {p(X)}$($\mathbb{E}$è¡¨ç¤ºæ•°å­¦æœŸæœ›).</p><p>æœ‰äº†è¿™ä¸ªå†™æ³•ï¼Œè¯æ˜å˜å¾—å°±å¾ˆç®€å•ï¼š</p><script type="math/tex; mode=display">P(X,Y) = P(X)P(Y|X)\\\log P(X,Y) = \log P(X)+ \log P(Y|X)\\\mathbb{E}_{X,Y}P(X,Y) = \mathbb{E}_{X,Y} \log P(X) + \mathbb{E}_{X,Y} \log P(Y|X)\\H(X,Y) = H(X) + H(Y|X)</script><p>ä¸Šè¿°è¯æ˜æ²¡æœ‰å†™æ¸…è´Ÿå·ï¼Œä½†æ˜¯å®Œå…¨ä¸å½±å“ç»“æœã€‚</p><p>æ ¹æ®ä¸Šé¢å¯ä»¥æ¨æ–­å‡ºåˆ«çš„ç»“è®º(å¦‚æœåˆ©ç”¨ç”»å›¾å°±æ›´å¥½ç†è§£)ï¼š</p><ul><li><p>$H(X,Y|Z) = H(X|Z) + H(Y|X,Z)$</p><p>æ¨è®ºï¼š$H(X_1,X_2,â€¦,H_n) =  \sum _{i=1} ^N H(X_i|X_{i-1},â€¦,X_1)$</p></li></ul><h3 id="ä¿¡æ¯ç†µçš„æ€§è´¨"><a href="#ä¿¡æ¯ç†µçš„æ€§è´¨" class="headerlink" title="ä¿¡æ¯ç†µçš„æ€§è´¨"></a>ä¿¡æ¯ç†µçš„æ€§è´¨</h3><ul><li><p>å¯¹ç§°æ€§ï¼š<br>$H(P_1,P_2,â€¦,P_N) = H(P_{k(1)},P_{k(2)},â€¦,P_{k(N)})$</p></li><li><p>éè´Ÿæ€§ï¼š $H(P) \geq 0$</p></li><li><p>å¯åŠ æ€§ï¼š $H(X,Y) = H(X) + H(Y|X)$</p></li><li><p>æ¡ä»¶å‡å°‘ç†µï¼š $H(X|Y) \leq H(X)$<br>(æ¡ä»¶ç†µï¼Œè€Œéé’ˆå¯¹Yçš„æŸä¸€ç‰¹å®šå–å€¼ï¼Œä¹Ÿå°±æ˜¯ä¸æ„å‘³ç€$H(X|Y=y) \leq H(X)$)</p></li><li><p>æœ€å¤§ç¦»æ•£ç†µå®šç†ï¼š$H(p_1,p_2,â€¦,p_n) \leq H(\frac 1 N ,â€¦,\frac 1 N) = \log N = \log |X|$.</p></li></ul><h2 id="äº’ä¿¡æ¯-Mutual-Information"><a href="#äº’ä¿¡æ¯-Mutual-Information" class="headerlink" title="äº’ä¿¡æ¯(Mutual Information)"></a>äº’ä¿¡æ¯(Mutual Information)</h2><p>äº’ä¿¡æ¯æè¿°äº†ä¸¤ä¸ªç³»ç»Ÿä¹‹é—´çš„å…³ç³»ã€‚äº’ä¿¡æ¯çš„å®šä¹‰ï¼š$I(X;Y) = H(X) - H(X|Y)$.</p><p>ä¹Ÿå¯ä»¥ç›´æ¥å®šä¹‰äº’ä¿¡æ¯ä¸º:$I(X;Y)= \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}}p(x,y) \log \frac{p(x,y)}{p(x)p(y)}$.</p><p>äº’ä¿¡æ¯è¿˜æœ‰å¦ä¸€ç§å†™æ³•ï¼š$I(X;Y) = I(p;Q) =  \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x)q(y|x)\log \frac{q(y|x)}{\sum _{x \in \mathcal{X}}p(x)q(y|x)}$.å¼å­ä¸­Qä¸ºçŸ¥é“xåyçš„æ¡ä»¶æ¦‚ç‡çŸ©é˜µã€‚</p><p>ä»å¦ä¸€æ–¹é¢æ¥è¯´ï¼š$I(X;Y) = H(X)+H(Y) - H(X,Y)$.ä»ç›´è§‚ä¸Šä¹Ÿæ˜¯å¾ˆå®¹æ˜“ç†è§£çš„ã€‚</p><p>å¾ˆæ˜æ˜¾äº’ä¿¡æ¯$I(X;Y) = I(Y;X)$,å…·æœ‰å¯¹ç§°æ€§ã€‚å¦‚æœXä¸Yç‹¬ç«‹ï¼Œ$I(X;Y)=0$ï¼›å¦‚æœXä¸Yä¸€ä¸€å¯¹åº”ï¼Œåˆ™$I(X;Y) = H(X) = H(Y)$.</p><p>äº’ä¿¡æ¯å’Œä¿¡é“å®¹é‡æœ‰ç€åƒä¸ä¸‡ç¼•çš„å…³ç³»ã€‚ä¿¡æºè¿™è¾¹ä¸ºXï¼Œä¿¡é“å°¾éƒ¨ä¸ºYï¼Œé‚£ä¹ˆI(X;Y)è¶Šå¤§çš„è¯ä¿¡é“å®¹é‡è¶Šå¤§ã€‚</p><h3 id="å¤šå˜é‡çš„äº’ä¿¡æ¯"><a href="#å¤šå˜é‡çš„äº’ä¿¡æ¯" class="headerlink" title="å¤šå˜é‡çš„äº’ä¿¡æ¯"></a>å¤šå˜é‡çš„äº’ä¿¡æ¯</h3><p>å¦‚æœæœ‰éšæœºå˜é‡Xï¼ŒYï¼ŒZï¼Œåˆ™$I(X;Y,Z) = H(X) - H(X|Y,Z) = H(Y,Z) - H(Y,Z|X)$.</p><p>æˆ–è€…ä»æ•°å­¦å®šä¹‰ä¸Šï¼š</p><script type="math/tex; mode=display">I(X;Y,Z)= \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} \sum_{z \in \mathcal{Z}}p(x,y,z) \log \frac{p(x|y,z)}{p(x)} = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} \sum_{z \in \mathcal{Z}}p(x,y,z) \log \frac{p(x,y,z)}{p(x)p(y,z)}</script><h3 id="æ¡ä»¶äº’ä¿¡æ¯"><a href="#æ¡ä»¶äº’ä¿¡æ¯" class="headerlink" title="æ¡ä»¶äº’ä¿¡æ¯"></a>æ¡ä»¶äº’ä¿¡æ¯</h3><p>å®šä¹‰I(X;Y|Z)ä¸ºçŸ¥é“Zçš„ä¿¡æ¯ä¹‹åï¼ŒXä¸Yä¹‹é—´çš„äº’ä¿¡æ¯ã€‚å®ƒçš„å®šä¹‰å¦‚ä¸‹ï¼š</p><p>$I(X;Y|Z) = H(X|Z) - H(X|Y,Z)= H(Y|Z) - H(Y|X,Z)$.</p><p>ä¹Ÿå¯ä»¥ç›´æ¥å®šä¹‰æ¡ä»¶äº’ä¿¡æ¯ä¸º:</p><script type="math/tex; mode=display">I(X;Y|Z) = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} \sum_{z \in \mathcal{Z}} p(x,y,z) \log {p(x,y|z)}{p(x|z)p(y|z)}</script><script type="math/tex; mode=display">I(X;Y|Z) = H(X|Z) - H(X,Y|Z) + H(Y|Z)</script><script type="math/tex; mode=display">\begin{align}I(X;Y|Z) = &H(X,Z) - H(Z) - H(X,Y,Z) + H(Z)+H(Y,Z) - H(Z)\\&=H(X,Z)+H(Y,Z)-H(X,Y,Z)-H(Z)\end{align}</script><p>æœ‰æ¡ä»¶å‡å°‘ç†µï¼Œä½†æ˜¯æ²¡æœ‰æ¡ä»¶å‡å°‘äº’ä¿¡æ¯ã€‚æ¡ä»¶äº’ä¿¡æ¯éè´Ÿã€‚</p><ul><li><p>å¯¹ç§°æ€§ $I(X;Y)=I(Y;X)$</p></li><li><p>éè´Ÿæ€§ $I(X;Y) \geq 0$</p></li><li><p>æå€¼æ€§ $I(X;Y) \leq min\{H(X),H(Y)\}$</p></li><li><p>å¯åŠ æ€§ $I(X_1,X_2,â€¦,X_n;Y) = \sum_{i=1} ^n I(X_i;Y|X_{i-1},â€¦,X_1)$</p></li></ul><p>å¯åŠ æ€§ç”¨ç”»å›¾æ¥è¡¨ç¤ºçš„è¯ä¹Ÿæ›´æ¸…æ™°ã€‚</p><p>ä¸‹é¢ä»‹ç»ä¸€ä¸ªç”¨äº’ä¿¡æ¯æ¥è§£å†³çš„é¢˜ç›®ã€‚å¦‚æœæœ‰25ä¸ªå°çƒï¼Œå…¶ä¸­åªæœ‰ä¸€ä¸ªé‡é‡å’Œå…¶ä»–çš„ä¸ä¸€è‡´ã€‚æœ‰ä¸€ä¸ªå¤©å¹³ï¼Œå¯ä»¥æ£€æµ‹è½»é‡ï¼Œæˆ–è€…ä¸€æ ·é‡ã€‚é‚£ä¹ˆæœ€å°‘ç”¨å‡ æ¬¡æ‰èƒ½ä¸€å®šæ‰¾å‡ºä¸ä¸€æ ·é‡é‡çš„å°çƒ?ä¹Ÿè®¸ä½ èƒ½æ‰¾åˆ°3æ¬¡çš„æ–¹æ³•ï¼Œä½†æ˜¯å¦‚ä½•è¯æ˜2æ¬¡æ˜¯ä¸å¯ä»¥ï¼Ÿ</p><p>åªæœ‰ä¸€ä¸ªé‡é‡ä¸ä¸€æ ·ï¼Œå¦‚æœæ˜¯å‡åŒ€åˆ†å¸ƒï¼Œåˆ™ä¿¡æ¯ç†µä¸º$H(X) = \log_2^{25}$.å‡è®¾è¿›è¡Œäº†Næ¬¡å®éªŒï¼Œåˆ™äº’ä¿¡æ¯ä¸º$I(X^N;X)$.</p><script type="math/tex; mode=display">\begin{align}I(X^N;X) &\leq H(X^N) \leq H(X_1,X_2,...,X_N)\\       &\leq H(X_1)+...+H(X_n)\\       &= N\log _2^3.\\\end{align}</script><p>æˆ‘ä»¬å¸Œæœ›çš„æ˜¯äº’ä¿¡æ¯å’ŒåŸæ¥çš„ç†µä¸€æ ·å¤§ï¼Œè¿™æ ·æ‰èƒ½ååº”å®ƒçš„æƒ…å†µã€‚</p><script type="math/tex; mode=display">\begin{align}H(X) = I(X^N;X) \leq N \log_2^3\\      \log 2^{25} \leq N log_2^3\\      N \geq 3\end{align}</script><p>æ‰€ä»¥å¯ä»¥å¾—åˆ°Nå¿…é¡»å¤§äºç­‰äº3.ä¿¡æ¯è®ºå¾ˆå¤šæ—¶å€™ç»™äº†æˆ‘ä»¬ä¸€ä¸ªä¸Šç•Œã€‚</p><h2 id="é‰´åˆ«ä¿¡æ¯-K-L-divergence"><a href="#é‰´åˆ«ä¿¡æ¯-K-L-divergence" class="headerlink" title="é‰´åˆ«ä¿¡æ¯(K-L divergence)"></a>é‰´åˆ«ä¿¡æ¯(K-L divergence)</h2><p>é‰´åˆ«ä¿¡æ¯è¡¨ç¤ºä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œå®šä¹‰å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">D(p\Vert q) = \sum_{x \in \mathcal{X}}p(x)\log \frac{p(x)}{q(x)}</script><ul><li>å½“p = qçš„æ—¶å€™ï¼Œ$D(p\Vert q) =0$. </li><li>é‰´åˆ«ä¿¡æ¯å…·æœ‰éè´Ÿæ€§ã€‚</li></ul><p>ä½†æ˜¯é‰´åˆ«ä¿¡æ¯ä¸èƒ½è¯´æ˜¯ä¸¥æ ¼çš„ä¿¡æ¯åº¦é‡ã€‚å®ƒä¸å…·æœ‰å¯¹ç§°æ€§ï¼Œä¹Ÿä¸æ»¡è¶³ä¸‰è§’ä¸ç­‰å¼ï¼ˆï¼Ÿï¼Ÿï¼‰ã€‚</p><h3 id="é‰´åˆ«ä¿¡æ¯ï¼Œç†µï¼Œäº’ä¿¡æ¯"><a href="#é‰´åˆ«ä¿¡æ¯ï¼Œç†µï¼Œäº’ä¿¡æ¯" class="headerlink" title="é‰´åˆ«ä¿¡æ¯ï¼Œç†µï¼Œäº’ä¿¡æ¯"></a>é‰´åˆ«ä¿¡æ¯ï¼Œç†µï¼Œäº’ä¿¡æ¯</h3><ul><li>$H(X) = \log |X| - D(\mathbf{p}\Vert \mathbf{u})$</li></ul><p>ä¸Šå¼ä¸­uè¡¨ç¤ºå¹³å‡åˆ†å¸ƒã€‚è¯æ˜å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\begin{align}H(X) &= -\sum _{x \in \mathcal{X}}p(x)\log p(x) \\&=\log N - \log N+ \sum_{x \in \mathcal{X}}p(x) \log p(x)\\&= \log |X| + \sum_{x \in mathcal{X}} p(x)\log \frac 1 N + \sum_{x \in \mathcal{X}}p(x) \log p(x)\\&= \log |X| -  D(\mathbf{p}\Vert \mathbf{u})\end{align}</script><ul><li>$I(X;Y) = D(p(x,y)\Vert p(x)p(y)) $</li></ul><p>è¿™ä¸ªè¯æ˜ä¸¤è¾¹æ ¹æ®å®šä¹‰å±•å¼€å°±å¯ä»¥ç›´æ¥å¾—åˆ°ã€‚</p><p>ä¿¡æ¯è®ºçœŸæ˜¯æŠ½è±¡å•Šã€‚</p>]]></content>
      
      
      <categories>
          
          <category> ä¿¡æ¯è®º </category>
          
      </categories>
      
      
        <tags>
            
            <tag> information theory </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>å›¾å½¢å­¦â€”â€”Homework1</title>
      <link href="/2018/10/26/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E2%80%94%E2%80%94Homework1/"/>
      <url>/2018/10/26/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E2%80%94%E2%80%94Homework1/</url>
      
        <content type="html"><![CDATA[<p>å®Œæˆç¬¬ä¸€ä¸ªä½œä¸šï¼ˆå®é™…ä¸Šæ˜¯ç¬¬äºŒä¸ªï¼‰ã€‚è¿™ä¸ªä½œä¸šè¿˜æ˜¯æ¯”è¾ƒè´¹åŠ²çš„ï¼Œä¸€ä¸ªåŸå› æ˜¯å¯¹OpenGLååˆ†ä¸ç†Ÿæ‚‰ã€‚<br><a id="more"></a></p><p>é¦–å…ˆï¼ŒopenGLä¸­ï¼ŒZè½´æŒ‡å‘å±å¹•å¤–ï¼ŒYè½´æŒ‡å‘ä¸Šä¾§ï¼ŒXè½´æŒ‡å‘å³ä¾§ã€‚è¿™æ˜¯ä¸€ä¸ªéœ€è¦æ³¨æ„çš„åœ°æ–¹ã€‚</p><p>å…¶æ¬¡ï¼ŒopenGLä¸­3Dçš„å‘ˆç°ï¼Œå®é™…ä¸Šæ˜¯æ¨¡æ‹Ÿä¸€ä¸ªç›¸æœºå†çœ‹è¿™ä¸ªç‰©å“ã€‚åœ¨ä½œä¸šä¸­ï¼Œå¯ä»¥çŸ¥é“çš„æ˜¯èŒ¶å£¶æ”¾åœ¨ä¸–ç•Œåæ ‡çš„åŸç‚¹ï¼Œè€Œåˆšå¼€å§‹çš„è§†ç‚¹æ˜¯(0,0,5).</p><p>ä½œä¸šè¦å®Œæˆå·¦å³æ—‹è½¬ï¼Œè€Œå®é™…ä¸Šå°±æ˜¯è§†ç‚¹ç›¸å¯¹äºè§†ç‚¹ç›¸å¯¹äºä¸–ç•Œåæ ‡è¦è½¬åŠ¨ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘è§„å®šçš„è½¬åŠ¨æ–¹å‘æ˜¯å‘å·¦çš„è¯ï¼Œçœ‹åˆ°èŒ¶å£¶çš„å·¦ä¾§ï¼Œä¹Ÿå°±æ˜¯è§†ç‚¹å‘å·¦ä¾§è½¬åŠ¨ï¼Œè€Œä¸æ˜¯è®©èŒ¶å£¶å‘å·¦ä¾§è½¬åŠ¨ï¼ˆé‚£æ ·çš„è¯æˆ‘ä»¬èƒ½çœ‹åˆ°çš„å®é™…ä¸Šå˜æˆäº†å³ä¾§ï¼‰ã€‚å…¶ä»–æ–¹å‘ä¹Ÿæ˜¯ä¸€æ ·çš„é“ç†ã€‚</p><p>ä¸ºäº†è®©è§†ç‚¹çš„åæ ‡è½¬åŠ¨ï¼Œé¦–å…ˆè¦å®Œæˆrotateå‡½æ•°çš„å®šä¹‰ï¼Œè¿™ä¸ªå‡½æ•°ç›´æ¥ä½¿ç”¨Rodrigueså…¬å¼å°±å¯ä»¥æ±‚å¾—æ—‹è½¬çŸ©é˜µã€‚ä»£ç å¦‚ä¸‹ï¼š<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mat3 Transform::rotate(<span class="keyword">const</span> <span class="keyword">float</span> degrees, <span class="keyword">const</span> vec3&amp; axis) &#123;</span><br><span class="line">  <span class="comment">// YOUR CODE FOR HW1 HERE</span></span><br><span class="line">  <span class="comment">// You will change this return call</span></span><br><span class="line">vec3 _axis = glm::normalize(axis);</span><br><span class="line"><span class="keyword">float</span> theta = degrees/<span class="number">360</span>* pi;</span><br><span class="line">mat3 a_ta = mat3(_axis.x*_axis.x, _axis.x*_axis.y, _axis.x*_axis.z, _axis.y*_axis.x, _axis.y*_axis.y, _axis.y*_axis.z, _axis.z*_axis.x, _axis.z*_axis.y, _axis.z*_axis.z);</span><br><span class="line">mat3 I = mat3(<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">mat3 Astar = mat3(<span class="number">0</span>, -_axis.z, _axis.y, _axis.z, <span class="number">0</span>, -_axis.x, -_axis.y, _axis.x, <span class="number">0</span>);</span><br><span class="line"><span class="keyword">return</span> a_ta + (I - a_ta)*<span class="built_in">cos</span>(theta) + Astar * <span class="built_in">sin</span>(theta);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>å¾ˆå°´å°¬çš„æ˜¯æˆ‘ä¸çŸ¥é“OpenGLä¸­æœ‰ä»€ä¹ˆç®€æ´çš„åŠæ³•è®¡ç®—$\mathbf{a}\mathbf{a}^T$ï¼Œå› æ­¤ç”¨æ‰‹æŠŠå®ƒæ•²å‡ºæ¥äº†ã€‚</p><p>ç¬¬äºŒä¸ªå°±æ˜¯å®šä¹‰å‘å·¦çš„å‡½æ•°ã€‚OpenGLä¸­ï¼Œé™¤äº†è§†ç‚¹åæ ‡ä»¥å¤–è¿˜æœ‰ä¸€ä¸ªupå‘é‡ï¼Œè¡¨ç¤ºè§†ç‚¹åæ ‡å‘ä¸Šçš„æ–¹å‘ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å¤´å‘æ‰€æŒ‡çš„æ–¹å‘ã€‚å› ä¸ºæˆ‘ä»¬éœ€è¦ç”¨ä¸¤ä¸ªå‘é‡æ¥ç¡®å®šè§†ç‚¹åæ ‡ç³»ï¼ˆè¿™ä¸ªåé¢å†è¯´ï¼‰ã€‚æ‰€ä»¥æˆ‘ä»¬åœ¨ç§»åŠ¨è§†ç‚¹çš„æ—¶å€™ä¹Ÿè¦ç§»åŠ¨upå‘é‡ã€‚</p><p>å·¦å³è½¬çš„æ—¶å€™ï¼Œå¾ˆå®¹æ˜“ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ”¹å˜upå‘é‡ï¼Œå› ä¸ºæˆ‘ä»¬å°±æ˜¯ç»•ç€upå‘é‡è½¬çš„ã€‚æ‰€ä»¥è¿™ä¸ªå¾ˆç®€å•å°±å¯ä»¥å†™å‡ºæ¥ï¼ˆéœ€è¦æ³¨æ„çš„æ˜¯è½¬åŠ¨è§’åº¦çš„æ–¹å‘å’Œè½¬åŠ¨è½´å‘é‡ä¹Ÿæ˜¯ç¬¦åˆå³æ‰‹å®šåˆ™çš„ï¼Œè¿™æ˜¯ä¹‹å‰æ¨å…¬å¼çš„ç»“æœã€‚å‘å·¦è½¬çš„è¯ï¼Œ$\theta$åº”è¯¥å–è´Ÿï¼Œç„¶è€Œä»£ç ä¸­<strong>æˆ‘å¹¶æ²¡æœ‰å–è´Ÿï¼Œä¾ç„¶å¾—åˆ°æƒ³è¦çš„ç»“æœ</strong>ï¼‰ã€‚</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> Transform::left(<span class="keyword">float</span> degrees, vec3&amp; eye, vec3&amp; up) &#123;</span><br><span class="line">  <span class="comment">// YOUR CODE FOR HW1 HERE</span></span><br><span class="line">mat3 r = rotate(degrees,up);</span><br><span class="line">eye = r * eye;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>è€Œä¸Šä¸‹è½¬çš„æ—¶å€™å°±éœ€è¦æ³¨æ„äº†ï¼Œæˆ‘ä»¬ç»•çš„è½´å°±å˜äº†ï¼Œå®é™…ä¸Šä¸Šä¸‹è½¬çš„æ—¶å€™æˆ‘ä»¬ç»•çš„è½´æ˜¯upå‘é‡ä¸è§†ç‚¹å‘é‡å‰ä¹˜çš„ç»“æœï¼Œè€Œupå‘é‡è½¬åŠ¨åä¹Ÿè¦ä½œç›¸åº”çš„è½¬å˜ã€‚è¿˜è®°å¾—upå‘é‡ä¸eyeå§‹ç»ˆå‚ç›´ï¼Œé‚£ä¹ˆå¯ä»¥çœ‹ä½œæ˜¯å®ƒçš„æ³•å‘é‡ã€‚å› æ­¤ï¼Œæ³•å‘è½¬æ¢å°±ç”¨åˆ°äº†:$(M^{-1})^T$.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span> Transform::up(<span class="keyword">float</span> degrees, vec3&amp; eye, vec3&amp; up) &#123;</span><br><span class="line">  <span class="comment">// YOUR CODE FOR HW1 HERE </span></span><br><span class="line">mat3 r = rotate(degrees, -glm::cross(eye, up));</span><br><span class="line">up = glm::transpose(glm::inverse(r))*up;<span class="comment">//up vector is not easy to compute</span></span><br><span class="line">eye = r * eye;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>æœ€åä¸€ä¸ªå°±æ˜¯lookAtå‡½æ•°ã€‚è¦æƒ³å†™å‡ºæ¥lookAtå‡½æ•°ï¼Œé¦–å…ˆè¦çŸ¥é“lookAtåœ¨åšä»€ä¹ˆã€‚lookAtå‡½æ•°æ˜¯åšçš„äº‹æƒ…ï¼Œæ˜¯æŠŠèŒ¶å£¶æŠ•å½±åˆ°è§†ç‚¹åæ ‡å½“ä¸­ã€‚å¦‚æœlookAtè¿”å›çš„æ˜¯0å‘é‡ï¼Œé‚£ä¹ˆæˆ‘ä»¬çœ‹åˆ°çš„èŒ¶å£¶çš„å†…éƒ¨ã€‚</p><p>è¿™å°±è¦æ±‚æˆ‘ä»¬è¦å»ºç«‹ä¸€ä¸ªåæ ‡ç³»å‡ºæ¥äº†ã€‚ä¹‹å‰è®²è¿‡å»ºåæ ‡çš„æ–¹æ³•ï¼Œä½†æ˜¯u,v,wåªè¦ç¬¦åˆå³æ‰‹å®šåˆ™å°±å¥½ï¼Œå…¶ä»–çš„ä¸åšè¦æ±‚ã€‚ä½†æ˜¯OpenGLä¸­ï¼Œzè½´æ˜¯æœç€å¹³é¢å¤–çš„ï¼Œå› æ­¤æˆ‘ä»¬å°±å¿…é¡»è§„å®šè§†ç‚¹çš„å‘é‡å°±æ˜¯zè½´çš„æ–¹å‘ï¼Œå¯¹åº”ç€wã€‚æ¥ç€ç”¨å‰ä¹˜ï¼ˆupä¸eyeå‘é‡ï¼‰åšå‡ºuè½´ï¼Œæœå³çš„å‘é‡ï¼Œæœ€åæ±‚å¾—vè½´å³å¯ã€‚è¿™å°±å»ºç«‹äº†è§†ç‚¹åæ ‡ã€‚</p><p>å»ºç«‹è§†ç‚¹åæ ‡ååˆå¦‚ä½•å¾—åˆ°åŸæ¥çš„ç‚¹åœ¨è¯¥åæ ‡ç³»ä¸‹çš„åæ ‡å‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥çœ‹å‡ºæ¥è¿™éœ€è¦ä¸¤æ­¥ï¼šæ—‹è½¬å’Œå¹³ç§»ã€‚æ—‹è½¬å’Œå¹³ç§»æ˜¯ä¸å¯é€†çš„ï¼Œå› æ­¤æˆ‘ä»¬é¦–å…ˆè¦æ³¨æ„é¡ºåºã€‚è¿™ä¸ªé—®é¢˜æœ‰ç‚¹æ£˜æ‰‹ã€‚åœ¨lookAtå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬éœ€è¦åšçš„æ˜¯å…ˆå¹³ç§»å†æ—‹è½¬ï¼ˆä¸ºæ¯›æˆ‘è§‰å¾—æ˜¯å…ˆæ—‹è½¬åå¹³ç§»ï¼Ÿå¯èƒ½æˆ‘å¯¹OpenGLåˆæœ‰ä»€ä¹ˆè¯¯è§£ã€‚å¦‚æœæ˜¯ç§»åŠ¨åæ ‡ç³»çš„è¯æ˜¯å…ˆå¹³ç§»åæ—‹è½¬çš„ï¼‰ã€‚</p><p>ï¼ˆå¥½å§ï¼Œç»è¿‡å®é™…è®¡ç®—äº†ä¹‹åæˆ‘æ˜ç™½äº†ã€‚å…¶å®æƒ³è±¡ç§»åŠ¨ç‚¹çš„è¯æ˜¯æ¯”è¾ƒæŠ½è±¡çš„ï¼Œä½†æ˜¯ç‚¹çš„ç§»åŠ¨å®é™…ä¸Šå°±æ˜¯åæ ‡ç³»çš„ç›¸å¯¹è¿åŠ¨ã€‚å› æ­¤lookAtå‡½æ•°å¯ä»¥çœ‹ä½œå°†ä¸–ç•Œåæ ‡ç³»ç§»åŠ¨åˆ°ç›¸æœºåæ ‡ç³»ã€‚è€Œè¿™ä¸ªæ—¶å€™çš„ç§»åŠ¨æ¯”è¾ƒå®¹æ˜“ç†è§£çš„æ˜¯å…ˆå¹³ç§»åæ—‹è½¬ï¼Œå› ä¸ºå¦‚æœå…ˆæ—‹è½¬äº†ï¼Œå¹³ç§»æ—¶å€™åŠ ä¸Šç›¸æœºåæ ‡å¾—åˆ°çš„å¹¶ä¸æ˜¯åŸç›¸æœºçš„ä½ç½®ï¼Œå› ä¸ºåæ ‡è½´æ–¹å‘å˜äº†ã€‚è€Œå¯¹åº”åˆ°ç‚¹ï¼Œä¸€æ ·ä¹Ÿæ˜¯å…ˆå¹³ç§»åæ—‹è½¬çš„ã€‚</p><p>è‡³äºä¸ºä»€ä¹ˆgluLookAtéœ€è¦ç‰©ä½“çš„ä¸­å¿ƒåæ ‡ï¼Œæˆ‘æ˜¯å› ä¸ºç‰©ä½“æœ¬èº«ä¹Ÿæœ‰ä¸€ä¸ªè‡ªå·±çš„å±€éƒ¨åæ ‡ç³»ï¼Œéœ€è¦ç”¨ä¸­å¿ƒåæ ‡ï¼ˆä¸–ç•Œåæ ‡ï¼‰ï¼Œæ‰èƒ½é€šè¿‡å°†å±€éƒ¨åæ ‡è¿›è¡Œä¸€ä¸ªåç§»ï¼Œå¾—åˆ°ä¸–ç•Œåæ ‡åï¼Œç»§ç»­ä¸Šé¢çš„å¹³ç§»æ—‹è½¬æ“ä½œï¼Œæœ¬é¢˜ä¸­ä¸­å¿ƒåæ ‡ä¸º(0,0,0)ï¼Œæ‰€ä»¥æ²¡æœ‰ä¾¿å®œï¼Œå±€éƒ¨åæ ‡å°±æ˜¯ä¸–ç•Œåæ ‡ï¼‰</p><p>å¦‚æœç†è§£äº†ä¹‹å‰çš„æ—‹è½¬çŸ©é˜µï¼Œæˆ‘ä»¬å°±çŸ¥é“æ—‹è½¬çŸ©é˜µå®é™…ä¸Šå°±æ˜¯åæ ‡ç³»çš„ä¸‰ä¸ªå•ä½å‘é‡ï¼Œè€Œæ—‹è½¬åçš„ç»“æœå°±æ˜¯è¯¥ç‚¹åœ¨è¯¥åæ ‡ç³»çš„åæ ‡å€¼ï¼Œå› æ­¤å¾ˆå®¹æ˜“å¾—åˆ°ï¼š<br>$r = \begin{bmatrix}<br>\mathbf{u}\\<br>\mathbf{v}\\<br>\mathbf{w}<br>\end{bmatrix}$.</p><p>è€Œå¹³ç§»çš„é‡å®é™…ä¸Šå°±æ˜¯å½“å‰$eye$å–è´Ÿã€‚è¿™ä¸ªä¹Ÿå¾ˆå¥½ç†è§£ã€‚ç„¶åå¾—åˆ°äº†å¹³ç§»æ—‹è½¬çŸ©é˜µï¼š<br>$\begin{bmatrix}<br>R_{3 \times 3}&amp; R_{3 \times 3}\mathbf{eye}_{3 \times 1}\\<br>0_{1\times 3}&amp;1<br>\end{bmatrix}$</p><p>è¿™å°±å¾—åˆ°äº†æœ€åçš„lookAtå‡½æ•°ã€‚<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mat4 Transform::lookAt(vec3 eye, vec3 up) &#123;</span><br><span class="line">  <span class="comment">// YOUR CODE FOR HW1 HERE</span></span><br><span class="line">vec3 w = glm::normalize(eye);</span><br><span class="line">vec3 u = glm::normalize(glm::cross(up, eye)) ;</span><br><span class="line">vec3 v = glm::normalize(glm::cross(w, u));</span><br><span class="line">mat3 r = mat3(u,v,w );</span><br><span class="line">vec3 t = vec3(-glm::dot(u,eye), -glm::dot(v,eye), -glm::dot(w,eye));</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; t.x &lt;&lt; t.y &lt;&lt; t.z &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">mat4 result = mat4(r[<span class="number">0</span>][<span class="number">0</span>], r[<span class="number">0</span>][<span class="number">1</span>], r[<span class="number">0</span>][<span class="number">2</span>], t.x, r[<span class="number">1</span>][<span class="number">0</span>], r[<span class="number">1</span>][<span class="number">1</span>], r[<span class="number">1</span>][<span class="number">2</span>], t.y, r[<span class="number">2</span>][<span class="number">0</span>], r[<span class="number">2</span>][<span class="number">1</span>], r[<span class="number">2</span>][<span class="number">2</span>], t.z, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">return</span> glm::transpose(result);</span><br><span class="line">  <span class="comment">// You will change this return call</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>æˆ‘ä¸€ç›´ä¸æ˜ç™½ä¸ºä»€ä¹ˆæœ€åè¦åŠ ä¸€ä¸ªtranspose.</p><p>ç°åœ¨æˆ‘çŸ¥é“äº†OpenGLï¼ˆglmï¼‰ä¸­çŸ©é˜µæ„é€ æ—¶å€™æ˜¯åˆ—ä¼˜å…ˆçš„ï¼Œå¦‚m[0][1],æŒ‡çš„æ˜¯ç¬¬0åˆ—ç¬¬1è¡Œã€‚æ‰€ä»¥æˆ‘æ„é€ å‡ºæ¥çš„æ‰€æœ‰çŸ©é˜µéƒ½åº”è¯¥åŠ ä¸ªè½¬ç½®ï¼Œè¿™ä¹Ÿè§£é‡Šäº†ä¸ºä»€ä¹ˆä¸Šé¢ä»£ç degreeæ²¡æœ‰å–è´Ÿä¾ç„¶å¾—åˆ°äº†æ­£ç¡®çš„ç»“æœã€‚æ­£ç¡®ä»£ç å¦‚ä¸‹ï¼š<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">mat3 Transform::rotate(<span class="keyword">const</span> <span class="keyword">float</span> degrees, <span class="keyword">const</span> vec3&amp; axis) &#123;</span><br><span class="line">  <span class="comment">// YOUR CODE FOR HW1 HERE</span></span><br><span class="line">  <span class="comment">// You will change this return call</span></span><br><span class="line">vec3 _axis = glm::normalize(axis);</span><br><span class="line"><span class="keyword">float</span> theta = degrees/<span class="number">180</span>* pi;</span><br><span class="line">mat3 a_ta = mat3(_axis.x*_axis.x, _axis.x*_axis.y, _axis.x*_axis.z, _axis.y*_axis.x, _axis.y*_axis.y, _axis.y*_axis.z, _axis.z*_axis.x, _axis.z*_axis.y, _axis.z*_axis.z);</span><br><span class="line">mat3 I = mat3(<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">mat3 Astar = mat3(<span class="number">0</span>, -_axis.z, _axis.y, _axis.z, <span class="number">0</span>, -_axis.x, -_axis.y, _axis.x, <span class="number">0</span>);</span><br><span class="line"><span class="keyword">return</span> glm::transpose(a_ta + (I - a_ta)*<span class="built_in">cos</span>(theta) + Astar * <span class="built_in">sin</span>(theta));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Transforms the camera left around the "crystal ball" interface</span></span><br><span class="line"><span class="keyword">void</span> Transform::left(<span class="keyword">float</span> degrees, vec3&amp; eye, vec3&amp; up) &#123;</span><br><span class="line">  <span class="comment">// YOUR CODE FOR HW1 HERE</span></span><br><span class="line"></span><br><span class="line">mat3 r = rotate(-degrees,up);</span><br><span class="line"></span><br><span class="line">eye = r * eye;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; eye.x &lt;&lt; eye.y &lt;&lt; eye.z &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Transforms the camera up around the "crystal ball" interface</span></span><br><span class="line"><span class="keyword">void</span> Transform::up(<span class="keyword">float</span> degrees, vec3&amp; eye, vec3&amp; up) &#123;</span><br><span class="line">  <span class="comment">// YOUR CODE FOR HW1 HERE </span></span><br><span class="line">mat3 r = rotate(-degrees, -glm::cross(eye, up));</span><br><span class="line">up = glm::transpose(glm::inverse(r))*up;<span class="comment">//up vector is not easy to compute</span></span><br><span class="line">eye = r * eye;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Your implementation of the glm::lookAt matrix</span></span><br><span class="line">mat4 Transform::lookAt(vec3 eye, vec3 up) &#123;</span><br><span class="line">  <span class="comment">// YOUR CODE FOR HW1 HERE</span></span><br><span class="line">vec3 w = glm::normalize(eye);</span><br><span class="line">vec3 u = glm::normalize(glm::cross(up, eye)) ;</span><br><span class="line">vec3 v = glm::normalize(glm::cross(w, u));</span><br><span class="line">mat3 r = mat3(u,v,w );</span><br><span class="line">vec3 t = vec3(-glm::dot(u,eye), -glm::dot(v,eye), -glm::dot(w,eye));<span class="comment">//-r * eye;</span></span><br><span class="line">mat4 result = mat4(r[<span class="number">0</span>][<span class="number">0</span>], r[<span class="number">0</span>][<span class="number">1</span>], r[<span class="number">0</span>][<span class="number">2</span>], t.x, r[<span class="number">1</span>][<span class="number">0</span>], r[<span class="number">1</span>][<span class="number">1</span>], r[<span class="number">1</span>][<span class="number">2</span>], t.y, r[<span class="number">2</span>][<span class="number">0</span>], r[<span class="number">2</span>][<span class="number">1</span>], r[<span class="number">2</span>][<span class="number">2</span>], t.z, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">return</span> glm::transpose(result);</span><br><span class="line">  <span class="comment">// You will change this return call</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>æœ€åç»“æœï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/IMG_0296.GIF" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> å›¾å½¢å­¦ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> homework </tag>
            
            <tag> computer graphics </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>å›¾å½¢å­¦â€”â€”Transformation</title>
      <link href="/2018/10/25/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E2%80%94%E2%80%94Transformation/"/>
      <url>/2018/10/25/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E2%80%94%E2%80%94Transformation/</url>
      
        <content type="html"><![CDATA[<p>ä¸Šæ¬¡åšå®¢æœ€åä¸€ä¸ªä¸»é¢˜çŸ©é˜µï¼Œåªè¯´äº†å¥çŸ©é˜µå¯ä»¥å®Œæˆå¾ˆå¤šè½¬æ¢ã€‚è€Œè¿™æ¬¡å°±ä¸»è¦æ¥è¯´æ˜å„ç§è½¬æ¢ã€‚<br><a id="more"></a></p><p>æ‰€æœ‰çš„è½¬æ¢æˆ‘ä»¬éƒ½æ˜¯é€šè¿‡çŸ©é˜µå’Œå‘é‡å®Œæˆçš„ã€‚</p><h3 id="ç¼©æ”¾ï¼ˆScaleï¼‰"><a href="#ç¼©æ”¾ï¼ˆScaleï¼‰" class="headerlink" title="ç¼©æ”¾ï¼ˆScaleï¼‰"></a>ç¼©æ”¾ï¼ˆScaleï¼‰</h3><script type="math/tex; mode=display">Scale(s_x,s_y,s_z) = \begin{pmatrix}s_x&0&0\\0&s_y&0\\0&0&s_z\end{pmatrix} \ Scale^{-1}(s_x,s_y,s_z) = \begin{pmatrix}s_x^{-1}&0&0\\0&s_y^{-1}&0\\0&0&s_z^{-1}\end{pmatrix}</script><p>ä¸Šè¿°ä¸­$s_x,s_y,s_z$åˆ†åˆ«ä¸ºå„ä¸ªåæ ‡è½´çš„ç¼©æ”¾å€æ•°ã€‚å®é™…çš„è½¬æ¢è¿‡ç¨‹å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\begin{pmatrix}s_x&0&0\\0&s_y&0\\0&0&s_z\end{pmatrix} \begin{pmatrix}x\\y\\z\end{pmatrix} = \begin{pmatrix}s_xx\\s_yy\\s_zz\end{pmatrix}</script><h3 id="é”™åˆ‡ï¼ˆShearï¼‰"><a href="#é”™åˆ‡ï¼ˆShearï¼‰" class="headerlink" title="é”™åˆ‡ï¼ˆShearï¼‰"></a>é”™åˆ‡ï¼ˆShearï¼‰</h3><p>é”™åˆ‡å¯ä»¥å°†ä¸€ä¸ªçŸ©å½¢è½¬æ¢æˆå¹³è¡Œå››è¾¹å½¢ã€‚</p><script type="math/tex; mode=display">Shear(s_x,s_y) = \begin{pmatrix}s_x&a\\0&s_y\end{pmatrix} \ Shear^{-1}(s_x,s_y) = \begin{pmatrix}s_x&-a\\0&s_y\end{pmatrix}</script><p>ä¸Šé¢å¼å­ä¸»è¦å®Œæˆçš„æ˜¯å¯¹äºy&gt;0çš„ç‚¹å‰ç§»ï¼Œy&lt;0çš„ç‚¹åç§»ï¼Œè€Œå„ç‚¹çš„yåæ ‡ä¸å˜ã€‚ä»è€Œäº§ç”Ÿäº†é”™åˆ‡çš„æ„Ÿè§‰ã€‚å¾—åˆ°çš„ç»“æœï¼š$yâ€™=y,xâ€™=x+ay$.</p><h3 id="æ—‹è½¬ï¼ˆRotationï¼‰"><a href="#æ—‹è½¬ï¼ˆRotationï¼‰" class="headerlink" title="æ—‹è½¬ï¼ˆRotationï¼‰"></a>æ—‹è½¬ï¼ˆRotationï¼‰</h3><p>2ç»´ç©ºé—´çš„æ—‹è½¬å˜æ¢æ¬¡åºæ˜¯æ²¡æœ‰å½±å“çš„ï¼Œä½†æ˜¯å¯¹äºä¸‰ç»´ç©ºé—´å´å¯èƒ½å¾—åˆ°ä¸ä¸€æ ·çš„ç»“æœã€‚</p><h4 id="2D-rotation"><a href="#2D-rotation" class="headerlink" title="2D rotation"></a>2D rotation</h4><p>è¿™é‡Œè¯´çš„æ—‹è½¬æ˜¯ç»•ç€åŸç‚¹æ—‹è½¬ã€‚</p><script type="math/tex; mode=display">\begin{bmatrix}x'\\y'\end{bmatrix} = \begin{bmatrix}\cos \theta & - \sin \theta\\\sin \theta &\cos \theta\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}</script><p>æœ‰è¶£çš„æ˜¯ï¼š$R^T R = I$</p><h4 id="3D-rotation"><a href="#3D-rotation" class="headerlink" title="3D rotation"></a>3D rotation</h4><p>3Dçš„rotationç›¸æ¯”äº2Dçš„è¦å¤æ‚å¾ˆå¤šã€‚ä½†æ˜¯å®é™…ä¸Šæˆ‘ä»¬å¯ä»¥è¿™æ ·å»ç†è§£ï¼šäºŒç»´çš„æ—‹è½¬ï¼Œç›¸å½“äºç»•ç€Zè½´æ—‹è½¬ï¼Œå› ä¸ºzåæ ‡ä¸ä¼šå˜ã€‚</p><p>å› æ­¤å¯ä»¥å¾—åˆ°ç»•å„ä¸ªåæ ‡è½´æ—‹è½¬çš„çŸ©é˜µï¼š</p><script type="math/tex; mode=display">R_z = \begin{pmatrix}\cos \theta & - \sin \theta & 0\\\sin \theta &\cos \theta & 0\\0&0&1\end{pmatrix}</script><p>åŒæ ·çš„é“ç†ä¹Ÿå°±å¯ä»¥å¾—åˆ°ç»•xè½´ä¸ç»•yè½´çš„æ—‹è½¬ï¼š</p><script type="math/tex; mode=display">R_x = \begin{pmatrix}1&0&0\\0&\cos \theta & - \sin \theta \\0&\sin \theta &\cos \theta \\\end{pmatrix} \ R_y = \begin{pmatrix}\cos \theta & 0&- \sin \theta \\0&1&0\\\sin \theta &0 &\cos \theta\end{pmatrix}</script><p>åŒæ ·çš„:$R^TR = I$.</p><p>å¦‚æœæˆ‘ä»¬ä»”ç»†è§‚å¯Ÿå¯ä»¥å‘ç°ï¼Œ3Dä¸­Rçš„å„ä¸ªè¡Œï¼ˆåˆ—ï¼‰å‘é‡æ»¡è¶³ä¸€ä¸ªä¸‰ç»´åæ ‡ç³»çš„è¦æ±‚ï¼šå•ä½å‘é‡ä¸”æ­£äº¤ã€‚å› æ­¤å¦‚æœæˆ‘ä»¬å°†æ—‹è½¬çŸ©é˜µå†™æˆä¸‹é¢çš„å½¢å¼ï¼š</p><script type="math/tex; mode=display">R = \begin{pmatrix}x_u & y_u & z_u\\x_v & y_v & z_v\\x_w & y_w & z_w\end{pmatrix}</script><p>åˆ™</p><script type="math/tex; mode=display">R_p = \begin{pmatrix}x_u & y_u & z_u\\x_v & y_v & z_v\\x_w & y_w & z_w\end{pmatrix}\begin{pmatrix}x_p\\y_p\\z_p\end{pmatrix} = \begin{pmatrix}\mathbf{u} \cdot \mathbf{p} \\\mathbf{v} \cdot \mathbf{p} \\\mathbf{z} \cdot \mathbf{p}\end{pmatrix}</script><p>ä¸Šå¼æœ€åä¸€é¡¹æ­£æ˜¯$\mathbf{p}$åœ¨uvwåæ ‡è½´ä¸‹çš„åæ ‡å€¼ã€‚å› æ­¤æ—‹è½¬çš„æœ¬è´¨å®é™…ä¸Šæ˜¯å°†å®ƒæ˜ å°„åˆ°å¦ä¸€ä¸ªåæ ‡ç³»å½“ä¸­äº†ã€‚</p><p>å¯¹äºæ—‹è½¬çŸ©é˜µå¦‚ä½•æ±‚é€†ï¼Ÿå› ä¸ºæ—‹è½¬çŸ©é˜µæ˜¯æ­£äº¤çŸ©é˜µï¼Œæ‰€æœ‰$R^{-1} = R^T$.</p><p>3Dæ—‹è½¬æ˜¯ä¸å¯äº¤æ¢çš„ã€‚</p><p>ä½†æ˜¯ä¸Šé¢çš„æ—‹è½¬çŸ©é˜µç›¸å¯¹æ¥è¯´æ¯”è¾ƒç®€å•ï¼Œå› ä¸ºæ˜¯ç»•ç€åæ ‡è½´æ—‹è½¬çš„ã€‚é‚£ä¹ˆæœ‰æ²¡æœ‰åŠæ³•ç»•ç€ä»»æ„ä¸€ä¸ªå‘é‡æ—‹è½¬$\theta$çš„å…¬å¼ï¼Ÿ</p><h5 id="ç½—å¾·é‡Œæ ¼æ—‹è½¬-Rodrigues-å…¬å¼"><a href="#ç½—å¾·é‡Œæ ¼æ—‹è½¬-Rodrigues-å…¬å¼" class="headerlink" title="ç½—å¾·é‡Œæ ¼æ—‹è½¬(Rodrigues)å…¬å¼"></a>ç½—å¾·é‡Œæ ¼æ—‹è½¬(Rodrigues)å…¬å¼</h5><p>æ¥ä¸‹æ¥è¿™ä¸ªå…¬å¼å°±æ˜¯ç”¨æ¥è§£å†³ä¸Šè¿°é—®é¢˜çš„ã€‚å‡è®¾å‘é‡ï¼ˆç‚¹ï¼‰$b$ç»•ç€å•ä½å‘é‡$\mathbf{a}$æ—‹è½¬$\theta$.</p><p>$\mathbf{b}=\mathbf{b}_{âˆ¥}+\mathbf{b}_{âŠ¥}$ï¼Œåˆ†å¸ƒè¡¨ç¤ºå¹³è¡Œä¸å‚ç›´äº$\mathbf{a}$çš„åˆ†é‡ã€‚å¾ˆå®¹æ˜“çŸ¥é“ï¼Œ$\mathbf{b}_{âˆ¥}$åœ¨æ—‹è½¬è¿‡ç¨‹ä¸­æ˜¯ä¸å˜çš„ã€‚</p><p>$\mathbf{b}_{âˆ¥} = \mathbf{a} \cdot \mathbf{b} \mathbf{a}$</p><p>$\mathbf{b}_{âŠ¥} = 1 -\mathbf{b}_{âˆ¥}$</p><p>ç„¶åæˆ‘ä»¬è¦è€ƒè™‘åˆ°$\mathbf{b}_{âŠ¥}$æ˜¯å¦‚ä½•æ—‹è½¬çš„ã€‚å®é™…ä¸Šå®ƒæ—‹è½¬çš„å¹³é¢æ˜¯å‚ç›´äº$\mathbf{a}$ä¸$\mathbf{b}$çš„ï¼Œå®¹æ˜“æƒ³åˆ°åˆ©ç”¨å‰ä¹˜æ¥æ„é€ ï¼š</p><p>$\mathbf{c} = \mathbf{a} \times \mathbf{b}$ï¼Œè€Œä¸”ç”±äºå‰ä¹˜çš„æ€§è´¨ï¼Œ$\mathbf{c}$çš„é•¿åº¦æ°å¥½ç­‰äº$\mathbf{b}_{âŠ¥}$.</p><p>ç°åœ¨æ—‹è½¬è§’åº¦æ˜¯$\theta$ï¼Œåˆ™æ—‹è½¬åçš„å‘é‡åœ¨$\mathbf{b}_{âŠ¥}$ä¸$\mathbf{c}$æ–¹å‘ä¸Šçš„æŠ•å½±åˆ†åˆ«æ˜¯$\mathbf{b}_{âŠ¥} \cos \theta$ä¸$\mathbf{c} \sin \theta$.</p><p>æ—‹è½¬åçš„å‘é‡è¾¹ä¸Šä¸Šè¿°å‘é‡ä¹‹å’Œï¼Œå¹¶ä¸”å¸Œæœ›å†™æˆæ—‹è½¬çŸ©é˜µçš„å½¢å¼ï¼ˆçŸ©é˜µä¹˜ä»¥å‘é‡ï¼‰ï¼š</p><script type="math/tex; mode=display">\begin{align}b' &= \mathbf{b}_{âŠ¥} \cos \theta +\mathbf{c} \sin \theta + \mathbf{b}_{âˆ¥}\\&= (\mathbf{b} -\mathbf{a} \cdot \mathbf{b} \mathbf{a} )\cos \theta +\mathbf{a} \times \mathbf{b} \sin \theta +\mathbf{a} \cdot \mathbf{b} \mathbf{a}\\&= \mathbf{a}\mathbf{a}^T \mathbf{b} + (I -\mathbf{a}\mathbf{a}^T)\mathbf{b} \cos \theta + A^* \sin \theta \mathbf{b} \end{align}</script><p>æ‰€ä»¥$R(\mathbf{a},\theta) =\mathbf{a}\mathbf{a}^T+ (I -\mathbf{a}\mathbf{a}^T)\cos \theta + A^* \sin \theta$.</p><p>ä¸Šå¼æ˜¯è®¡ç®—æœºå›¾å½¢å­¦ä¸­å¾ˆé‡è¦çš„ä¸€ä¸ªåŸºç¡€å…¬å¼ã€‚</p><h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h3><p>å¦‚æœè¦å›åˆ°åŸæ¥çš„æ ·å­ï¼Œå¯ä»¥å¯¹æ‰€æœ‰çš„è½¬æ¢æ±‚é€†ã€‚ä½†æ˜¯è¦æ³¨æ„çš„æ˜¯ï¼š$M = M_1M_2M_3,M^{-1} = M_3^{-1}M_2^{-1}M_1^{-1}$.</p><h3 id="é½æ¬¡åæ ‡è½¬æ¢ï¼ˆHomogeneous-Coordinatesï¼‰"><a href="#é½æ¬¡åæ ‡è½¬æ¢ï¼ˆHomogeneous-Coordinatesï¼‰" class="headerlink" title="é½æ¬¡åæ ‡è½¬æ¢ï¼ˆHomogeneous Coordinatesï¼‰"></a>é½æ¬¡åæ ‡è½¬æ¢ï¼ˆHomogeneous Coordinatesï¼‰</h3><p>ä¸çŸ¥é“æœ‰æ²¡æœ‰äººæœ‰è¿™ä¹ˆä¸€ä¸ªç–‘é—®ï¼šä¸ºä»€ä¹ˆæ²¡æœ‰å¹³ç§»ï¼Ÿï¼Ÿå¹³ç§»åº”è¯¥æ˜¯æœ€ç®€å•çš„è½¬æ¢ï¼Œä½†æ˜¯å®é™…èµ·æ¥çš„å®ç°å´æ²¡æœ‰é‚£ä¹ˆå®¹æ˜“ã€‚é¦–å…ˆåšä¸ªå°è¯•ï¼š</p><script type="math/tex; mode=display">\begin{pmatrix}x'\\y'\\z'\end{pmatrix} = \begin{pmatrix}?&?&?\\?&?&?\\?&?&?\end{pmatrix}\begin{pmatrix}x\\y\\z\end{pmatrix}=\begin{pmatrix}x+5\\y\\z\end{pmatrix}</script><p>ä¸­é—´çš„çŸ©é˜µæ€ä¹ˆåšï¼Ÿæœ‰äººä¼šè¯´å°†ç¬¬ä¸€è¡Œç¬¬ä¸€åˆ—å†™ä¸º$\frac 5 z$å³å¯ï¼Œä½†æ˜¯è¿™å°±å¤±å»äº†è½¬æ¢çŸ©é˜µçš„æ„ä¹‰ã€‚è½¬æ¢çŸ©é˜µä¸­ä¸åº”è¯¥åŒ…å«æˆ‘ä»¬è¦è½¬æ¢çš„åæ ‡çš„ä¿¡æ¯ã€‚</p><p>è®¡ç®—æœºå›¾å½¢å­¦ä¸­ï¼Œè§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•å°±è¯´ç”¨é½æ¬¡åæ ‡ï¼Œå…ˆçœ‹ä¸€ä¸‹ä¸‹é¢çš„è½¬æ¢ï¼š</p><script type="math/tex; mode=display">\begin{pmatrix}x'\\y'\\z'\\w'\end{pmatrix} = \begin{pmatrix}1&0&0&5\\0&1&0&0\\0&0&1&0\\0&0&0&1\end{pmatrix}\begin{pmatrix}x\\y\\z\\1\end{pmatrix}=\begin{pmatrix}x+5\\y\\z\\1\end{pmatrix}</script><p>é€šè¿‡åŠ å…¥ä¸€ä¸ªå…¶æ¬¡åæ ‡wè€Œå®ç°äº†å¹³ç§»ã€‚è€ŒåŠ å…¥è¿™ä¸ªé‡ä¸ä¼šå¯¹ä¸Šé¢æåˆ°çš„æ—‹è½¬ç­‰æ“ä½œäº§ç”Ÿå½±å“ï¼Œè€Œä¸”ä»–æœ‰å¾ˆå¤šçš„å¥½å¤„ï¼Œå› æ­¤è¢«æ™®éç”¨äºå›¾å½¢ç›¸å…³çš„è½¯ä»¶ä¸ç¡¬ä»¶ä¸­ã€‚</p><h4 id="å¹³ç§»-Translate"><a href="#å¹³ç§»-Translate" class="headerlink" title="å¹³ç§»(Translate)"></a>å¹³ç§»(Translate)</h4><script type="math/tex; mode=display">\begin{pmatrix}x'\\y'\\z'\\w'\end{pmatrix} = \begin{pmatrix}1&0&0&T_x\\0&1&0&T_y\\0&0&1&T_z\\0&0&0&1\end{pmatrix}\begin{pmatrix}\mathbf{p}_x\\\mathbf{p}_y\\\mathbf{p}_z\\1\end{pmatrix}=\begin{pmatrix}\mathbf{p}_x+T_x\\\mathbf{p}_y+T_y\\\mathbf{p}_z+T_z\\1\end{pmatrix} = \mathbf{p}+T</script><p>ä¸»è¦æ—‹è½¬å¹³ç§»çŸ©é˜µå’Œå¹³ç§»æ—‹è½¬çŸ©é˜µæ˜¯ä¸ä¸€æ ·çš„ï¼Œå› ä¸ºå¹³ç§»çŸ©é˜µå®é™…ä¸Šéœ€è¦çš„æ˜¯æœ€å³ä¾§çš„ä¸€ä¸ªå‘é‡ï¼š</p><p>æ—‹è½¬å¹³ç§»çŸ©é˜µï¼š</p><script type="math/tex; mode=display">\begin{bmatrix}R_{3 \times 3}&T_{3 \times 1}\\0_{1 \times 3}&1\end{bmatrix}</script><p>å¹³ç§»æ—‹è½¬çŸ©é˜µ:</p><script type="math/tex; mode=display">\begin{bmatrix}R_{3 \times 3}&R_{3 \times 3}T_{3 \times 1}\\0_{1\times 3}&1\end{bmatrix}</script><p>ä»”ç»†æ¨å¯¼å°±å¯ä»¥å¾—åˆ°ä¸Šé¢çš„ç»“æœã€‚</p><h3 id="æ³•å‘è½¬æ¢ï¼ˆNormal-Transformationï¼‰"><a href="#æ³•å‘è½¬æ¢ï¼ˆNormal-Transformationï¼‰" class="headerlink" title="æ³•å‘è½¬æ¢ï¼ˆNormal Transformationï¼‰"></a>æ³•å‘è½¬æ¢ï¼ˆNormal Transformationï¼‰</h3><p>æ³•å‘çš„è½¬æ¢å¹¶ä¸ä¼šæŒ‰ç…§å¹³é¢å„ä¸ªç‚¹çš„è½¬æ¢è¿›è¡Œã€‚</p><p>å‡å¦‚åŸæ¥çš„åˆ‡çº¿å‘é‡ä¸º$\mathbf{t}$,åŸæ¥çš„æ³•å‘å‘é‡ä¸º$\mathbf{n}$ï¼Œè½¬æ¢çŸ©é˜µä¸º$M$ï¼Œåˆ™åˆ‡çº¿å‘é‡æ˜¯ä¼šä¾ç…§åŸæ¥çš„è½¬æ¢è€Œæ”¹å˜çš„ï¼š$\mathbf{t}â€™ = M\mathbf{t}$.</p><p>å‡è®¾è½¬æ¢å®Œæˆåçš„æ³•å‘å‘é‡ä¸º$\mathbf{n}â€™ = Q\mathbf{n}$.</p><p>å½“ç„¶ä¸ç®¡ä»€ä¹ˆæ—¶å€™ï¼Œæ³•å‘ä¸åˆ‡å‘éƒ½åº”è¯¥æ˜¯å‚ç›´çš„ï¼š$\mathbf{n}â€™^T\mathbf{tâ€™} = 0$</p><p>å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">\mathbf{n}^TQ^TM \mathbf{t} = 0.</script><p>å› æ­¤æˆ‘ä»¬å¸Œæœ›$Q^TM = I$,æ‰€ä»¥å¾—åˆ°ï¼š$Q = (M^{-1})^T$.</p><p>å½“ç„¶ä¸Šé¢çš„è§£å¹¶ä¸æ˜¯å”¯ä¸€è§£ï¼Œä½†æ˜¯è®¡ç®—æœºå›¾å½¢å­¦æ˜¯å·¥ç¨‹å­¦ç§‘ï¼Œæˆ‘ä»¬å¸Œæœ›çš„æ˜¯èƒ½å¤Ÿè§£å†³è¿™ä¸ªé—®é¢˜å³å¯ã€‚</p><p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼š$M$ä¸º3Ã—3çŸ©é˜µã€‚å› ä¸ºæ³•å‘å’Œåˆ‡å‘æ˜¯å‘é‡ï¼Œæ‰€ä»¥å¹³ç§»ä¸ä¼šå½±å“æ³•å‘å’Œåˆ‡å‘ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> å›¾å½¢å­¦ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> computer graphics </tag>
            
            <tag> transformation </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>å›¾å½¢å­¦â€”â€”Basic Math</title>
      <link href="/2018/10/24/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E2%80%94%E2%80%94Basic-Math/"/>
      <url>/2018/10/24/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E2%80%94%E2%80%94Basic-Math/</url>
      
        <content type="html"><![CDATA[<p>åŠ å…¥äº†æ™ºèƒ½æˆåƒå®éªŒå®¤ï¼Œä½†æ˜¯æˆ‘å¯¹è®¡ç®—æœºå›¾å½¢å­¦äº†è§£è¿˜å¤ªæµ…ï¼Œå› æ­¤éœ€è¦å­¦ä¹ ä¸€äº›å›¾å½¢å­¦çš„åŸºç¡€çŸ¥è¯†ã€‚æœ¬ç¯‡åšå®¢å…ˆä»‹ç»ä¸€äº›å¾ˆç®€å•çš„æ•°å­¦åŸºç¡€ã€‚<br><a id="more"></a></p><h3 id="å³æ‰‹åæ ‡ç³»"><a href="#å³æ‰‹åæ ‡ç³»" class="headerlink" title="å³æ‰‹åæ ‡ç³»"></a>å³æ‰‹åæ ‡ç³»</h3><p>é¦–å…ˆæ˜¯XYZåæ ‡è½´ï¼Œä¸€èˆ¬æˆ‘ä»¬ä½¿ç”¨çš„åæ ‡è½´æ˜¯ç¬¦åˆå³æ‰‹å®šåˆ™çš„ã€‚è¿™ä¹Ÿæ˜¯å¤§å¤šæ•°æ•°å­¦æ•™æä½¿ç”¨çš„åæ ‡è½´çš„å®šä¹‰ã€‚å³æ‰‹å®šåˆ™å³ç”¨å³æ‰‹ä»Xè½´æ–¹å‘æ¡å‘Yè½´æ–¹å‘ï¼Œè¿™æ—¶å€™ä¼¸å‡ºå¤§æ‹‡æŒ‡ï¼Œå¤§æ‹‡æŒ‡çš„æœå‘å°±æ˜¯Zè½´çš„æ–¹å‘ã€‚å®é™…ä¸Šè¿™æ˜¯ä¸€ä¸ªå¾ˆåŸºç¡€çš„çŸ¥è¯†ï¼Œä½†æ˜¯æˆ‘è¿‡å»æ²¡å¤šä¹…æ‰å‘ç°è‡ªå·±ä¸€ç›´ä¸çŸ¥é“è¿™ä¸ªä¸œè¥¿ã€‚ç›´åˆ°è®¡ç®—ç¬¬äºŒå‹æ›²é¢ç§¯åˆ†æ—¶å€™å‘ç°ç»“æœæ€»æ˜¯ç›¸åæ‰å‘ç°ã€‚</p><h3 id="å‘é‡ç‚¹ä¹˜å’Œå‰ä¹˜"><a href="#å‘é‡ç‚¹ä¹˜å’Œå‰ä¹˜" class="headerlink" title="å‘é‡ç‚¹ä¹˜å’Œå‰ä¹˜"></a>å‘é‡ç‚¹ä¹˜å’Œå‰ä¹˜</h3><h4 id="ç‚¹ä¹˜ï¼ˆDot-Productï¼‰"><a href="#ç‚¹ä¹˜ï¼ˆDot-Productï¼‰" class="headerlink" title="ç‚¹ä¹˜ï¼ˆDot Productï¼‰"></a>ç‚¹ä¹˜ï¼ˆDot Productï¼‰</h4><p>$\mathbf{a} \cdot \mathbf{b} = \vert a \vert \vert b\vert \cos {\theta}$</p><p>$\theta = \cos ^{-1} \frac {\mathbf{a} \cdot \mathbf{b}}{\vert a \vert \vert b\vert}$</p><p>å®é™…ä¸Šåœ¨ç¬›å¡å°”åæ ‡ç³»ä¸­æ±‚è§£ä¸¤ä¸ªå‘é‡çš„ç‚¹ä¹˜æ˜¯éå¸¸å®¹æ˜“çš„ï¼š</p><p>$\mathbf{a} = x_a \mathbf{x} + y_a \mathbf{y},\mathbf{b} = x_b \mathbf{x} + y_b \mathbf{y}$</p><p>åˆ™ï¼š$\mathbf{a} \cdot \mathbf{b} = x_a x_b + y_a y_b$</p><p>å› æ­¤ç‚¹ä¹˜å¯ä»¥ç”¨æ¥æ±‚ä¸¤ä¸ªå‘é‡çš„å¤¹è§’ã€‚</p><p>å¦ä¸€ä¸ªç‚¹ä¹˜çš„åº”ç”¨æ˜¯æ±‚æŸä¸ªå‘é‡åˆ°å¦ä¸€ä¸ªå‘é‡ä¸Šçš„æŠ•å½±ï¼Œå¦‚$\mathbf{a}$åœ¨$\mathbf{b}$ä¸Šçš„æŠ•å½±é•¿åº¦å®é™…ä¸Šæ˜¯$\vert \mathbf{a} \vert \cos \theta$ï¼Œè€Œ$\vert \mathbf{a} \vert \cos \theta = \frac {\vert \mathbf{b}\vert\vert \mathbf{a} \vert \cos \theta}{\mathbf{b}} = \frac {\mathbf{a} \cdot \mathbf{b}}{\vert \mathbf{b}\vert}$.</p><p>åŒæ ·çš„æƒ³è¦æ±‚æŠ•å½±ä¹‹åçš„å‘é‡ä¹Ÿå¾ˆç®€å•ï¼Œåªè¦ç”¨é•¿åº¦ä¹˜ä¸Š$\mathbf{b}$æ–¹å‘çš„å•ä½å‘é‡å³å¯ï¼š<br>$\mathbf{p} =  \frac {\mathbf{a} \cdot \mathbf{b} \mathbf{b}}{\vert \mathbf{b}\vert ^2}$.</p><h4 id="å‰ä¹˜ï¼ˆCross-Productï¼‰"><a href="#å‰ä¹˜ï¼ˆCross-Productï¼‰" class="headerlink" title="å‰ä¹˜ï¼ˆCross Productï¼‰"></a>å‰ä¹˜ï¼ˆCross Productï¼‰</h4><p>$\vert \mathbf{a} \times \mathbf{b} \vert= \vert \mathbf{a} \vert \vert \mathbf{b}\vert \sin \theta$</p><p>ä¸Šé¢ä¸ºå‰ä¹˜çš„é•¿åº¦ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œå‰ä¹˜å¾—åˆ°çš„æ˜¯ä¸€ä¸ªå‘é‡ï¼Œå®ƒçš„æ–¹å‘ç¬¦åˆå³æ‰‹å®šåˆ™ï¼Œä¹Ÿä¸åŸæ¥ä¸¤ä¸ªå‘é‡å‚ç›´ã€‚</p><p>ä»å³æ‰‹å®šåˆ™å¯ä»¥å¾ˆå®¹æ˜“æ¨å¯¼å‡ºï¼š$\mathbf{a} \times \mathbf{b} = - \mathbf{b} \times \mathbf{a}$.</p><p>åŒæ—¶ä¹Ÿæœ‰ä»¥ä¸‹çš„ä¸€äº›ç­‰å¼ï¼š</p><script type="math/tex; mode=display">\mathbf{x} \times \mathbf{y} = \mathbf{z},\mathbf{y} \times \mathbf{x} = -\mathbf{z}</script><script type="math/tex; mode=display">\mathbf{a} \times \mathbf{a} = \mathbf{0}</script><script type="math/tex; mode=display">\mathbf{a} \times (\mathbf{b} + \mathbf{c}) = \mathbf{a} \times \mathbf{b}+ \mathbf{a} \times \mathbf{c}</script><script type="math/tex; mode=display">\mathbf{a} \times (k\mathbf{b}) = k(\mathbf{a} \times \mathbf{b})</script><p>åœ¨ç¬›å¡å°”åæ ‡ç³»ä¸‹ï¼Œå‰ä¹˜çš„è®¡ç®—ä¹Ÿä¸ç®—å›°éš¾ï¼š</p><script type="math/tex; mode=display">\mathbf{a}\times \mathbf{b} = \begin{vmatrix} \mathbf{x}&\mathbf{y}&\mathbf{z}\\x_a&y_a&z_a\\x_b&y_b&z_b\end{vmatrix} = \begin{pmatrix} y_az_b - y_bz_a,\\z_ax_b - x_az_b,\\x_ay_b - y_ax_b \end{pmatrix}</script><p>è€Œä¸”ä¹Ÿå¯ä»¥å†™æˆä¸‹é¢çš„å½¢å¼ï¼š</p><script type="math/tex; mode=display">\mathbf{a}\times \mathbf{b} = A^* \mathbf{b}\begin{pmatrix}0&-z_a&y_a\\z_a&0&-x_a\\-y_a&x_a&0\end{pmatrix} \begin{pmatrix}x_b\\y_b\\z_b\\\end{pmatrix}</script><p>ä¸Šå¼ä¸­$A^*$è¢«æˆä¸ºå‘é‡$\mathbf{a}$çš„å¯¹å¶çŸ©é˜µã€‚</p><h3 id="åæ ‡ç³»-Coordinate-Frames"><a href="#åæ ‡ç³»-Coordinate-Frames" class="headerlink" title="åæ ‡ç³»(Coordinate Frames)"></a>åæ ‡ç³»(Coordinate Frames)</h3><p>åæ ‡ç³»å¹¶ä¸æ˜¯åªæœ‰XYZï¼ŒXYZåªæ˜¯æ ‡è¯†è€Œå·²ã€‚ä»»ä½•æ»¡è¶³ä¸‹é¢æ¡ä»¶çš„ä¸‰ä¸ªå‘é‡ï¼Œéƒ½å¯ä»¥ä½œä¸ºåæ ‡ç³»ï¼š</p><ul><li>$\vert \mathbf{u} \vert = \vert \mathbf{v} \vert = \vert \mathbf{w} \vert = 1$</li><li>$\mathbf{u} \cdot \mathbf{v} = \mathbf{v} \cdot \mathbf{w} = \mathbf{u} \cdot \mathbf{w} = 0$</li><li>$\mathbf{w} = \mathbf{u} \times \mathbf{v}$</li></ul><p>ä»»ä½•ä¸€ä¸ªå‘é‡å¯ä»¥å†™æˆå„ä¸ªåæ ‡ç³»çš„æŠ•å½±ä¹‹å’Œï¼š</p><script type="math/tex; mode=display">\mathbf{p}  = (\mathbf{p} \cdot \mathbf{u} ) \mathbf{u} + (\mathbf{p} \cdot \mathbf{v}) \mathbf{v} + (\mathbf{p} \cdot \mathbf{w})\mathbf{w}</script><p>å¦‚ä½•ä½¿ç”¨ä¸¤ä¸ªéé›¶éåŒæ–¹å‘çš„å‘é‡åˆ›é€ ä¸€ä¸ªåæ ‡ç³»ï¼ˆåœ¨ä¸‰ç»´é‡å»ºä¸­å¯èƒ½ä¼šç»å¸¸ç¢°åˆ°ï¼‰ï¼Ÿ</p><script type="math/tex; mode=display"> \mathbf{u} = \frac {\mathbf{a}}{ \vert \mathbf{a}\vert},\mathbf{w} = \frac {\mathbf{a} \times \mathbf{b}}{\vert \mathbf{a} \times \mathbf{b} \vert}, \mathbf{v}= \mathbf{w} \times \mathbf{v}.</script><p>ä»ä¸Šé¢æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å‰ä¹˜å¯¹äºåˆ›å»ºä¸€ä¸ªåæ ‡ç³»çš„ä½œç”¨ã€‚</p><h3 id="çŸ©é˜µï¼ˆMatrixï¼‰"><a href="#çŸ©é˜µï¼ˆMatrixï¼‰" class="headerlink" title="çŸ©é˜µï¼ˆMatrixï¼‰"></a>çŸ©é˜µï¼ˆMatrixï¼‰</h3><p>$(AB)^{-1} = B^{-1}A^{-1}$,å› ä¸º$ AB B^{-1} A^{-1} = I$.</p><p>é€šè¿‡çŸ©é˜µå¯ä»¥å®ç°ç©ºé—´ç‚¹çš„å„ç§è½¬æ¢ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> å›¾å½¢å­¦ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mathematics </tag>
            
            <tag> computer graphics </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Learning From Dataâ€”â€”Generalized Linear Model</title>
      <link href="/2018/10/22/Learning-From-Data%E2%80%94%E2%80%94Generalized-Linear-Model/"/>
      <url>/2018/10/22/Learning-From-Data%E2%80%94%E2%80%94Generalized-Linear-Model/</url>
      
        <content type="html"><![CDATA[<p>è¿™æ¬¡æ•°æ®å­¦ä¹ è¯¾ä¸Šï¼Œè®²çš„æ˜¯Generalized Linear Modelã€‚æˆ‘å¿ƒé‡Œæƒ³ç€æ˜¯è¦æ¦‚å†µçº¿æ€§æ¨¡å‹ï¼Œæˆ‘åº”è¯¥éƒ½æ¸…æ¥šå§ã€‚ä¸Šè¯¾äº†ä¹‹åæ‰å‘ç°ï¼Œè¿™å®é™…ä¸Šæ˜¯å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼Œæœ‰å¾ˆå¤šæ–°ä¸œè¥¿ã€‚ç„¶è€Œæˆ‘è¿˜æ˜¯ç¡ç€äº†ã€‚<br><a id="more"></a></p><p>é¦–å…ˆå¼•å…¥ä¸€ä¸ªæ¦‚å¿µï¼Œå«åš<strong>æŒ‡æ•°æ—åˆ†å¸ƒ</strong>ã€‚</p><h3 id="Exponential-Family"><a href="#Exponential-Family" class="headerlink" title="Exponential Family"></a>Exponential Family</h3><p>å¦‚æœä¸€ä¸ªåˆ†å¸ƒå¯ä»¥è¢«å†™æˆä¸‹é¢çš„å½¢å¼ï¼š</p><script type="math/tex; mode=display">p(y;\eta) = b(y)e^{\eta ^T T(y) - a(\eta)}</script><p>é‚£ä¹ˆè¿™ä¸ªåˆ†å¸ƒå±äºExponential Familyã€‚å…¶ä¸­ï¼š</p><p>$\eta$: natural/canonical parameter(è‡ªç„¶å‚æ•°) </p><p>$T(y)$: suï¬ƒcient statistic of the distribution(å……åˆ†ç»Ÿè®¡é‡) </p><p>$a(Î·)$: log partition function(å¯¹æ•°åˆ’åˆ†å‡½æ•°)</p><p>å…¶ä¸­$a(\eta)$æ˜¯ä¸€ä¸ªå½’ä¸€åŒ–å¸¸æ•°çš„å¯¹æ•°ã€‚ä¹Ÿå°±æ˜¯ï¼š</p><p>$p(y;\eta) = b(y)e^{\eta ^T T(y) - a(\eta)} = \frac {b(y)e^{\eta^T T(y)} } {e^{a(\eta)} }$</p><p>$\sum_{y} p(y;\eta) = 1(or \int _y p(y;\eta) dy = 1) $</p><p>æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š<br>$a(\eta) = \log {\left(\sum _y b(y)e^{\eta ^T T(y)} \right)}$</p><p>æŒ‡æ•°åˆ†å¸ƒæ—æœ‰å¾ˆå¤šæˆå‘˜ï¼Œå®é™…ä¸Šæˆ‘ä»¬ç†Ÿæ‚‰çš„å¾ˆå¤šåˆ†å¸ƒéƒ½æ˜¯æŒ‡æ•°åˆ†å¸ƒæ—çš„ã€‚ä¸‹é¢ä¸¾å‡ ä¸ªä¾‹å­ï¼š</p><h4 id="Bernoulli-Distribution"><a href="#Bernoulli-Distribution" class="headerlink" title="Bernoulli Distribution"></a>Bernoulli Distribution</h4><p>ä¼¯åŠªåˆ©åˆ†å¸ƒåº”è¯¥æ˜¯æœ€ç®€å•çš„åˆ†å¸ƒä¹‹ä¸€äº†ã€‚$y \in {1,0}$ï¼Œè€Œä¸”$p(y=1) = Ï†,p(y=0) = 1 - Ï†$ï¼Œå› æ­¤å®ƒçš„åˆ†å¸ƒå¯ä»¥å†™æˆä¸‹é¢çš„æ ·å­ï¼š</p><p>$p(y;Ï†) = Ï†^y(1-Ï†)^{1-y}$</p><p>å¦‚ä½•å°†å®ƒè½¬åŒ–ä¸ºæŒ‡æ•°æ—çš„å½¢å¼ï¼Ÿ</p><ul><li><p>$\eta = \log {\frac {\phi } {1-\phi} }$</p></li><li><p>$T(y) = y$</p></li><li><p>$a(\eta) = \log {(1 + e^{\eta})}$ </p></li><li><p>$b(y) = 1$</p></li></ul><h4 id="Gaussian-Distribution-unit-variance"><a href="#Gaussian-Distribution-unit-variance" class="headerlink" title="Gaussian Distribution(unit variance)"></a>Gaussian Distribution(unit variance)</h4><p>é«˜æ–¯åˆ†å¸ƒä¹Ÿæ˜¯å¾ˆå¸¸è§çš„åˆ†å¸ƒï¼Œè¿™é‡Œæˆ‘ä»¬å…ˆè¯´æ˜ä¸€ä¸‹unit varianceçš„æƒ…å†µï¼Œä¹Ÿå°±æ˜¯$\sigma = 1$ã€‚å®ƒçš„æ¦‚ç‡å¯†åº¦å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">p(y;\mu) = \frac 1 {\sqrt{2 \pi} } exp\left(- \frac {(y - \mu)^2}{2} \right)</script><ul><li><p>$\eta = \mu$</p></li><li><p>$ T(y) = y$</p></li><li><p>$a(\eta) = \frac {\eta ^2} {2}$</p></li><li><p>$b(y) = \frac 1 {\sqrt{2 \pi} } e^{-\frac {y^2}{2} } $</p></li></ul><h4 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h4><p>ç°åœ¨å°†ç›®æ ‡æ”¾åˆ°æ™®é€šçš„é«˜æ–¯åˆ†å¸ƒä¸Šã€‚</p><script type="math/tex; mode=display">p(y;\mu) = \frac 1 {\sqrt{2 \pi \sigma ^ 2} } exp\left(- \frac {(y - \mu)^2}{2\sigma ^ 2} \right)</script><ul><li><script type="math/tex; mode=display">\eta = \left[\begin{matrix} \frac {\mu}{\sigma^2} \\ -\frac {1}{2\sigma^2} \end{matrix}\right]</script></li><li><script type="math/tex; mode=display">  T(y) = \left[ \begin{matrix} y \\y^2 \end{matrix}\right]</script></li><li><p>$a(\eta) = \frac {\mu^2}{2\sigma^2} + \log {\sigma}$</p></li><li><p>$b(y) = \frac 1 {\sqrt {2 \pi} }$</p></li></ul><p>è¿™é‡Œæƒ…å†µå˜å¾—å°±ç¨å¾®å¤æ‚äº†ç‚¹ã€‚</p><h4 id="Poisson-Distribution-Poisson-lambda"><a href="#Poisson-Distribution-Poisson-lambda" class="headerlink" title="Poisson Distribution:Poisson($\lambda$)"></a>Poisson Distribution:Poisson($\lambda$)</h4><p>æ³Šæ¾åˆ†å¸ƒå¹³æ—¶æˆ‘ä»¬æ¥è§¦ä¸å¦‚å‰ä¸¤é¡¹å¤šã€‚æ³Šæ¾åˆ†å¸ƒä¸€èˆ¬å¯ä»¥ç”¨åœ¨ä¼°è®¡ä¸€ä¸ªå›ºå®šçš„æ—¶é—´æ®µå†…æŸä¸ªäº‹æƒ…å‘ç”Ÿçš„æ¬¡æ•°ï¼Œå‡è®¾å„ä¸ªäº‹ä»¶ä¹‹é—´äº’ç›¸ç‹¬ç«‹ï¼Œå®ƒä»¬å‘ç”Ÿæœ‰ä¸€ä¸ªå›ºå®šçš„æ¯”ç‡$\lambda$.</p><p>æ³Šæ¾åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦å‡½æ•°å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">p(y;\lambda) = \frac {\lambda ^y e ^{- \lambda} }{y!}</script><ul><li><p>$\eta = \log {\lambda}$</p></li><li><p>$T(y) = y$</p></li><li><p>$a(\eta) = e^{\eta}$</p></li><li><p>$b(y) = \frac {1}{y!}$</p></li></ul><h3 id="Generalized-Linear-Models"><a href="#Generalized-Linear-Models" class="headerlink" title="Generalized Linear Models"></a>Generalized Linear Models</h3><p>æ‰€ä»¥ä»€ä¹ˆæ˜¯å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼ŸGLMæ˜¯ä»æ¥è‡ªäºæŒ‡æ•°æ—åˆ†å¸ƒ$y|X;\theta$ä¸€ç§æ„é€ çº¿æ€§æ¨¡å‹çš„æ–¹æ³•ã€‚</p><p>å¹¿ä¹‰çº¿æ€§æ¨¡å‹çš„è®¾è®¡åŠ¨æœºä¸ºï¼š</p><ul><li>ç›¸åº”å˜é‡yå¯ä»¥æ˜¯ä»»æ„åˆ†å¸ƒ</li><li>å…è®¸yçš„ä»»æ„å‡½æ•°ï¼ˆé“¾æ¥å‡½æ•°ï¼‰å¯ä»¥éšè¾“å…¥å€¼xçº¿æ€§å˜åŒ–</li></ul><p>å¹¿ä¹‰çº¿æ€§æ¨¡å‹å½¢å¼åŒ–å®šä¹‰æœ‰ä¸‹é¢å‡ ä¸ªå‡è®¾ï¼š</p><ol><li>$y|x;\theta$ ~ Exponential Family(\eta),å¦‚é«˜æ–¯åˆ†å¸ƒï¼Œä¼¯åŠªåˆ©åˆ†å¸ƒï¼Œæ³Šæ¾åˆ†å¸ƒç­‰</li><li>å‡è®¾ç›®æ ‡å‡½æ•°æ˜¯$h(x) = \mathbb{E}[T(y)|x]$</li><li>è‡ªç„¶å¸¸æ•°$\eta$å’Œè¾“å…¥$X$æ˜¯çº¿æ€§ç›¸å…³çš„ï¼š$\eta = \theta^TX$ or $\eta_i = \theta_i^T X (\eta = \Theta^T X)$ </li></ol><p>å°†è‡ªç„¶å‚æ•°ä¸åˆ†å¸ƒå¹³å‡å€¼è¿æ¥å¾—åˆ°ï¼š$\mathbb{E}[T(y);\eta]$.</p><p>æƒå¨å“åº”å‡½æ•°ï¼ˆCanonical response functionï¼‰gç»™å‡ºäº†åˆ†å¸ƒå¹³å‡å€¼ï¼š$g(\eta) = \mathbb{E}[T(y);\eta]$.</p><p>åˆ™ $\eta = g^{-1}(\mathbb{E}[T(y);\eta])$,è¢«ç§°ä¸ºæƒå¨é“¾æ¥å‡½æ•°ï¼ˆcanonical link functionï¼‰ã€‚</p><p>å†™æˆå¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼Œå¯ä»¥å¾—åˆ°ï¼š$\mathbb{E}(y;\eta)=\frac{d}{d\eta}a({\eta})$ï¼ˆè¯æ˜è¾ƒä¸ºå¤æ‚ï¼‰ã€‚å› æ­¤ï¼Œå¯ä»¥å¾ˆè½»æ˜“å¾—æ±‚å‡ºå‡è®¾å‡½æ•°ã€‚</p><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><h4 id="Ordinary-Least-Square"><a href="#Ordinary-Least-Square" class="headerlink" title="Ordinary Least Square"></a>Ordinary Least Square</h4><p>åº”ç”¨GLMåˆ°ä¸‹é¢çš„å‡è®¾ï¼š</p><ol><li><p>$y|X;\theta ~ N(\mu,1)$,åˆ™$\eta = \mu,T(y) = y$.</p></li><li><p>Derive Hypothesis function $h_\theta(X) = \mathbb{E}[y|X;\theta] = \mu = \eta$.</p></li><li><p>Adopt linear model $\eta = \theta ^TX $: $h_\theta (X) = \eta = \theta ^T X$.</p></li></ol><p>Canonical response function:$g(\eta) = \mu = \eta$</p><p>Canonical link function:$\eta = g^{-1}(\mathbb{E}[T(y);\eta] = gâ€™(\mu) = mu$</p><h4 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h4><ol><li><p>$y|X;\theta ~ Bernoulli(\phi)$ï¼Œåˆ™$\eta = \log {\left(\frac {\phi}{1 - \phi}\right)},T(y) = y$</p></li><li><p>Derive hypothesis function $h_\theta(X) = \mathbb{E}[y|X;\theta] = \phi = $,åˆ™$\phi = \frac {1}{1 + e^{-\eta} }$</p></li><li><p>Adopt linear model $\eta = \theta ^T X$: $h_\theta(X) = \phi = \frac {1}{1 + e^{-\theta^TX} }$</p></li></ol><p>Canonical response function:$ Ï† = g(Î·) = sigmoid(Î·)$ </p><p>Canonical link function : $Î· = g^{âˆ’1}(Ï†) = logit(Ï†)$</p><h4 id="Possion-Regression"><a href="#Possion-Regression" class="headerlink" title="Possion Regression"></a>Possion Regression</h4><ol><li><p>$y|X;\theta ~ P(\lambda)$,åˆ™$\eta = \log{\lambda},T(y) = y$</p></li><li><p>Derive hypothesis function $h_\theta(X) = \mathbb{E}[y|X;\theta] = \lambda = e^{\eta}$</p></li><li><p>Adopt linear model $\eta = \theta^TX$: $h_\theta (X) = \lambda = e^{\theta^TX}$</p></li></ol><p>Canonical response function:$\lambda = g(\eta) = e^{\eta}$</p><p>Canonical link function:$\eta = g^{-1}(\lambda) = log(\lambda)$</p><h4 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h4><p>æœ€åæˆ‘ä»¬æ¥æ¨æ–­ä¸‹Softmax Regressionï¼Œå› ä¸ºsoftmaxæ˜¯å¤šç»´çš„åˆ†å¸ƒï¼Œæ‰€ä»¥è¿˜æ˜¯æœ‰ç‚¹éš¾åº¦çš„ã€‚</p><p>é¦–å…ˆæˆ‘ä»¬åº”è¯¥å†™å‡ºå®ƒçš„åˆ†å¸ƒå‡½æ•°å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">p(y;\theta) = \prod_{i=1}^k \phi_i^{\mathbf{1}\{y = i\} }</script><p>ç„¶åéœ€è¦åšçš„æ˜¯æŠŠå®ƒå†™æˆExponential Familyçš„å½¢å¼.</p><p>å¦‚æœç…§ç€å¹³æ—¶çš„æ€è·¯ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¼šå‘ç°ï¼Œ$a(\eta) = 0$,è¿™æ˜¯ä¸å…è®¸å‘ç”Ÿçš„ï¼ˆWhyï¼Ÿï¼‰ã€‚å› æ­¤æˆ‘ä»¬éœ€è¦æƒ³åŠæ³•ï¼Œå¦‚æœæŠŠ$\phi_k$ç§»åˆ°æœ€åï¼Œåˆå¦‚ä½•ä¿è¯å‰é¢æ²¡æœ‰$y$çš„å½±å“ï¼Ÿ</p><p>ä»”ç»†è§‚å¯Ÿä¸Šå¼ï¼Œæˆ‘ä»¬å‘ç°å¯ä»¥å°†ä¸Šå¼å†™ä¸ºï¼š$\prod _{i=1}^k \left(\frac{\phi_i}{\phi_k} \right)^{\mathbf{1}\{y=i\} } \phi_k$. (!!!Genius!).</p><ul><li><p>$\eta = \left [ \begin{matrix}<br>\log{\frac {\phi_1}{\phi_k} }\\<br>\log{\frac {\phi_2}{\phi_k} }\\<br>â€¦\\<br>\log{\frac{\phi_{k-1} }{\phi_k} }<br>\end{matrix} \right ]$</p></li><li><p>$T(y) = \left[<br>  \begin{matrix}<br>  \mathbf{1}\{y=1\}\\<br>  \mathbf{2}\{y=2\}\\<br>  â€¦\\<br>  \mathbf{k-1}\{y=k-1\}<br>  \end{matrix}<br>  \right]$</p></li><li><p>$b(y) = 1$</p></li><li><p>$a(\eta) = -\log{(\phi_k)}$</p></li></ul><p>æœ‰äº†ä¸Šé¢çš„æ ¼å¼ï¼Œå¦‚ä½•è¿ç”¨çº¿æ€§æ¨¡å‹å°±æ¯”è¾ƒé¡ºç†æˆç« äº†ã€‚</p><ol><li><p>$y|X;\theta ~ P(\Phi)$,åˆ™$\eta ,T(y)$å¦‚ä¸Šã€‚</p></li><li><p>Derive hypothesis function :</p><script type="math/tex; mode=display">h_\theta(X) = \mathbb{E}[y|X;\theta] = \Phi =</script><script type="math/tex; mode=display">\begin{bmatrix} \frac {e^{\eta_1} }{\sum _{i=1}^k e^{\eta_i} }\\\frac {e^{\eta_2} }{\sum _{i=2}^k e^{\eta_i} }\\...\\1 - \frac {e^{\eta_k} }{\sum _{i=1}^k e^{\eta_i} }\end{bmatrix}</script><p>(æ³¨æ„ï¼Œåœ¨è¿™é‡Œä¸ºäº†æ–¹ä¾¿æˆ‘ä»¬å®šä¹‰$\eta_k = \log { {\eta_k}{\eta_k} } = 0$)</p></li><li><p>Adopt linear model $eta = \Theta^TX$:</p><p>$ h_\Theta (X)  =\begin{bmatrix}<br>\frac {e^{\theta_1^TX} }{\sum _{i=1}^k e^{\theta_i^TX} }\\<br>\frac {e^{\theta_2^TX} }{\sum _{i=1}^k e^{\theta_i^TX} }\\<br>â€¦\\<br>1 - \frac {e^{\theta_k ^TX} }{\sum _{i=1}^k e^{\eta_i^TX} }<br>\end{bmatrix} $</p></li></ol><p>Canonical response function:$\phi_i = g(\eta) =\frac  {e^{\eta_i} }{\sum _{i=2}^k e^{\eta_i} }$</p><p>Canonical link function:$\eta_i = g^{-1}(\phi) = \log {\frac {\phi_i}{\phi_k} }$.</p><p>å› æ­¤ï¼Œæ ¹æ®å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥æ¨å‡ºéœ€è¦çš„hypothesis funtionçš„å½¢å¼ï¼Œä»è€Œè¿›è¡Œè¿›ä¸€æ­¥çš„å­¦ä¹ ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æ•°æ®å­¦ä¹ è¯¾ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LFD class </tag>
            
            <tag> mathematics </tag>
            
            <tag> exponential family </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Learning From Dataâ€”â€”Softmax Regression</title>
      <link href="/2018/10/16/Learning-From-Data%E2%80%94%E2%80%94Softmax-Regression/"/>
      <url>/2018/10/16/Learning-From-Data%E2%80%94%E2%80%94Softmax-Regression/</url>
      
        <content type="html"><![CDATA[<p>Learning From Dataæ˜¯ç ”ç©¶ç”Ÿä¿®çš„ä¸€é—¨è¯¾ï¼Œå…¶å®ä¹Ÿå°±æ˜¯æœºå™¨å­¦ä¹ çš„å¦ä¸€ç§å«æ³•ã€‚ç¬¬ä¸€é—¨è¯¾ä¸­ä»‹ç»äº†Linear Regressionï¼ŒLogistic Regressionï¼ŒSoftmax Regression.è™½ç„¶å‰ä¸¤ä¸ªéƒ½å­¦è¿‡ï¼Œä½†æ˜¯è¿˜æ˜¯æœ‰ä¸€äº›æ”¶è·ï¼Œæ¯”å¦‚å¦å¤–çš„è§£é‡Šæ–¹æ³•ç­‰ç­‰ã€‚<a id="more"></a></p><h2 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h2><p>è¿™æ¬¡Linear Regressionä¸»è¦å­¦ä¹ åˆ°çš„æ–°çš„ä¸œè¥¿æ˜¯ï¼Œä»æ¦‚ç‡è§’åº¦æ¥ç†è§£ä¸ºä»€ä¹ˆä½¿ç”¨Least Square.</p><p>å‡è®¾ç›®æ ‡å‡½æ•°æ˜¯ $Y = W^Tx+ \epsilon$ï¼Œå…¶ä¸­$\epsilon$æ˜¯Nç»´å‘é‡.å‡è®¾$\epsilon$æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆIIDï¼‰çš„ï¼Œè€Œä¸”æ»¡è¶³é«˜æ–¯åˆ†å¸ƒ$N(0,\sigma)$,åˆ™:</p><script type="math/tex; mode=display">p(y_n|X_n,W) = \frac 1 {\sqrt{2\pi }\sigma } exp \left(-\frac{(Y_n - W^TX_n)^2}{2\sigma ^2} \right)</script><p>è€Œå‡ºç°è¿™ä¸ªæ ·æœ¬çš„æ¦‚ç‡å¦‚ä¸‹ï¼š</p><p>$L(W) = p(Y|X,W) = \prod _{n=1}^N p(y_n|X_n,W)$.</p><p>æˆ‘ä»¬æƒ³è¦æ±‚å¾—æœ€å¤§æ¦‚ç‡ä¼°è®¡ï¼ˆMaximum Likelihood Estimationï¼‰:$W_{MLE} = argmax_W(L{W})$.</p><p>å±•å¼€ä¹‹å‰æˆ‘ä»¬åº”è¯¥åŠ ä¸ªlogï¼Œå› ä¸ºæˆ‘ä»¬å–œæ¬¢sumè€Œä¸æ˜¯prodã€‚å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\begin{array} {l}\log{L(W)} &= \sum _{n=1}^N(\log{\frac 1 {\sqrt {2 \pi} \sigma}}- \frac 1 {2\sigma ^2 } (Y_n - W^TX_n)^2 \log e )\\ &= m \log{\frac 1 {\sqrt {2 \pi }\sigma}} - \frac 1 {2\sigma ^2 }  \sum_{n=1}^N  (Y_n - W^TX_n)^2 \end{array}</script><p>æ‰€ä»¥ï¼Œ$argmax_{W} (L(W)) = argmin_{W} (\sum_{n=1}^N  (Y_n - W^TX_n)^2 )$.è¿™ä¹Ÿæ­£æ˜¯æˆ‘ä»¬çš„cost functionçš„å®šä¹‰ã€‚</p><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>Logistic Regressionå­¦ä¹ äº†ä»å¦ä¸€ç§è§’åº¦æ€è€ƒå¾—åˆ°å¦ä¸€ç§å®šä¹‰cost functionçš„æ–¹æ³•ï¼Œå½“ç„¶æœ€ç»ˆæ•ˆæœæ˜¯ä¸€è‡´çš„ã€‚</p><p>ä¹‹å‰çš„logsitic regressionå¯¹äº$P(X_i \bigcap y_i)$çš„ä¼°è®¡å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">P(X_i \bigcap y_i) =\frac {(y_i+1)} 2 P(X_i) \times P(y_i = +1|X_i) +\frac {(1 - y_i)} 2 P(X_i) \times (1 - P(y_i = +1|X_i))</script><p>å®é™…ä¸Šæœ‰å¦å¤–ä¸€ç§å¯ä»¥è¾¾åˆ°ä¸€æ ·çš„æ•ˆæœ,ä¸è¿‡æ­¤æ—¶æˆ‘ä»¬éœ€è¦çš„å°±æ˜¯å¦å¤–ä¸€ç§å¯¹$y$çš„å®šä¹‰äº†ï¼š$y \in {0,1}$:</p><script type="math/tex; mode=display">P(X_i \bigcap y_i) = (P(X_i) \times P(y_i = 1|X_i))^{y_i} (P(X_i) \times (1 - P(y_i = 1|X_i)))^{1-y_i}</script><p>å› æ­¤å‡ºç°è¿™ä¸ªæ ·æœ¬çš„æ¦‚ç‡ä¸ºï¼š</p><script type="math/tex; mode=display">L(W) = \prod _{i=1}^N (P(X_i \bigcap y_i) = P(X_i) \times h_w(X_i) )^{y_i} (P(X_i) \times (1-h_W(X_i)))^{1-y_i} .</script><p>æˆ‘ä»¬å¯ä»¥ç•¥å»è¿™äº›$P(X_i)$,å› ä¸ºè¿™æ˜¯ç¡®å®šçš„è€Œä¸”ä¹Ÿä¸æ˜¯æˆ‘ä»¬éœ€è¦æ³¨æ„çš„ã€‚<br>è¿™æ—¶å€™logä¹‹åï¼Œå¾—åˆ°æœ€åçš„cost funtionçš„å½¢å¼ä¸ä¹‹å‰å°±æœ‰äº†ä¸€äº›ä¸åŒï¼š</p><script type="math/tex; mode=display">f(W) = -(\sum _{i=1}^N y_i \log h_W{X_i} + (1 - y_i)\log{ (1 -h_W{X_i})})</script><p>æ¥ä¸‹æ¥è¦åšçš„å°±æ˜¯æ±‚è¿™ä¸ªå‡½æ•°çš„æ¢¯åº¦ï¼Œä½†æ˜¯ä¸ºäº†çœ‹çš„æ¸…æ¥šï¼Œé¦–å…ˆè¯´æ˜ä¸‹å„ä¸ªå‡½æ•°çš„æ„ä¹‰ï¼š</p><script type="math/tex; mode=display">h_W(X) = \frac 1 {1 - e^{-g_W(X)}}</script><script type="math/tex; mode=display">g_W(X) = W^TX</script><p>æ±‚æ¢¯åº¦è¿‡ç¨‹å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\begin{array}{l}\nabla f(W) &= -(\sum _{i=1} ^N (y_i log(h_W(X_i)) + (1-y_i) log(1-h_W(X_i))))\\&= -(\sum_{i=1}^N \left[ \frac {y_i}{h_W(X_i)} - \frac {1-y_i}{1-h_W(X_i)} \right] h_W'(x))\\&= -(\left[y_i + \frac {1-y_i}{e^{-g_W(X_i)}} \right] \frac {e^{-g_W(X_i)}}{1 - e^{-g_W(X_i)}} g_W'(X_i)) \\\ &= -(\left[ \frac 1 {1 - e^{-g_W(X_i)}} - y_i \frac {1 - e^{-g_W(X_i)}}{1 - e^{-g_W(X_i)}} \right] g_W'(X_i))\\&=-(y_i - h_W(X_i))X_i  \end{array}</script><p>è€Œä¸”è¿™ä¸ªcost functionçš„å¥½å¤„æ˜¯ï¼Œåˆ©ç”¨æ¢¯åº¦ä¸‹é™çš„æ—¶å€™å®ƒå’Œçº¿æ€§å›å½’çš„æ­¥éª¤æ˜¯éå¸¸ç›¸ä¼¼çš„,çº¿æ€§å›å½’ä¸­ï¼š</p><script type="math/tex; mode=display">\frac {\partial f(W)}{\partial w_j} = \sum _{i=1}^N(y_i - g_{W}(X_i))x_{i,j}.</script><p>å³</p><script type="math/tex; mode=display">\nabla f(W) = \sum_{i=1}^N (y_i - g_{W}(X_i))X_i.</script><p>æœ€åå›åˆ°ä¸¤ç§ä¸åŒçš„cost funtionï¼Œå®é™…ä¸Šä¸¤è€…æœ¬è´¨æ²¡æœ‰å¤ªå¤§çš„åŒºåˆ«ï¼Œåªæ˜¯negativeï¼Œpositiveçš„æ ‡è¯†æ•°å­—ä¸åŒã€‚æœ€åå¾—åˆ°çš„ç»“æœå¯èƒ½ä¹Ÿä¸ä¸€æ ·ï¼Œä½†æ˜¯å·®è·ä¸ä¼šå¤ªå¤§ï¼Œéƒ½ä¼šå¾—åˆ°æ¯”è¾ƒç†æƒ³çš„ç»“æœã€‚</p><h2 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h2><p>Softmax Regressionæ˜¯ä¸€ç§å¤šç»´åˆ†ç±»ç®—æ³•ã€‚ä¾ç„¶æ˜¯ç«™åœ¨æ¦‚ç‡çš„è§’åº¦æ¥è®¨è®ºã€‚</p><p>å‡è®¾å…±æœ‰kç±»ï¼Œå³$y \in {1,â€¦,k}$.æˆ‘ä»¬å…ˆç»™å‡ºä¸€ä¸ªæ¦‚ç‡ä¼°è®¡ï¼Œä¹‹å‰å¾—æ¦‚ç‡ä¼°è®¡æ˜¯logisticå‡½æ•°ï¼Œç°åœ¨æˆ‘ä»¬ç»™å‡ºå¦ä¸€ç§æƒ…å†µï¼š</p><script type="math/tex; mode=display">h_W(X_i) = \left [    \begin{matrix}p(y_i = 1|X_i;W_1)\\p(y_i=2|X_i;W_2)\\...\\p(y_i=k|X_i;W_k) \end{matrix}\right] = \frac 1 {\sum _{j=1}^k e^{W_j^TX_i}}  \left[\begin{matrix} e^{W_1^TX_i}\\ e^{W_2^TX_i}\\ ...\\ e^{W_k^TX_i} \end{matrix}\right]  = softmax(W,X_i)</script><p>åŒæ—¶æˆ‘ä»¬å®šä¹‰:$softmax(z_i) = p{y = i|X,W} = \frac {e^{z_i}}{\sum _{j=1}{k} e^{z_j}}$ï¼Œæ­¤æ—¶$i \in {1,â€¦,k}$.</p><p>å½“ç„¶ï¼ŒWå‚æ•°ä¹Ÿä¼šå‘ç”Ÿå˜åŒ–ï¼š</p><script type="math/tex; mode=display">W = \left[\begin{matrix}-W_1^T-\\-W_2^T-\\...\\-W_k^T- \end{matrix}\right]</script><p>å› æ­¤æˆ‘ä»¬ç¡®å®šäº†ç»™å®š$W$å’Œ$X$çš„æ—¶å€™ï¼Œ$y$çš„æ¦‚ç‡ã€‚</p><p>è€Œå‡ºç°å½“å‰æ ·æœ¬çš„æ¦‚ç‡ï¼ˆæˆ‘ä»¬å¿½ç•¥$P(W,X)$,åƒä¹‹å‰ä¸€æ ·å®ƒä¸ä¼šå½±å“ç»“æœï¼‰ï¼š</p><script type="math/tex; mode=display">L(W) = \prod_{i=1}^{N} P(y_i|X_i,W).</script><p>å…¶å®æˆ‘ä»¬å¯ä»¥æƒ³è±¡çš„æ˜¯è¿™ä¸ªå¼å­å±•å¼€äº†åä¼šå¾ˆå¤æ‚ï¼Œå› ä¸ºå¯¹$y_i$å¯èƒ½çš„å„ä¸ªæƒ…å†µä¹Ÿè¦è¿ä¹˜ã€‚ä¸å¦‚å…ˆlogå¥½äº†ï¼š</p><script type="math/tex; mode=display">\begin {array}{l} F(W) = \log{L(W)} &= \sum_{i=1}^N \log {p(y_i|X_i,W) }\\ &= \sum_{i=1}^N \log{\prod_{j=1}^k p_{W_j}(j|X_i,W)^{\mathbf{1}\{y_i = j\}}}\\ &= \sum_{i=1}^N \sum_{j=1}^k \mathbf{1}\{y_i = j\} \log { \frac {e^{W_jX_i}}{\sum _{l=1}^k e^{W_l^TX_i}}}\end {array}</script><p>è¿™ä¸ªä¸œè¥¿ï¼Œå…¶å®æˆ‘æ¨ç®—çš„æ—¶å€™å¯¹ä»–çš„ç¬¦å·è¡¨ç¤ºå·²ç»å¾ˆå¤´å¤§äº†ã€‚ä½†æ˜¯å®ƒè™½ç„¶å¤æ‚ä½†åŸç†ä¸éš¾æ‡‚ï¼Œå’Œlogistic regressionçš„é“ç†åŸºæœ¬ä¸Šä¸€æ ·çš„ã€‚</p><p>æœ€åï¼Œæˆ‘ä»¬å°±æ˜¯è¦æ±‚è¿™ä¸ªå‡½æ•°çš„æ¢¯åº¦äº†ã€‚è¿™ä¸ªå‡½æ•°çš„æ¢¯åº¦æ±‚è§£æƒ³å¿…æ˜¯éå¸¸å¤æ‚çš„ï¼Œä½†æ˜¯å®é™…ä¸Šæ²¡æœ‰æƒ³è±¡çš„é‚£ä¹ˆéº»çƒ¦ã€‚æœ€åçš„ç»“æœä¹Ÿéå¸¸çš„ç®€å•ï¼š</p><script type="math/tex; mode=display">\nabla _{W_j} F(W) = \sum _{i=1}^m \left( \mathbf{1}\{y_i = j\} \log {\frac {e^{W_j^TX_i}}{\sum _{l=1}^k e^{W_l^TX_i}}} + \mathbf{1}\{y_i \ne j\} \log {\frac {e^{W_{y_i}^TX_i}}{\sum _{l=1}^k e^{W_l^TX_i}}} \right)</script><p>æˆ‘ä»¬ä»”ç»†è§‚å¯ŸåŸå¼å°±å¯ä»¥åŒ–ç®€ä¸Šé¢çš„æ ·å­ã€‚ä¸ºäº†ç®€åŒ–åé¢çš„æ­¥éª¤ï¼Œå‡è®¾$g(W_l) = W_l^TX_i$.<br>ç¬¬ä¸€ç§æƒ…å†µ$ {y_i = j}$ï¼š</p><script type="math/tex; mode=display">f_1(W) = \log {\frac {e^{g(W_j)}}{\sum_{l=1}^k e^{g(W_l)}}}</script><script type="math/tex; mode=display">\begin{array}{l}\nabla _{W_j} f_1(W)  &= \frac {\sum_{l=1}^k e^{g(W_l)}}{e^{g(W_j)}} \cdot \frac { -e^{2g(W_j)}+ (\sum_{l=1}^k e^{W_l})e^{W_j}}{(\sum_{l=1}^k e^{g(W_l)})^2} \cdot g'(W_j)\\&= \frac{\sum_{l=1}^k e^{g(W_l) - e^{g(W_j)}}}{\sum_{l=1}^k e^{g(W_l)}} \cdot g'(W_j) \\\ &= (1 - p(y_i = l|X_i,W))X_i\end{array}</script><p>ç¬¬äºŒç§æƒ…å†µ$y_i \ne j$,å‡è®¾$y_i = q \ne j$:</p><script type="math/tex; mode=display">f_2(W) = \log {\frac {e^{g(W_q)}}{\sum_{l=1}^k e^{g(W_l)}}}</script><script type="math/tex; mode=display">\begin{array}{c}\nabla _{W_j} f_2(W) &=\frac {\sum_{l=1}^k e^{g(W_l)}}{e^{g(W_q)}} \cdot \frac { -e^{g(W_j) e^{g(W_q)}}}{(\sum_{l=1}^k e^{g(W_l)})^2} \cdot g'(W_j)\\&= - p(y_i = l|X_i,W)X_i\end{array}</script><p>ä¹Ÿæ˜¯ä¸¤ç§æƒ…å†µçš„å·®åˆ«åªæœ‰å‰é¢æ˜¯å¦åŠ ä¸€ä¸ª1ã€‚åˆå¹¶ä¸¤ç§æƒ…å†µï¼Œå¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">\nabla _{W_j} F(W) = \sum _{i=1} ^N [(\mathbf{1}\{y_i=j\} - p(y_i=l|X_i,W))X_i]</script><p>ä¸Šé¢æ¨å‡ºæ¥çš„è¦æ³¨æ„æ˜¯æˆ‘ä»¬æƒ³è¦æœ€å¤§åŒ–çš„å‡½æ•°ã€‚</p><p>è€Œcost funtionçš„æ¢¯åº¦åº”è¯¥æ˜¯ï¼š $\sum _{i=1} ^N [(-\mathbf{1}\{y_i=j\} + p(y_i=l|X_i,W))X_i] $</p><p>å¯¹äºsoftmax regressionæˆ‘ä»¬éœ€è¦çŸ¥é“ï¼Œå®ƒçš„å‚æ•°$W_j$ä¹‹é—´å¹¶ä¸æ˜¯ç‹¬ç«‹çš„ï¼Œå› ä¸ºå„ä¸ªæ¦‚ç‡åŠ èµ·æ¥ä¸º1ï¼Œæœ‰è¿™ä¸ªçº¦æŸåå®é™…ä¸Šï¼Œåªè¦çŸ¥é“$k-1$ä¸ªå‚æ•°ï¼Œå°±å¯ä»¥ç¡®å®šè¿™ä¸ªæ¨¡å‹ã€‚</p><p>å®é™…ä¸Šï¼Œå¯ä»¥å¾ˆå®¹æ˜“è¯æ˜logistic regression æ˜¯ softmax regressionçš„ç‰¹æ®Šæƒ…å†µã€‚</p><p>ä»¥ä¸Šå°±æ˜¯ä¸ŠèŠ‚è¯¾å­¦åˆ°çš„æ‰€æœ‰æ–°ä¸œè¥¿ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æ•°æ®å­¦ä¹ è¯¾ç¨‹ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> LFD class </tag>
            
            <tag> regression </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æ•°å­¦â€”â€”Newton Method</title>
      <link href="/2018/10/16/%E6%95%B0%E5%AD%A6%E2%80%94%E2%80%94Newton-Method/"/>
      <url>/2018/10/16/%E6%95%B0%E5%AD%A6%E2%80%94%E2%80%94Newton-Method/</url>
      
        <content type="html"><![CDATA[<p>æ¢¯åº¦ä¸‹é™æ—¶å€™ï¼Œæœ‰æ—¶å€™æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Newton Direction.ç‰›é¡¿è¿­ä»£æ³•å…¶å®å¤§å®¶å¬èµ·æ¥å¾ˆç†Ÿæ‚‰çš„ã€‚<br><a id="more"></a></p><p>é¦–å…ˆæ¥è¯´æ˜ä¸‹ï¼Œç®€å•çš„ç‰›é¡¿è¿­ä»£æ³•çš„åŸç†ã€‚ç‰›é¡¿è¿­ä»£æ³•æ˜¯æ±‚è¿‘ä¼¼è§£çš„ä¸€ä¸ªåŠæ³•ï¼Œå¾ˆå¤šæ—¶å€™è§£æ— æ³•ç®—å‡ºæ¥ï¼Œæˆ‘ä»¬åªèƒ½ç”¨ç‰›é¡¿è¿­ä»£æ³•æ¥ä¸€æ­¥æ­¥é€¼è¿‘ã€‚</p><p>é¦–å…ˆç»™ä¸ªå¾ˆç›´è§‚çš„ä¾‹å­ï¼Œä¹Ÿå°±æ˜¯ä¸€ç»´çš„å‡½æ•°ã€‚å…ˆè§‚çœ‹ä¸€ä¸‹ä¸‹é¢çš„gifã€‚</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/NewtonIteration_Ani.gif" alt=""></p><p>ä¸ºäº†æ±‚å¾—$f(x) = 0$,æˆ‘ä»¬ä»å›¾ä¸Šç›´è§‚çœ‹åˆ°å¯ä»¥ä¸€ç›´è¿™æ ·é€¼è¿‘ï¼Œæœ€ç»ˆä¼šé€¼è¿‘åˆ°f(x) = 0çš„è§£ã€‚</p><p>åŸç†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å°†f(x)ä¸€é˜¶æ³°å‹’å±•å¼€,å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">f(x) \approx f(x_0)+f'(x_0)(x - x_0) = g(x)</script><p>è€Œä¸Šå¼g(x) = 0æ˜¯å¾ˆå®¹æ˜“è§£å†³çš„ï¼š$x = x_0 - \frac {f(x_0)}{fâ€™(x_0)}$.</p><p>å› ä¸ºæ³°å‹’åªæ˜¯è¿‘ä¼¼ï¼Œå› æ­¤ä¸Šè¿°å¾—åˆ°çš„è§£å¹¶ä¸æ˜¯çœŸæ­£çš„è§£ï¼Œåªæ˜¯ç¦»åŸæœ‰çš„è§£æ›´æ¥è¿‘äº†ã€‚ä¹Ÿå°±æ˜¯ï¼Œç‰›é¡¿è¿­ä»£æ³•ç§ï¼Œä¸‹ä¸€æ­¥æ›´æ–°ç­–ç•¥ä¸º:$x_{n+1} =x_n - \frac {f(x_n)}{fâ€™(x_n)} $.</p><p>å¦‚ä½•å°†ç‰›é¡¿è¿­ä»£æ³•ç”¨æ¥è§£å†³ä¼˜åŒ–é—®é¢˜ï¼Ÿæˆ‘ä»¬çŸ¥é“ä¼˜åŒ–é—®é¢˜ï¼Œæƒ³è¦å¾—åˆ°æœ€å°å€¼ï¼Œæˆ–è€…æœ€å¤§å€¼ï¼Œåœ¨è¯¥ç‚¹å¯¼æ•°æ˜¯ä¸º0çš„ï¼Œè¿™ä¸ªé—®é¢˜å°±å˜æˆäº†ï¼Œå¦‚ä½•æ‰¾åˆ°å¯¼æ•°ä¸º0çš„ç‚¹ï¼Œé‚£ä¹ˆå°±å¾ˆç®€å•äº†ï¼Œå¯¹äºä¸€ç»´å‡½æ•°çš„ä¼˜åŒ–é—®é¢˜è¿­ä»£æ­¥éª¤å¦‚ä¸‹:$x_{n+1} =x_n - \frac {fâ€™(x_n)}{fâ€™â€™(x_n)} $.</p><p>å¤šç»´å‡½æ•°æ¥è¯´ï¼Œæƒ…å†µè¾ƒä¸ºå¤æ‚ä¸€ç‚¹ï¼Œå› ä¸ºé«˜çº¬åº¦çš„äºŒé˜¶å¯¼æ•°å®åœ¨æ˜¯å¾ˆå¤šã€‚ä¸è¿‡åŸç†ä¹Ÿæ˜¯å˜åŒ–ä¸å¤§çš„ï¼Œæˆ‘ä»¬éœ€è¦åˆ©ç”¨HessiançŸ©é˜µï¼š</p><p>$x_{n+1} = x_{n+1}-H_f^{-1}(x_n)\nabla f(x_n)$.</p><p>HessiançŸ©é˜µå®šä¹‰å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">H_f = \begin{bmatrix} \frac {\partial^2f}{\partial x_1^2}& \frac {\partial^2f}{\partial x_1 \partial x_2}&...&\frac {\partial^2f}{\partial x_1 \partial x_n} \\\frac {\partial^2f}{\partial x_2 \partial x_1}& \frac {\partial^2f}{\partial x_2 \partial x_2}&...&\frac {\partial^2f}{\partial x_2 \partial x_n} \\...\\\frac {\partial^2f}{\partial x_n \partial x_1}& \frac {\partial^2f}{\partial x_n \partial x_2}&...&\frac {\partial^2f}{\partial x_n^2} \end{bmatrix}</script><p>å¯ä»¥çœ‹åˆ°çš„æ˜¯ï¼Œå¦‚æœç»´åº¦è¾ƒé«˜ï¼Œè¿™ä¸ªæµ·æ£®çŸ©é˜µçš„æ±‚é€†æ˜¯éå¸¸è€—è´¹æ—¶é—´çš„ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œä¼˜åŒ–é—®é¢˜æ—¶å€™ï¼Œç»´åº¦è¾ƒä½çš„æƒ…å†µä¸‹ï¼Œå®ƒçš„æ•ˆæœè¿˜æ˜¯éå¸¸å¥½çš„ï¼Œæ¯”æ¢¯åº¦ä¸‹é™æ›´å¿«ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æ•°å­¦ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mathematics </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”Soft-Margin Support Vector Machine</title>
      <link href="/2018/10/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Soft-Margin-Support-Vector-Machine/"/>
      <url>/2018/10/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Soft-Margin-Support-Vector-Machine/</url>
      
        <content type="html"><![CDATA[<p>ä¹‹å‰æåˆ°çš„ä¹‹å‰çš„SVMä¼šoverfittingé™¤äº†æ¨¡å‹è¿‡äºå¤æ‚ï¼Œå¦ä¸€ä¸ªé—®é¢˜å°±æ˜¯å®ƒè¦å°†æ ·æœ¬åˆ†ç±»åœ¨è®­ç»ƒé›†ä¸Šåšåˆ°å®Œå…¨æ­£ç¡®ã€‚è¿™æ—¶å€™ä¸€äº›å™ªå£°å°±ä¼šå¾ˆå¤§ç¨‹åº¦ä¸Šå½±å“ç»“æœã€‚ä¸ºäº†é€‚åº”è¿™äº›å™ªå£°ï¼Œä¸å¾—ä¸åšå‡ºå¾ˆå¤æ‚çš„æ¨¡å‹ã€‚<a id="more"></a></p><p>å› æ­¤æœ‰æ—¶å€™æˆ‘ä»¬å¸Œæœ›å¯ä»¥å®¹å¿ä¸€äº›æ ·æœ¬è¢«é”™è¯¯åˆ†ç±»ã€‚å› æ­¤åŸæœ‰çš„æ•°å­¦æ¡ä»¶å°±éœ€è¦æ”¹å˜ä¸€ä¸‹äº†ã€‚</p><p>ç°åœ¨æˆ‘ä»¬å›åˆ°æœ€å¼€å§‹æè¿°çš„é—®é¢˜ï¼š</p><p><strong>min</strong>  $\frac 1 2 W^TW$</p><p>$s.t.  y_n(W^TX_n+b) \ge 1,n =1,2,â€¦,N $.</p><p>ç°åœ¨æˆ‘ä»¬ä¸è¦æ±‚æ‰€æœ‰çš„$ y_n(W^TX_n+b) \ge 1$,å¯ä»¥å®¹å¿ä¸€äº›é”™è¯¯ã€‚å½“ç„¶è¿™ä¸ªé”™è¯¯ä¸èƒ½æ— é™å¤§ã€‚å‡è®¾ç°åœ¨è¢«åˆ†é”™çš„æ ·æœ¬çŠ¯çš„é”™è¯¯æ˜¯$\xi _n$,é‚£ä¹ˆé—®é¢˜å¯ä»¥è¢«æè¿°ä¸ºä¸‹ï¼š</p><p><strong>min</strong>  $\frac 1 2 W^TW + C\sum_{n=1}^N\xi_n$</p><p>$s.t.  y_n(W^TX_n+b) \ge 1 - \xi_n,n =1,2,â€¦,N $.</p><p>$\xi_n \ge 0,n=1,2,â€¦,N$</p><p>ä»”ç»†çœ‹ä¸Šé¢çš„æè¿°æˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œå¦‚æœä¸€ä¸ªæ ·æœ¬æ²¡æœ‰çŠ¯é”™ï¼Œé‚£ä¹ˆå®ƒå¯¹åº”çš„$\xi_n = 0$.å¦‚æœä¸€ä¸ªæ ·æœ¬çŠ¯é”™äº†ï¼Œé‚£ä¹ˆå®ƒå¯¹åº”çš„$\xi_n = 1 - y_n(W^TX_n+b)$.</p><p>å› æ­¤å®é™…ä¸Šä¸Šé¢çš„é—®é¢˜ä¹Ÿå¯ä»¥è¢«æè¿°æˆä¸‹é¢çš„å½¢å¼ï¼š</p><p><strong>min</strong>  $\frac 1 2 W^TW + C\sum_{n=1}^N \ell(y_i,W^TX_i+b)$</p><p>where  $\ell(\cdot,\cdot)$ is the hinge loss defined by $\ell(x,y) \triangleq max\{1-yz,0\}$.</p><p>å¸¸æ•°$C$çš„ä½œç”¨åœ¨äºæˆ‘ä»¬å¯ä»¥æ¥å—çš„çŠ¯é”™ç¨‹åº¦å¤§å°ã€‚å¯ä»¥æƒ³è±¡çš„æ˜¯å¦‚æœ$C$æ¯”è¾ƒå¤§ï¼Œæ•´ä¸ªç›®æ ‡æ—¢ç„¶åœ¨æœ€å°åŒ–ä¸Šé¢çš„å¼å­ï¼Œé‚£ä¹ˆ$\xi_n$çš„å€¼å°±ä¼šå˜å¾—éå¸¸å°ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å¯ä»¥å¯¹åˆ’åˆ†é”™è¯¯çš„å®¹å¿åº¦æ˜¯æ¯”è¾ƒå°çš„ï¼Œå¦‚æœ$C$æ¯”è¾ƒå¤§ï¼Œé‚£ä¹ˆå®¹å¿åº¦åˆ™è¾ƒå¤§ï¼Œå› æ­¤è¿™é‡Œä¹Ÿæœ‰ä¸€ä¸ªæƒè¡¡ã€‚</p><p>æˆ‘ä»¬ä»ä¸Šé¢çš„æè¿°å‡ºå‘ç»§ç»­æ¨å¯¼è¿™ä¸ªé—®é¢˜çš„Lagrange Dual Problemï¼š</p><script type="math/tex; mode=display">\frac 1 2||W||^2 + C\sum_{n=1}^N \xi_n + \sum_{n=1}^N \alpha_n(1 - \xi_n - y_n(W_TX_n+b)) + \sum_{n=1}^N(-\beta \xi_n)</script><p>è¿™ä¸ªæ—¶å€™ï¼Œå®é™…ä¸Šæ‰€æœ‰çš„å…³äº$W,b$çš„åå¯¼æ•°ä¸ä¹‹å‰éƒ½æ˜¯ä¸€è‡´çš„ã€‚åœ¨è¿™é‡Œå°±ä¸è¯¦ç»†æ¨å¯¼äº†ï¼Œåªæ˜¯æœ€åæˆ‘ä»¬éœ€è¦å¯¹$\xi_n$æ±‚åå¯¼ï¼š</p><script type="math/tex; mode=display">\frac {\partial \ell} {\partial \xi_n} = 0 = C - \alpha_n - \beta _n</script><p>ç”±ä¸Šå¼å¯ä»¥å¾—åˆ°ï¼š$\beta _n = C - \alpha_n$å› ä¸ºæˆ‘ä»¬æœ‰å‚æ•°é™åˆ¶ï¼Œ$\alpha_n \ge 0,\beta_n \ge 0,n=1,â€¦,N$,å› æ­¤å®é™…ä¸Šæˆ‘ä»¬å¯ä»¥å¾—åˆ°çš„çº¦æŸæ˜¯ï¼š$ 0 \leq \alpha_n \leq C$.</p><p>åŒæ—¶ç”±ä¸Šé¢çš„ç»“è®ºï¼Œå†ç»“åˆåŸæ¥çš„å¼å­ï¼Œè¿˜å¯ä»¥æ¶ˆæ‰çš„æ˜¯$\xi$.</p><p>å› æ­¤æœ€åå¾—åˆ°çš„é‚£äº›KKTæ¡ä»¶ä¸åŸæ¥HardMaginå”¯ä¸€çš„ä¸åŒå°±åœ¨äº$\alpha_n$çš„é™åˆ¶å˜äº†ã€‚ä»è¿™é‡Œå¯ä»¥çœ‹å‡ºæ¥$C$çš„ä½œç”¨:$C$å¾ˆå¤§çš„æ—¶å€™ï¼Œè¯´æ˜è¿™ä¸ªé™åˆ¶ç›¸å¯¹åŸæ¥è¾ƒå°ï¼Œä¹Ÿå°±æ˜¯è¦æ±‚çŠ¯é”™è¾ƒå°‘ï¼ˆå› ä¸ºåŸæ¥çš„æƒ…å†µæˆ‘ä»¬æ˜¯ä¸å…è®¸çŠ¯é”™è¯¯çš„ï¼‰ã€‚</p><p>é€šè¿‡äºŒæ¬¡è§„åˆ’ï¼Œæˆ‘ä»¬å¯ä»¥ä¸€æ ·å¾—åˆ°$\alpha_n $çš„å€¼ï¼Œä»è€Œå¾—åˆ°$W$ï¼Œä¸$b$.ä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯$b$ä¸ä¹‹å‰çš„ç®—æ³•ä¸ä¸€æ ·äº†ã€‚</p><p>ä¹‹å‰æˆ‘ä»¬é€šè¿‡$a_n(1 - y_n(W^TX_n+b)) = 0$ï¼Œé€šè¿‡æ‰¾åˆ°æ˜¯æ”¯æŒå‘é‡çš„ç‚¹ï¼ˆ$a_n \ne 0$ï¼‰,ä»è€Œé€šè¿‡è¯¥ç‚¹è®¡ç®—å‡ºæ¥$b = y_n - W^TX_n$.</p><p>è€Œæ­¤æ—¶ï¼Œæˆ‘ä»¬æƒ³è¦è®¡ç®—çš„$b = y_n - y_n \xi_n - W^TX_n$.</p><p>æœ‰ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¸çŸ¥é“$\ell_n$çš„å€¼å•Šï¼ˆå…¶å®æˆ‘ä»¬æ˜¯çŸ¥é“çš„$\xi_n = max(1 - y_n(W^TX_n+b),0)$ï¼Œä¸è¿‡è¿™æ˜¯è¦ç­‰$b$æ±‚å‡ºæ¥ä¹‹åï¼‰ã€‚ä½†æ˜¯æˆ‘ä»¬çŸ¥é“å¦ä¸€ä¸ªä¿¡æ¯$\beta_n \xi_n = (C - \alpha_n) \xi_n = 0$ï¼Œè¿™æ„å‘³ç€å¦‚æœ$\alpha_n \ne C$çš„ç‚¹ï¼Œ$\xi = 0$.æ‰€æœ‰å®é™…ä¸Šæˆ‘ä»¬éœ€è¦çš„æ˜¯$0&lt;\alpha_n &lt; C$çš„ç‚¹ï¼Œè¿™æ—¶å€™$\xi= 0$,å¯ä»¥è®¡ç®—å‡º$b = y_n - W^TX_n$ï¼Œè¿™æ ·çš„ç‚¹å«free Support Vector.ä¸ªåˆ«æ—¶å€™æˆ‘ä»¬æ— æ³•æ‰¾åˆ°$free Support Vector$ï¼Œé‚£ä¹ˆè¿™ä¸ª$b$çš„å€¼åªèƒ½ç”¨kktæ¡ä»¶æ¥é™åˆ¶äº†ã€‚</p><p>è¿™é‡Œï¼Œæˆ‘ä»¬å¸Œæœ›å¯ä»¥ä»”ç»†æ€è€ƒä¸€ä¸‹$\alpha_n$èƒŒåæ˜¯å¦æœ‰ä»€ä¹ˆæŒ‡ç¤ºã€‚</p><p>å¦‚æœ$\alpha_n = 0$ï¼Œé‚£ä¹ˆ$1 - \xi_n - y_n(W^TX_n +b) \leq 0$ï¼Œå¯ä»¥å¾—åˆ°çš„æ˜¯è¿™äº›ç‚¹ä¸€èˆ¬æ˜¯å®Œå…¨æ²¡æœ‰é”™è¯¯çš„ï¼Œè¿™ç‚¹å’Œä¹‹å‰æ˜¯ä¸€æ ·çš„ã€‚</p><p>å¦åˆ™ï¼Œ$C&gt;\alpha_n &gt; 0$ï¼Œåˆ™æˆ‘ä»¬é€šè¿‡ä¸Šé¢çš„æ¨å¯¼ä¹ŸçŸ¥é“$\xi_n = 0$.è¿™æ„å‘³ç€ï¼Œå®ƒä»¬å¯¹åº”çš„$y_n(W^TX_n+b) = 1$.æ‰€ä»¥è¿™äº›ç‚¹æ˜¯Support Vectorï¼Œå®ƒä»¬å®šä¹‰äº†æœ€å®½çš„åˆ†ç•Œçº¿ã€‚</p><p>è¿˜æœ‰ä¸€ç§æƒ…å†µï¼Œ$\alpha_n = C$.è¿™ç§ç‚¹å°±æ˜¯è¢«åˆ†é”™çš„ç‚¹äº†ï¼Œ$\xi \ne 0$ï¼Œä½†æ˜¯$ 1 - \xi - y_n(W^TX_n+b) = 0$.è¦æ³¨æ„çš„æ˜¯è¿™é‡Œçš„åˆ†é”™å¹¶ä¸ä¸€å®šæ˜¯åˆ†ç±»ç»“æœé”™è¯¯ï¼Œè¿˜æœ‰å¯èƒ½æ˜¯åœ¨åˆ†åˆ°marginä¸­é—´å»äº†ã€‚</p><p>ä¸Šé¢çš„å†…å®¹å°±æ˜¯Soft Margin SVMï¼Œä½†æ˜¯å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬éœ€è¦è°ƒä¸€ä¸ªå‚æ•°ï¼šCï¼Œå¦‚æœCè¿‡å¤§ï¼Œä»ç„¶å¯èƒ½ä¼šoverfittingã€‚</p><p>Soft Margin SVMå¯ä»¥ä¸Kernelç»“åˆï¼Œåœ¨å®é™…ä¸­ä½¿ç”¨æ¯”Hard Margin SVMæ›´åŠ é¢‘ç¹ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”Kernel Support Vector Macine</title>
      <link href="/2018/10/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Kernel-Support-Vector-Macine/"/>
      <url>/2018/10/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Kernel-Support-Vector-Macine/</url>
      
        <content type="html"><![CDATA[<p>ä¸Šæ¬¡é‡åˆ°çš„é—®é¢˜æ˜¯ï¼ŒQçŸ©é˜µçš„è®¡ç®—ï¼Œä»ç„¶å¯èƒ½éœ€è¦è€—è´¹å¾ˆå¤§è®¡ç®—é‡ï¼Œä¹Ÿå°±æ˜¯å¯¹äºå¾ˆé«˜ç»´åº¦çš„ç‰¹å¾è½¬æ¢ï¼Œæˆ‘ä»¬ä¸ä¸€å®šèƒ½é«˜æ•ˆè§£å†³ï¼Œæ›´ä¸ç”¨è¯´æ— é™ç»´åº¦ã€‚<a id="more"></a></p><p>å› æ­¤è¿™æ¬¡å¼•å…¥äº†æ ¸å‡½æ•°ï¼Œå‘Šè¯‰æˆ‘ä»¬å¦‚ä½•é«˜æ•ˆåœ°å¯¹å¾…ç‰¹å¾è½¬æ¢åœ°é—®é¢˜ã€‚</p><h2 id="Polynomial-Kernel"><a href="#Polynomial-Kernel" class="headerlink" title="Polynomial Kernel"></a>Polynomial Kernel</h2><p>ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬å¸Œæœ›å¯ä»¥æŠŠåŸæ¥é—®é¢˜æè¿°ä¸­çš„$X$æ¢ä¸º$Z$,è¡¨ç¤º$Z$æ˜¯$X$ç»è¿‡ç‰¹å¾è½¬æ¢ä¹‹åå¾—åˆ°çš„é«˜ç»´åº¦ç©ºé—´ï¼Œè€Œå‡è®¾$X$ç»´åº¦æ˜¯è¾ƒä½çš„ã€‚å› æ­¤ï¼Œç°åœ¨çš„é—®é¢˜æè¿°å¦‚ä¸‹ï¼š</p><p>$min_{\alpha} \frac 1 2 \sum_{n=1}^N \sum_{m=1}^N a_na_my_ny_mZ_n^TZ_m - \sum_{n=1}^N \alpha_n$</p><p> <strong>subject to</strong> $\sum_{n=1}^N y_n\alpha_n = 0;a_n \ge 0,n=1,â€¦,N$</p><p>ä¸Šæ¬¡æˆ‘ä»¬ä¹Ÿä»‹ç»äº†QçŸ©é˜µçš„è®¡ç®—ï¼Œå…¶ä¸­$q_{n,m} = y_ny_mZ_n^TZ_m$.è¿™å…¶ä¸­åŒ…å«äº†å¯¹$Z$å‘é‡çš„ä¹˜ç§¯ï¼Œå› æ­¤éšå«äº†å¾ˆå¤§çš„è®¡ç®—é‡ã€‚</p><p>å‡è®¾ï¼Œæˆ‘ä»¬å¯¹$X$åˆ°$Z$å‘é‡çš„è½¬æ¢è¡¨ç¤ºå¦‚ä¸‹ï¼š$Z = \phi(X)$,é‚£ä¹ˆä¸Šå¼ä¸­$Z_n^TZ_m = \phi(X_n)^T\phi(X_m)$.</p><p>æˆ‘ä»¬çŸ¥é“ï¼Œå¯¹äºå•å•$X_n^TX_m$çš„è®¡ç®—æ˜¯å®¹æ˜“å®Œæˆçš„ï¼Œé‚£ä¹ˆèƒ½ä¸èƒ½é€šè¿‡ä»€ä¹ˆåŠæ³•ç”¨ä¸Šé¢çš„è®¡ç®—æ¥ä»£æ›¿åŸæ¥çš„ç¡¬ç®—ï¼Ÿ</p><p>å‡è®¾å¦‚ä¸‹ï¼š$\phi(X) = {1,x_1,x_2,x_3â€¦x_d,x_1^2,x_1x_2,â€¦x_2x_1,x_2^2,â€¦,x_dx_1,â€¦x_d^2}$.</p><p>é‚£ä¹ˆ$\phi(X_n)^T\phi(X_m) = 1 + \sum_{i=1}^{d}x_i^nx_i^m + \sum_{i=1}^d\sum_{j=1}^d x_i^nx_j^nx_i^mx_j^m $</p><p>$\phi(X_n)^T \phi(X_m) = 1+X_n^TX_m + \sum_{i=1}^{d}x_i^nx_i^m  \sum_{j=1}^{d} x_j^n x_j^m = 1+X_n^TX_m + (X_n^TX_m)^2 $.</p><p>å¯ä»¥å‘ç°ï¼Œé€šè¿‡è¿™æ ·çš„å˜æ¢ï¼Œæˆ‘ä»¬å¾ˆè½»æ˜“åœ°è®¡ç®—å‡º$Z_n^TZ_m$.</p><p>åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç§°$k(X,Xâ€™) = 1+X^TXâ€™ + (X^TXâ€™)^2 $ä¸ºä¸€ç§æ ¸å‡½æ•°ã€‚å¦‚æœæˆ‘ä»¬å¯¹ç‰¹å¾è½¬æ¢å†è¿›è¡Œä¸€äº›å¤„ç†ï¼Œæ¯”å¦‚ï¼š$\phi(X) = {1,\sqrt 2 x_1,\sqrt 2 x_2,\sqrt 2 x_3â€¦\sqrt 2 x_d,x_1^2,x_1x_2,â€¦x_2x_1,x_2^2,â€¦,x_dx_1,â€¦x_d^2}$,</p><p>é‚£ä¹ˆæœ€åå¾—åˆ°çš„æ˜¯$k(x,xâ€™) = (1+X^TXâ€™)^2$ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥è½¬æ¢åˆ°æ›´é«˜ç»´çš„ç©ºé—´ï¼Œç»§ç»­æ¨å¹¿åˆ°æ›´ä¸€èˆ¬çš„ï¼š$K(x,xâ€™) = (\zeta + \xi x^Txâ€™)^d$. è¿™å°±æ˜¯å¾ˆæœ‰åçš„Polynomial Kernelã€‚</p><p>å½“ç„¶ï¼Œé€šè¿‡å¤šé¡¹å¼æ ¸å‡½æ•°ï¼Œæˆ‘ä»¬æ— æ³•å®ç°æ— é™ç»´åº¦çš„è½¬æ¢ã€‚</p><h2 id="Gaussian-Kernel-RBF-Kernel"><a href="#Gaussian-Kernel-RBF-Kernel" class="headerlink" title="Gaussian Kernel(RBF Kernel)"></a>Gaussian Kernel(RBF Kernel)</h2><p>å¯¹äºé«˜æ–¯Kernelçš„ä»‹ç»ï¼Œæˆ‘ä»¬å°è¯•ç”¨å¦ä¸€ç§åŠæ³•æ¥æ¨å¯¼ã€‚ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾ç»´åº¦åªæœ‰ä¸€ç»´ï¼Œå³$X = {x}$.</p><p>åœ¨è¿™é‡Œç›´æ¥ç»™å‡º$K(X,Xâ€™)$çš„å®šä¹‰å¦‚ä¸‹ï¼š$K(X,Xâ€™) = e^{-(x -xâ€™)^2}$.</p><p>ç„¶åæˆ‘ä»¬ä¸€æ­¥æ­¥æ¨å‘å‰æ¨å¯¼ï¼Œè¯´æ˜å®ƒå…¶å®æ˜¯æ— é™ç»´åº¦è½¬æ¢åçš„$X^TX$.</p><script type="math/tex; mode=display">\begin{align}K(X,X') &= e^{-(x - x')^2}\\&= e^{-x^2} e^{-(x')^2}e^{2xx'} \\&=Taylor=>e^{-x^2}e^{-(x')^2}(\sum _{i=0} ^ {\infty} \frac {(2xx')^2}{i!})\\&= \sum_{i=0}^{\infty} \frac {(\sqrt 2 x)^i}{\sqrt{i!}}e^{-x^2} \frac {(\sqrt 2 x')^i}{\sqrt{i!}} e^{-(x')^2}\end{align}</script><p>å› æ­¤ï¼Œè¿™ä¸ªè½¬æ¢å°±æ˜¯ $\phi(x) = exp(-x^2)(1,\sqrt{\frac 2 {1!}}X,\sqrt{\frac {2^2}{2!}}X^2,â€¦)$</p><p>å¯ä»¥è¯æ˜çš„æ˜¯ï¼Œä¸Šå‡åˆ°å¤šç»´åº¦ï¼ŒGaussian Kernelï¼š</p><p>$K(X,Xâ€™)$ = $e^{-\gamma ||X - Xâ€™||^2}$ with $\gamma &gt; 0$.</p><p>è¿™å°±æ˜¯é«˜æ–¯æ ¸å‡½æ•°ã€‚ä½†æ˜¯éœ€è¦æ³¨æ„çš„ä¸€ç‚¹ï¼Œé«˜æ–¯æ ¸å‡½æ•°æ”¾å¤§æ— é™ç»´åº¦ç©ºé—´ï¼Œæ‰€ä»¥å¦‚æœå‚æ•°$\gamma$ä¸å½“ï¼Œä»ç„¶æœ‰å¯èƒ½overfitting.å¦‚ä¸‹å›¾ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/TAE0Z%7D9NZWU7D%291%7EI8C1SLY.png" alt=""></p><h2 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h2><p>è¿˜æœ‰ä¸€ä¸ªæ ¸å‡½æ•°ï¼Œå«çº¿æ€§æ ¸å‡½æ•°ï¼š$K(x,xâ€™) = x^Txâ€™$.</p><p>è¿™ä¸ªæ ¸å‡½æ•°ï¼Œç®€å•ï¼Œä¹Ÿè¿…é€Ÿï¼Œä½†æ˜¯èƒ½åŠ›æœ‰é™ã€‚</p><p>å¤šé¡¹å¼æ ¸å‡½æ•°ï¼š$K(x,xâ€™) = (\zeta + \xi x^txâ€™)^d$.</p><p>ç›¸å¯¹äºçº¿æ€§æ ¸å‡½æ•°ï¼Œå®ƒçš„èƒ½åŠ›å¼ºäº†å¾ˆå¤šï¼Œä½†æ˜¯è°ƒå‚å¾ˆéš¾ï¼Œå› ä¸ºæœ‰3ä¸ªå‚æ•°ã€‚ç›¸åº”çš„å®ƒçš„é€Ÿåº¦æ²¡æœ‰çº¿æ€§é‚£ä¹ˆå¿«ã€‚è€Œä¸”å¦‚æœdå¾ˆå¤§,è¦ä¹ˆç»“æœå¾ˆæ¥è¿‘0ï¼Œè¦ä¹ˆå¾ˆå¤§ï¼Œä¸ä¼šå–å¾—å¾ˆå¥½çš„ç»“æœã€‚å› æ­¤ï¼Œå®ƒä¸€èˆ¬æ¥è¯´ï¼Œåªåœ¨dæ¯”è¾ƒå°çš„æ—¶å€™é€‚ç”¨ã€‚</p><p>é«˜æ–¯æ ¸å‡½æ•°ï¼š$K(X,Xâ€™)$ = $e^{-\gamma ||X - Xâ€™||^2}$</p><p>é«˜æ–¯æ ¸å‡½æ•°å¾ˆå¼ºå¤§ï¼Œè®¡ç®—é€Ÿåº¦æ¯”çº¿æ€§çš„ç•¥æ…¢ï¼Œä½†æ˜¯ä¹Ÿä¸å·®ã€‚ä½†æ˜¯å®ƒå¯èƒ½å¤ªè¿‡å¼ºå¤§äº†ï¼Œéœ€è¦æ…é‡é€‚ç”¨ï¼Œå› ä¸ºå¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆçš„æƒ…å†µã€‚ä½†æ˜¯æ€»ä½“æ¥è¯´ï¼Œä¸€èˆ¬æ¥è¯´é«˜æ–¯æ ¸å‡½æ•°æ˜¯æœ€å¸¸ç”¨çš„ã€‚</p><p>å½“ç„¶ï¼Œè¿˜æœ‰å¾ˆå¤šåˆ«çš„æ ¸å‡½æ•°ï¼Œåªéœ€è¦æ»¡è¶³Mercerå®šç†å³å¯ã€‚</p><blockquote><p>Mercerå®šç†ï¼š</p><p>å¦‚æœå‡½æ•°Kæ˜¯$\mathcal{R}^n \times \mathcal{R}^n-&gt;\mathcal{R}$ä¸Šçš„æ˜ å°„ï¼ˆä¹Ÿå°±æ˜¯ä»ä¸¤ä¸ªnç»´å‘é‡æ˜ å°„åˆ°å®æ•°åŸŸï¼‰ã€‚é‚£ä¹ˆå¦‚æœKæ˜¯ä¸€ä¸ªæœ‰æ•ˆæ ¸å‡½æ•°ï¼ˆä¹Ÿç§°ä¸ºMerceræ ¸å‡½æ•°ï¼‰ï¼Œé‚£ä¹ˆå½“ä¸”ä»…å½“å¯¹äºè®­ç»ƒæ ·ä¾‹${x_1,x_2,â€¦x_n}$ï¼Œå…¶ç›¸åº”çš„æ ¸å‡½æ•°çŸ©é˜µæ˜¯å¯¹ç§°åŠæ­£å®šçš„ã€‚</p></blockquote><p>æˆ‘ä»¬å¯ä»¥å‘ç°ï¼Œkernelçš„åŒºåˆ«å®é™…ä¸Šæ˜¯ç‰¹å¾è½¬æ¢çš„åŒºåˆ«ï¼Œåªä¸è¿‡æŸäº›ç‰¹å¾è½¬æ¢å¯ä»¥æ›´å®¹æ˜“åœ°è®¡ç®—QçŸ©é˜µã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”Dual Support Vector Machine</title>
      <link href="/2018/10/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Dual-Support-Vector-Machine/"/>
      <url>/2018/10/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Dual-Support-Vector-Machine/</url>
      
        <content type="html"><![CDATA[<p>ä¹‹å‰è¯´æ˜äº†linear SVMçš„ï¼Œä½†æ˜¯å®é™…ä¸Šä¾ç„¶è¿˜æœ‰ä¸€äº›é—®é¢˜ã€‚è™½ç„¶åœ¨ä¸€å®šç¨‹åº¦ä¸Šï¼Œlinear SVMä¼šå‡å°ç‰¹å¾è½¬æ¢å¸¦æ¥çš„å¤æ‚åº¦ï¼Œä½†æ˜¯å¦ä¸€æ–¹é¢ï¼Œå®ƒä¾ç„¶ä¾èµ–ç€d.<a id="more"></a>å¦‚æœdè¿‡å¤§ï¼Œå³ä½¿ä½¿ç”¨å¾ˆå¤šç°æœ‰çš„QPå·¥å…·ï¼Œä¾ç„¶å¾ˆéš¾å¾—åˆ°ç»“æœã€‚å¦‚ä½•å¤„ç†æ•°æ®ç»´åº¦å¾ˆå¤§ï¼Œç”šè‡³æ˜¯æ— ç©·ç»´çš„æƒ…å†µï¼Ÿè¿™æ˜¯æˆ‘ä»¬æƒ³è¦è§£å†³çš„é—®é¢˜ã€‚</p><p>ä½†æ˜¯è¦æ³¨æ„çš„äº‹ï¼Œå®é™…ä¸Šçš„æ•°å­¦æ¨å¯¼éå¸¸å¤æ‚ï¼Œå› æ­¤åœ¨è¿™é‡Œæˆ‘åªä¼šåšç®€å•çš„æ¨å¯¼ï¼Œæ¥æ…¢æ…¢è¾¾åˆ°è‡ªå·±çš„ç›®æ ‡ã€‚</p><p>é¦–å…ˆæˆ‘ä»¬æ‹¿å‡ºæ¥ä¸Šæ¬¡è®¨è®ºåˆ°æœ€åçš„æˆå‹çš„é—®é¢˜ï¼š</p><p><strong>$\min$</strong>  $\frac 1 2 W^TW$</p><p>$s.t.  y_n(W^TX_n+b) \ge 1,n =1,2,â€¦,N $.</p><p>æˆ‘ä»¬å¯ä»¥æƒ³åˆ°çš„æ˜¯åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°ï¼Œç±»ä¼¼äºä¹‹å‰çš„æ­£åˆ™åŒ–ï¼Œæ¥æ„é€ ä¸€ä¸ªå‡½æ•°$\zeta(W,b,\alpha)$,å®šä¹‰å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\zeta(W,b,\alpha) = \frac 1 2 W^TW + \sum _{n = 1} ^{N} \alpha_n (1 - y_n(W^TX_n+b))</script><p>æˆ‘ä»¬è¦åšçš„SVMæ˜¯ï¼š$\min_{W,b}(\max_{ \alpha_i \ge 0,i=1,â€¦,n} \zeta (W,b,\alpha) )$,å¾ˆç¥å¥‡çš„ï¼Œæˆ‘ä»¬éœ€è¦çš„é‚£äº›çº¦æŸéƒ½èå…¥åˆ°ä¸€ä¸ªå¼å­å½“ä¸­äº†ã€‚åœ¨è¿™é‡Œï¼Œå¸Œæœ›ç®€å•å¯ä»¥è¯´æ˜ä¸€ä¸‹ï¼Œå®é™…ä¸Šæˆ‘ä»¬ä¸Šé¢çš„SVMä¸åŸæ¥çš„æ•ˆæœæ˜¯ä¸€æ ·çš„ã€‚</p><p>é¦–å…ˆï¼Œå¦‚æœåŸæ¥çš„çº¦æŸä¸æ»¡è¶³ï¼Œåˆ™ï¼š$y_n(W^TX_n+b) <1$ï¼Œé‚£ä¹ˆ$(1-y_n(w^tx_n+b))>0$ï¼Œè€Œè¦æœ€å¤§åŒ–$\zeta(W,b,\alpha)$ï¼Œ$\alpha$åˆå¤§äºç­‰äº0ï¼Œé‚£ä¹ˆå¯ä»¥è‚¯å®šçš„æ˜¯$\sum _{n = 0} ^{N} \alpha_n (1 - y_n(W^TX_n+b)) $æœ€åçš„ç»“æœæ˜¯æ— ç©·å¤§äº†ï¼Œå®ƒä¸€å®šä¸ä¼šè¢«é€‰ä¸Šï¼›</1$ï¼Œé‚£ä¹ˆ$(1-y_n(w^tx_n+b))></p><p>å¦‚æœåŸæ¥çš„çº¦æŸæ»¡è¶³çš„è¯ï¼Œ$(1 - y_n(W^TX_n+b)) \leq 0$,å› ä¸ºå®ƒå°äº0ï¼Œè¦æœ€å¤§åŒ–$\zeta(W,b,\alpha)$ï¼Œåªèƒ½ä½¿å¾—$\sum _{n = 0} ^{N} \alpha_n (1 - y_n(W^TX_n+b)) $ç­‰äº0ï¼Œä¹Ÿå°±æ˜¯æœ€åå¾—åˆ°çš„ç»“æœæ˜¯$\zeta(W,b,\alpha) = \frac 1 2 W^TW$ï¼Œå› æ­¤å®é™…ä¸Šæœ€ç»ˆæ±‚çš„çš„æœ€å¤§å€¼ï¼Œä¾ç„¶æ˜¯æ»¡è¶³æ¡ä»¶çš„ã€‚</p><p>é€šè¿‡è¿™æ ·å°±å¾ˆå·§å¦™åœ°å°†æ¡ä»¶ä¸æˆ‘ä»¬æƒ³è¦åšçš„ä¼˜åŒ–é—®é¢˜èåˆæˆäº†ä¸€ä¸ªå¼å­ã€‚</p><p>è€Œä¸”æˆ‘ä»¬å¾ˆå®¹æ˜“çŸ¥é“çš„äº‹ï¼š$\min_{W,b}(\max_{\alpha_i \ge 0,i=1,â€¦,n} \zeta (W,b,\alpha) ) \ge \min_{W,b} \zeta (W,b,\alpha â€˜)$,ä¸Šå¼ä¸­$\alphaâ€™$æ˜¯ä¸ªå®šå€¼ï¼Œ<br>ä¹Ÿå°±å¯ä»¥æ¨æ–­å‡ºæ¥ï¼š$\min_{W,b}(\max_{\alpha_i \ge 0,i=1,â€¦,n} \zeta (W,b,\alpha) ) \ge \max_{\alpha_i \ge 0,i=1,â€¦,n}( \min_{W,b} \zeta (W,b,\alpha â€˜))$.</p><p>æ›´ä»¤äººå…´å¥‹çš„æ˜¯ï¼Œåœ¨è¿™äº›æ¡ä»¶ä¸‹ï¼š</p><p>1.convex primal</p><p>2.feasible primalï¼ˆtrue if separableï¼‰</p><p>3.linear constraints</p><p>ä¸Šå¼çš„ç­‰å·æ˜¯æˆç«‹çš„ã€‚</p><p>å› æ­¤æˆ‘ä»¬åªéœ€è¦è§£å†³å³è¾¹çš„éƒ¨åˆ†å°±å¥½äº†ã€‚è¿™å°±æ˜¯Lagrange Dualityï¼Œæ‹‰æ ¼æœ—æ—¥å¯¹å¶ã€‚ï¼ˆä¸ºä½•ä¸è§£å·¦è¾¹ï¼Ÿemmmï¼Œ$\alpha$ æ˜¯ä¸€ä¸ªå‘é‡ï¼ŒNç»´çš„ï¼Œä¸€èˆ¬æ¥è¯´N&gt;&gt;d+1ï¼‰</p><p>å—¯ï¼Œä½†æ˜¯ä¼¼ä¹è¿™ä¸ªå¼å­ï¼Œè¿˜æ˜¯å¾ˆå¤æ‚ï¼Œå…¨éƒ¨å†™å‡ºæ¥çœ‹ä¸€ä¸‹ï¼š</p><script type="math/tex; mode=display">\zeta(W,b,\alpha) = \frac 1 2 W^TW + \sum _{n = 1} ^{N} \alpha_n (1 - y_n(W^TX_n+b))</script><p>é¦–å…ˆï¼Œè¦åœ¨æŠŠ$\alpha$çœ‹ä½œå®šå€¼çš„æƒ…å†µä¸‹æ‰¾åˆ°æœ€å°å€¼ï¼Œé‚£ä¹ˆæˆ‘ä»¬çŸ¥é“å®ƒä¸€å®šæ»¡è¶³çš„æ¡ä»¶ï¼š</p><script type="math/tex; mode=display">\frac {\partial \zeta}{\partial b} = \sum _{n=1}\alpha_n y_n = 0</script><p>å› æ­¤ï¼Œä¸Šé¢çš„å¼å­å˜æˆäº†ï¼š</p><script type="math/tex; mode=display">\zeta(W,b,\alpha) = \frac 1 2 W^TW + \sum _{n = 1} ^{N} \alpha_n (1 - y_nW^TX_n)</script><p>ç®€åŒ–äº†å¾ˆå¤§ä¸€éƒ¨åˆ†ã€‚ç„¶åæ±‚$W$çš„åå¯¼ï¼š</p><script type="math/tex; mode=display">\frac {\partial \zeta}{\partial W} = W - \sum_{n=1}^N \alpha_n y_nX_n = 0</script><p>æˆ‘ä»¬å¯ä»¥å¾—åˆ°$  \sum_{n=1}^N \alpha_n y_nX_n = W$,å› æ­¤æœ€åå¼å­ç®€åŒ–ä¸ºï¼š</p><script type="math/tex; mode=display">\zeta(W,b,\alpha) =   \sum _{n = 1} ^{N} \alpha_n - \frac 1 2 W^TW.</script><p>å¼å­åˆç®€å•äº†å¾ˆå¤šã€‚åŒæ—¶æˆ‘ä»¬å†ç»§ç»­å°†$W$æ›¿æ¢:</p><script type="math/tex; mode=display">\zeta(W,b,\alpha) =   \sum _{n = 1} ^{N} \alpha_n - \frac 1 2 ||\sum_{n=1}^N \alpha_n y_nX_n||^2.</script><p>è€Œä¸”ä¸è¦å¿˜äº†æˆ‘ä»¬æœ€ä¹‹å‰æ¨å¯¼çš„ï¼š $\alpha_n (1 - y_n(W^TX_n+b)) = 0$.</p><p>å› æ­¤ï¼Œç°åœ¨çš„å¼å­é‡Œé¢å·²ç»æ²¡æœ‰$W$ä¸$b$äº†ï¼Œæˆ‘ä»¬è¦åšçš„å°±æ˜¯</p><p>$\max_{\alpha_i \ge 0,i=1,â€¦,n,\sum y_n \alpha_n = 0,W =\sum \alpha_n y_nX_n } - \frac 1 2 ||\sum_{n=1}^N \alpha_n y_nX_n||^2 +  \sum _{n = 1} ^{N} \alpha_n$.</p><p>æ€»ç»“ä¸€ä¸‹ï¼Œè¦è§£å†³å¯¹å¶é—®é¢˜å¾—åˆ°ä¸Šé¢çš„ç»“æœï¼Œéœ€è¦è¾¾åˆ°çš„æ¡ä»¶ï¼š</p><p>1.primal feasibleï¼š $y_n(W^TX_n+b) \ge 1$</p><p>2.dual feasible: $a_n \ge 0$</p><p>3.dual-inner optimal:$\sum y_n \alpha_n = 0;W =\sum \alpha_n y_nX_n$</p><p>4.primal-inner optimal: $\alpha_n (1 - y_n(W^TX_n+b)) = 0$.</p><p>ä¸Šé¢çš„è¿™äº›æ¡ä»¶ï¼Œè¢«ç§°ä¸ºKKTï¼ˆKarush-Kuhn-Tuckerï¼‰æ¡ä»¶ï¼Œå¯¹äºä¼˜åŒ–é—®é¢˜æ˜¯éå¸¸å¿…è¦çš„ã€‚å“‡ï¼Œä¹‹å‰å¬è¿‡çš„é«˜å¤§ä¸Šçš„åè¯é€æ¸æ‹¨å¼€äº‘é›¾è§é’å¤©äº†ã€‚</p><p>æˆ‘ä»¬å°†ä¸Šé¢çš„å¼å­ç»§ç»­å±•å¼€ï¼š</p><script type="math/tex; mode=display">-\frac 1 2 \sum_{n=1}^N \sum_{m=1}^N a_na_my_ny_mX_n^TX_m + \sum_{n=1}^N \alpha_n.</script><p>æ¥ä¸‹æ¥æˆ‘ä»¬å¼€å§‹å°è¯•æœ€å¤§åŒ–ä¸Šé¢çš„è¿™ä¸ªå¼å­,é¦–å…ˆä¾ç„¶æˆ‘ä»¬æŠŠæœ€å¤§åŒ–é—®é¢˜è½¬åŒ–æˆä¸ºæœ€å°åŒ–é—®é¢˜ï¼Œç”¨æ•°å­¦è¯­è¨€æè¿°ï¼š</p><p>$\min_{\alpha} \frac 1 2 \sum_{n=1}^N \sum_{m=1}^N a_na_my_ny_mX_n^TX_m - \sum_{n=1}^N \alpha_n$</p><p> <strong>subject to</strong> $\sum_{n=1}^N y_n\alpha_n = 0;a_n \ge 0,n=1,â€¦,N$</p><p> å› ä¸ºå¼å­ä¸­æ²¡æœ‰$W$,æˆ‘ä»¬æš‚æ—¶å°†çº¦æŸä¸­çš„$W$å»æ‰ï¼Œä¸“æ³¨è¿™ä¸ªé—®é¢˜ï¼Œæœ€åå†å°è¯•è®¡ç®—å‡º$W$.</p><p> è€Œè¿™ä¸ªå¦‚æœä»”ç»†è§‚å¯Ÿï¼Œæˆ‘ä»¬ä¼šå‘ç°å®ƒæ˜¯ä¸€ä¸ªQPé—®é¢˜ã€‚ä¹Ÿå°±æ˜¯é€šè¿‡ç°æˆçš„å·¥å…·ï¼Œå¯ä»¥è®¡ç®—å‡ºæœ€ä½³çš„$\alpha$.</p><p> è®¡ç®—å‡ºæœ€ä½³çš„$\alpha$ï¼Œå¯ä»¥å¾ˆè½»æ˜“åœ°è®¡ç®—å‡º$W$ï¼Œè€Œä¸”é€šè¿‡çº¦æŸä¹Ÿèƒ½è½»æ˜“åœ°è®¡ç®—å‡º$b$.è€Œä¸”æˆ‘ä»¬å¯ä»¥é€šè¿‡çº¦æŸå‘ç°ï¼Œå…¶å®ç›¸å½“ä¸€å¤§éƒ¨åˆ†$\alpha_n=0$ï¼Œè€Œ$\alpha \ne 0$çš„é‚£äº›ç‚¹ï¼Œä¹Ÿæ­£æ˜¯æˆ‘ä»¬çš„æ”¯æ’‘å‘é‡ã€‚</p><p> æœ€åï¼Œæˆ‘ä»¬æå‡ºä¸€ä¸ªç–‘é—®ï¼šè¿™ä¸ªè®¡ç®—æ–¹æ³•ï¼ŒçœŸçš„å’Œç»´åº¦æ²¡å…³ç³»äº†å—ï¼Ÿææ€•ä¸æ˜¯ï¼Œç»´åº¦éšå«åœ¨äº†è®¡ç®—$Q$çŸ©é˜µå½“ä¸­äº†.è¿™è¿˜æ˜¯æ²¡æœ‰è¾¾åˆ°æˆ‘ä»¬çš„ç›®çš„ã€‚è¿™éœ€è¦ä¸‹ä¸€ä¸ªæ”¹è¿›ï¼škernelã€‚</p><h2 id="p-s-QPé—®é¢˜çš„è§£å†³"><a href="#p-s-QPé—®é¢˜çš„è§£å†³" class="headerlink" title="p.s. QPé—®é¢˜çš„è§£å†³"></a>p.s. QPé—®é¢˜çš„è§£å†³</h2><p> ä¸€èˆ¬æ¥è¯´ï¼Œè§£å†³QPé—®é¢˜çš„å·¥å…·ï¼Œéœ€è¦æä¾›ä¸‹é¢å‡ ä¸ªå‚æ•°ï¼š</p><p>optimal $\alpha$ $\leftarrow$ $QP(Q,\mathbf{p},A,\mathbf{c})$</p><p> $\min_{\alpha}$  $\frac 1 2 \alpha ^T Q \alpha + p^T\alpha$</p><p>subject to $a_i^T \alpha \ge C_i$, for i = 1,2,â€¦</p><p>å› æ­¤ï¼Œåœ¨æœ¬ä¾‹ä¸­ï¼ŒQï¼š<br>$q_{n,m} = y_ny_mX^nX^m;$</p><p>$\mathbf{p} = -1_N;$</p><p>$a_1 = Y ,a_2 = -Y;a_3 = 1_N$</p><p>$c_1 = 0,c_2 = 0,c_3 = 0;$</p><p>å½“ç„¶ï¼Œå…·ä½“çš„å‚æ•°ç±»å‹è¿˜è¦çœ‹å…·ä½“çš„å·¥å…·åŒ…ï¼Œä½†æ˜¯æ‰€éœ€å‚æ•°éƒ½ä¸éš¾ä»å·²çŸ¥çš„æ¡ä»¶è½¬æ¢å¾—åˆ°ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”Linear Support Vector Machine</title>
      <link href="/2018/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Linear-Support-Vector-Machine/"/>
      <url>/2018/10/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Linear-Support-Vector-Machine/</url>
      
        <content type="html"><![CDATA[<p>è¿™ä¸ªåå­—çœŸæ˜¯å¾ˆå¥‡æ€ªã€‚æƒ³è¦äº†è§£ä¸ºä½•å«è¿™ä¹ˆå¥‡æ€ªçš„åå­—ï¼Œå°±è¦æ·±å…¥äº†è§£è¿™ä¸ªä¸œè¥¿ã€‚<br><a id="more"></a><br>é¦–å…ˆéœ€è¦å›é¡¾çš„æ˜¯ä¹‹å‰çš„Perceptron Learning Algorithmã€‚å¦‚æœè¿™ä¸ªèµ„æ–™çº¿æ€§å¯åˆ†ï¼Œä½¿ç”¨PLAç®—æ³•ï¼Œä¸€å®šå¯ä»¥æ‰¾åˆ°ä¸€ä¸ªå¾ˆå¥½çš„çº¿æˆ–è€…è¶…å¹³é¢ï¼ˆhyperplaneï¼‰æ¥å°†è¿™ä¸ªèµ„æ–™åˆ†å¼€ï¼Œä½†æ˜¯è¿™ä¸ªçº¿æˆ–è€…æ˜¯è¶…å¹³é¢çš„ä¸ªæ•°å¯èƒ½æ˜¯æ— æ•°ä¸ªï¼Œå®ƒä»¬æ˜¯å¦æ˜¯ä¸€æ ·å¥½çš„ï¼Ÿå¦‚ä¸‹å›¾ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/JRN%5BZGK%29K_J5YLHU4I3EBGF.png" alt=""></p><p>ä¸Šé¢3æ¡çº¿ï¼Œå¯¹äºPLAç®—æ³•æ¥è¯´æ˜¯ä¸€æ ·å¥½çš„ï¼Œå› æ­¤è¿è¡Œåˆ°å“ªä¸€æ¡ï¼Œéƒ½æ˜¯æ— æ³•é¢„æµ‹çš„ã€‚ä½†æ˜¯ä»æˆ‘ä»¬çš„è§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬ä¼šé€‰æ‹©ç¬¬ä¸‰æ¡ï¼Œå› ä¸ºè¿™æ¡çº¿æ›´robustï¼Œå¯ä»¥å®¹å¿æ›´å¤šæµ‹é‡è¯¯å·®ï¼Œå¦‚ç¬¬ä¸€æ¡ï¼Œæœ‰ä¸€ä¸ªç¦»çº¢è‰²ç‚¹å¾ˆè¿‘çš„æ ·æœ¬çš„è¯ï¼Œå®ƒæ›´å¤§æ¦‚ç‡æ˜¯negativeçš„ï¼Œä½†æ˜¯ç¬¬ä¸€æ¡å°±ä¼šå°†å®ƒå½’ç±»åˆ°positiveã€‚å› æ­¤ï¼Œé€‰æ‹©ç¬¬ä¸‰æ¡çº¿ï¼Œå¯ä»¥æ›´å¥½åœ°é¿å…overfittingã€‚</p><p>ä¸ºäº†æƒ³è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬éœ€è¦å°†é—®é¢˜æç‚¼æˆæ•°å­¦è¯­è¨€ã€‚é¦–å…ˆæˆ‘ä»¬æƒ³è¦æ±‚çš„æ˜¯ç°è‰²åŒºåŸŸæœ€å¤§çš„ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/9Y%60%60IZO%7BW%285P%28RI%24KPP14LP.png" alt=""></p><p>æˆ‘ä»¬ç§°ç°è‰²åŒºåŸŸä¸ºmarginã€‚è€Œè¿™ä¸ªmarginï¼Œå®é™…ä¸Šæ˜¯æœ€è¿‘çš„ä¸¤ä¸ªç‚¹åˆ°ä¸­é—´è¿™æ¡çº¿çš„è·ç¦»ã€‚</p><p>é‚£ä¹ˆæˆ‘ä»¬å°±è¦å…ˆæƒ³è±¡ï¼Œç‚¹åˆ°å¹³é¢ï¼ˆæˆ–è€…è¶…å¹³é¢ï¼‰çš„è·ç¦»å¦‚ä½•è®¡ç®—ï¼Ÿ</p><p>å¦‚æœä¸€ä¸ªè¶…å¹³é¢çš„æ–¹ç¨‹ä¸º$W^TX + b = 0$,åˆ™ä»»æ„ä¸¤ä¸ªåœ¨è¯¥å¹³é¢çš„ç‚¹$xâ€™$ä¸$xâ€™â€™$éƒ½åº”è¯¥æ»¡è¶³ä¸Šå¼ï¼Œä¹Ÿå°±æ˜¯</p><script type="math/tex; mode=display">\left\{\begin{array} W^TX'+b = 0\\W^TX''+b = 0\end{array}\right</script><p>å› æ­¤å¯ä»¥å¾—åˆ°ï¼š$W^T(Xâ€™-Xâ€™â€™) = 0$.<br>è€Œ$(Xâ€™-Xâ€™â€™)$å¯ä»¥è¡¨ç¤ºå¹³é¢ä¸Šçš„ä»»ä½•ä¸€ä¸ªå‘é‡ï¼Œè¿™è¯´æ˜äº†ï¼Œ$W$æ˜¯è¯¥å¹³é¢çš„æ³•å‘é‡ã€‚</p><p>è€Œä¸€ä¸ªç‚¹åˆ°å¹³é¢çš„è·ç¦»ï¼Œå®é™…ä¸Šå°±æ˜¯è¯¥ç‚¹åˆ°ä»»ä½•å¹³é¢ä¸Šä¸€ç‚¹è¿æ¥å¾—åˆ°çš„å‘é‡å¯¹è¯¥å¹³é¢æ³•å‘é‡çš„ä¸€ä¸ªæŠ•å½±ã€‚è®¡ç®—æŠ•å½±é•¿åº¦çš„åŠæ³•å…¶å®å¾ˆç®€å•,é¦–å…ˆæˆ‘ä»¬æœ‰$ab = |a||b|cos\theta$,å› æ­¤åªè¦è§„å®šaçš„é•¿åº¦ä¸º1ï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªå‘é‡çš„æ•°é‡ç§¯çš„ç»å¯¹å€¼å°±æ˜¯å‘é‡çš„é•¿åº¦ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">d = |\frac {W^T }{||W||}(X - X')| = |\frac 1 {||W||}(W^TX + b)|.</script><p>è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†ä¸€ä¸ªç‚¹åˆ°ä¸€ä¸ªè¶…å¹³é¢çš„è·ç¦»ã€‚</p><p>å®é™…ä¸Šä¸€ä¸ªè¶…å¹³é¢çš„è¡¨ç¤ºæ–¹æ³•æ˜¯æ— ç©·çš„ï¼Œæ¯”å¦‚$WTX+b = 0$ä¸$2W^TX+2b= 0$æ˜¯ä¸€ä¸ªå¹³é¢ï¼Œå¦‚æœæˆ‘ä»¬å°†ç»è¿‡è·ç¦»è¶…å¹³é¢æœ€è¿‘çš„ç‚¹çš„ä¸è¶…å¹³é¢å¹³è¡Œçš„å¹³é¢è¡¨ç¤ºä¸º:$W^X+b = Â±1$ï¼Œé‚£ä¹ˆdçš„å½¢å¼å°±æ›´ç®€å•äº†ï¼š$d = \frac 1 {||W||}$.</p><p>ä¸Šé¢çš„è·ç¦»ä¸­è¿˜æ˜¯åŠ äº†ç»å¯¹å€¼ï¼Œä½†æ˜¯å› ä¸ºè¿™ä¸ªé—®é¢˜çš„å‰ææ˜¯å°†æ‰€æœ‰çš„ç‚¹éƒ½åˆ†ç±»æ­£ç¡®ï¼Œå› æ­¤$y_i(WX_i+b)\ge 0$.</p><p>æ‰€ä»¥ç”¨æ•°å­¦è¯­è¨€æè¿°æˆ‘ä»¬çš„é—®é¢˜å¦‚ä¸‹ï¼š</p><p>max $\frac 1 {||W||}$</p><p>$s.t. min_{n = 1,â€¦,N} y_n(W^TX_n+b) = 1$.</p><p>æ³¨æ„çš„æ˜¯ä¸ºä»€ä¹ˆæœ€å°çš„ç‚¹$y_n(W^TX_n+b) = 1$,å› ä¸ºè·ç¦»è¾ƒè¿œçš„è¯ï¼Œæ ¹æ®è·ç¦»å…¬å¼$(W^TX+b)$ä¼šæ›´å¤§ã€‚</p><p>ä¸Šé¢çš„é—®é¢˜ä¾ç„¶æ˜¯å¾ˆéš¾è§£å†³çš„ï¼Œæˆ‘ä»¬å¸Œæœ›å¯ä»¥ç»§ç»­æ”¾æ¾è¿™ä¸ªçº¦æŸã€‚å¦‚æœæ˜¯æ‰€æœ‰ç‚¹$y_n(W^TX_n+b) \ge 1$å¦‚ä½•å‘¢ï¼Ÿ</p><p>è¿™é‡Œåˆ©ç”¨åè¯æ³•è¯æ˜ï¼Œæ”¾å®½åˆ°ä¸Šé¢çš„çº¦æŸä¾ç„¶æ²¡æœ‰é—®é¢˜ï¼Œè·ç¦»ç›´çº¿æœ€è¿‘çš„ç‚¹ä¾ç„¶æ˜¯æ»¡è¶³$y_n(W^TX_n+b) = 1$ï¼š</p><p>å¦‚æœæˆ‘ä»¬æ‰¾åˆ°æœ€è¿‘çš„ç‚¹$X_n$ï¼Œå®ƒæ»¡è¶³çš„æ˜¯$y_n(W^TX_n+b) =a (a&gt;1)$ï¼Œè€Œä¸”å¾—åˆ°äº†æœ€å¤§çš„$\frac 1 {||W||}$ï¼Œé‚£ä¹ˆå¯¹ä¸Šå¼å·¦å³åŒæ—¶é™¤ä»¥a,è€Œ$\frac W a$æ¯”$W$æ›´å°ï¼Œä¹Ÿå°±æ˜¯è¿™ä¸ª$\frac 1 {||W||}$å¹¶ä¸æ˜¯æœ€å¤§çš„ã€‚è¿™å°±çŸ›ç›¾äº†ã€‚å› æ­¤ä¾ç„¶åªæœ‰åœ¨$y_n(W^TX_n+b) =1$çš„æ—¶å€™æ‰èƒ½å–å¾—æœ€å¤§å€¼ã€‚æ‰€ä»¥æ”¾å¤§è¿™ä¸ªçº¦æŸï¼Œæˆ‘ä»¬ä¾ç„¶å¯ä»¥å¾—åˆ°ä¸€æ ·çš„æœ€ç»ˆç»“æœã€‚</p><p>ä¹‹å‰æˆ‘ä»¬ä¸€ç›´åœ¨æ±‚å¾—æ˜¯æœ€å°å€¼ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨è¿™é‡Œä¹Ÿå¯ä»¥è½¬æ¢æˆä¸ºæ±‚æœ€å°å€¼ï¼ŒåŒæ—¶èŒƒæ•°æ˜¯éœ€è¦æ ¹å·çš„ï¼Œè€Œå› ä¸ºèŒƒæ•°å’ŒèŒƒæ•°çš„å¹³æ–¹æ˜¯å•è°ƒé€’å¢çš„ï¼Œå› æ­¤è½¬åŒ–ä¸ºèŒƒæ•°çš„å¹³æ–¹ä¸ä¼šå½±å“ç»“æœï¼ŒåŒæ—¶å†æ·»ä¸Šä¸€ä¸ª$\frac 1 2$ï¼Œä¸ºäº†ä»¥åè®¡ç®—çš„æ–¹ä¾¿ã€‚</p><p>å› æ­¤ï¼Œæœ€ç»ˆçš„ç”¨æ•°å­¦è¯­è¨€æè¿°æˆ‘ä»¬çš„é—®é¢˜çš„ç‰ˆæœ¬å¦‚ä¸‹ï¼š</p><p>min  $\frac 1 2W^TW$</p><p>$s.t.  y_n(W^TX_n+b) \ge 1,n =1,2,â€¦,N $.</p><p>è¿™ä¸ªé—®é¢˜å®é™…ä¸Šæ˜¯ä¸€ä¸ªQPï¼ˆäºŒæ¬¡è§„åˆ’ï¼‰é—®é¢˜ã€‚è€ŒäºŒæ¬¡è§„åˆ’é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥å€ŸåŠ©å¾ˆå¤šå·¥å…·ï¼Œæä¾›å¿…è¦çš„å‚æ•°ï¼Œæ±‚å¾—æœ€ä½³è§£ã€‚</p><p>ä¸Šæœ¯é—®é¢˜å°±æ˜¯svmé—®é¢˜ã€‚ä¸ºä»€ä¹ˆå«support vector machineï¼Ÿæˆ‘ä»¬éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå®é™…ä¸Šå†³å®šæœ€ç»ˆçº¿çš„ï¼Œåªæœ‰å¯èƒ½æ˜¯æœ€è¾¹ä¸Šçš„ç‚¹ï¼Œè€Œå†³å®šæœ€ç»ˆç»“æœçš„ç‚¹ï¼Œå°±å«åšæ”¯æŒå‘é‡ã€‚</p><p>å½“æˆ‘ä»¬é¢å¯¹çº¿æ€§æ— æ³•å¯åˆ†çš„æƒ…å†µï¼Œå°±éœ€è¦ä½¿ç”¨ä¹‹å‰ä»‹ç»çš„ç‰¹å¾è½¬æ¢ï¼ˆnonlinear transformï¼‰ï¼Œå°†å½“å‰çš„ç‚¹è½¬æ¢åˆ°æ›´é«˜ç»´åº¦çš„ç©ºé—´ä¸­å»ï¼Œä½¿å…¶æˆä¸ºçº¿æ€§å¯åˆ†ã€‚</p><p>æœ€åï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œæƒ³è¦ç®€å•è¯´æ˜ä¸€ä¸‹è¿™èƒŒåçš„ç†è®ºåŸºç¡€ï¼Œä¸ºä»€ä¹ˆå¯»æ‰¾æœ€ç²—çš„é‚£æ¡çº¿ï¼Œå¯ä»¥è·å¾—æ›´å¥½çš„robustnessï¼Ÿ</p><p>è¿™å°±åˆå›åˆ°äº†vc dimension.å‡è®¾æˆ‘ä»¬æ‰¾åˆ°çš„æ¯”è¾ƒç²—çš„çº¿ï¼ˆmarginè¾ƒå¤§ï¼‰ï¼Œæ˜¯æˆ‘ä»¬è¦çš„æ ‡å‡†ã€‚é‚£ä¹ˆå…·æœ‰è¿™ä¹ˆå¤§marginçš„çº¿ï¼Œèƒ½å°†ç©ºé—´ä¸­çš„Nä¸ªæ ·æœ¬åˆ†æˆçš„dichotomyçš„ä¸ªæ•°å°±ä¼šå°‘å¾ˆå¤šï¼Œä¹Ÿå°±æ˜¯æœ‰æ•ˆvc dimensionä¼šå˜ä½ã€‚å½“ç„¶ï¼Œè¿™ä¸ªé—®é¢˜æ— æ³•åƒPLAæ—¶å€™é‚£æ ·åˆ†æï¼Œå› ä¸ºå…·ä½“èƒ½åˆ†å¤šå°‘ï¼Œä¸æ ·æœ¬ä¹‹é—´çš„è·ç¦»ä¹Ÿå¾ˆé‡è¦ï¼Œå…·ä½“æ ·æœ¬å¾—åˆ°çš„ç»“æœä¹Ÿä¸åŒï¼Œä½†æ˜¯å¯ä»¥è¯æ˜çš„æ˜¯,å¦‚æœè¿™äº›æ ·æœ¬åœ¨ä¸€ä¸ªåŠå¾„ä¸ºRçš„åœ†å†…ï¼Œmarginé•¿åº¦ä¸ºÏï¼š</p><script type="math/tex; mode=display">d_{vc}(\mathcal{A}_{\rho}) \leq min(\frac {R^2}{Ï^2} ,d)+1 \leq d+1</script><p>ä¹‹å‰ä»‹ç»çš„feature transformæœ‰ä¸ªé—®é¢˜æ˜¯å¤ªè¿‡äºå¤æ‚å¯¼è‡´vc dimensionä¼šå˜å¾—å¾ˆå¤§ï¼Œè€Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºæ¥é€šè¿‡SVMæˆ‘ä»¬æŸç§ç¨‹åº¦ä¸Šå¯ä»¥å¤„ç†å¥½è¿™ç§æƒ…å†µã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æ•°å­¦â€”â€”Lagrange Multiplier</title>
      <link href="/2018/10/09/%E6%95%B0%E5%AD%A6%E2%80%94%E2%80%94Lagrange-Multiplier/"/>
      <url>/2018/10/09/%E6%95%B0%E5%AD%A6%E2%80%94%E2%80%94Lagrange-Multiplier/</url>
      
        <content type="html"><![CDATA[<p>æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•ï¼Œæ˜¯æˆ‘ä»¬å¤§å­¦æˆ–è€…è€ƒç ”è¿‡æ¥çš„è€³ç†Ÿèƒ½è¯¦çš„åè¯äº†ã€‚æˆ‘ä»¬æ¥è§¦ä»–çš„æ—¶å€™ï¼Œåº”è¯¥æ˜¯åœ¨æ±‚æ¡ä»¶æå€¼çš„æ—¶å€™ã€‚<br><a id="more"></a><br>æ±‚$f(x,y)$åœ¨$g(x,y)=0$çš„æ¡ä»¶ä¸‹çš„æå€¼ã€‚éœ€è¦åˆ©ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ„é€ æ–°çš„å¼å­ï¼š</p><p>$w(x,y,\lambda ) = f(x,y)+\lambda g(x,y)$</p><p>è®©$w(x,y,\lambda)$åˆ†åˆ«å¯¹$x,y,\lambda$æ±‚åå¯¼ï¼Œå¹¶ä»¤å…¶ä¸º0ï¼š</p><script type="math/tex; mode=display">\left \{\begin {array }    w'_x(x,y,\lambda) = f'_x(x,y)+\lambda g'_x(x,y) = 0\\    w'_y(x,y,\lambda) = f'_y(x,y)+ \lambda g'_y(x,y) = 0\\    w'_{\lambda}(x,y,\lambda) = g(x,y) = 0\end{array}\right</script><p>ä¸è¿‡å¤§å­¦çš„æ—¶å€™ï¼Œæˆ‘è™½ç„¶ä¼šè¿™ä¹ˆè®¡ç®—ï¼Œä½†æ˜¯å´ä¸çŸ¥é“åŸç†ã€‚ç°åœ¨å¸Œæœ›ä»åŸç†è§£é‡Šä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆè¦è¿™ä¹ˆç®—ã€‚</p><p>æš‚æ—¶æˆ‘æƒ³åˆ°äº†ä¸¤ç§è§£é‡ŠåŠæ³•ï¼š</p><p>1.æˆ‘ä»¬é¦–å…ˆè¦çŸ¥é“çš„ä¸€ä¸ªå‰ææ˜¯ï¼Œ$g(x,y) = 0$åœ¨ä»»æ„ä¸€ç‚¹$(x_0,y_0)$åˆ‡çº¿çš„æ³•å‘é‡ä¸º$(gâ€™_x(x_0,y_0),gâ€™_y(x_0,y_0))$.æƒ³è±¡ç°åœ¨æœ‰ä¸€ä¸ªç‚¹åœ¨çº¦æŸçš„è¿™æ¡çº¿ä¸Šç§»åŠ¨ï¼Œä¸ºäº†æ‰¾åˆ°è®©$f(x,y)$å€¼æœ€å°çš„ç‚¹ï¼Œé¦–å…ˆæˆ‘ä»¬æ±‚å‡º$f(x,y)$åœ¨å½“å‰ç‚¹çš„æ¢¯åº¦ï¼Œæ¢¯åº¦ä¹Ÿå°±æ˜¯æœç€è¿™ä¸ªæ–¹å‘ï¼ˆæˆ–è€…åæ–¹å‘ï¼‰å‰è¿›ï¼Œ$f(x,y)$çš„å€¼ä¼šå˜å¤§ï¼ˆæˆ–è€…å˜å°ï¼‰,å› æ­¤åªè¦æ¢¯åº¦ä¸è¿™æ¡çº¿ä¸Šè¯¥ç‚¹çš„æ³•å‘é‡ä¸å¹³è¡Œï¼Œæˆ‘ä»¬æ€»æ˜¯å¯ä»¥å°†æ¢¯åº¦æŠ•å½±åˆ°è¯¥ç‚¹çš„åˆ‡çº¿ä¸Šï¼Œä¹Ÿå°±æ˜¯ä¾ç„¶èƒ½æŠ„ç€åˆ‡çº¿çš„æŸä¸ªæ–¹å‘è¿åŠ¨ï¼Œè®©$f(x,y)$çš„å€¼å˜å°ï¼ˆå¦‚æœè¦æ‰¾åˆ°æœ€å¤§å€¼ï¼Œå°±æ˜¯å˜å¤§ï¼‰ã€‚å½“æ¢¯åº¦ä¸æ³•å‘é‡å¹³è¡Œçš„æ—¶å€™ï¼Œä¸è®ºæœç€å“ªä¸ªæ–¹å‘ï¼Œéƒ½æ— æ³•è®©$f(x,y)$çš„å€¼å˜å°ï¼Œå› æ­¤è¿™ä¸ªç‚¹å°±æ˜¯ä¸€ä¸ªæå€¼å°ç‚¹ã€‚</p><p>è€Œæˆ‘ä»¬çŸ¥é“çš„ï¼Œä¸Šé¢çš„æ³•å‘é‡ï¼Œå®é™…ä¸Šæ˜¯$g(x,y)$åœ¨è¯¥ç‚¹çš„æ¢¯åº¦ï¼Œå› æ­¤æˆ‘ä»¬æœ‰åœ¨æå€¼ç‚¹çš„æ—¶å€™ï¼š</p><script type="math/tex; mode=display"> \nabla f(x,y) = \lambda \nabla g(x,y)</script><p>ä¸Šå¼ä¸­ï¼Œ$\lambda$çš„æ­£è´Ÿå–å†³äºè¦çš„æ˜¯æå¤§å€¼ç‚¹è¿˜æ˜¯æå°å€¼ç‚¹ã€‚è€Œä¸Šé¢çš„å¼å­ï¼Œå®é™…ä¸Šå°±ä¸æ–¹ç¨‹ç»„çš„å‰ä¸¤ä¸ªæ–¹ç¨‹ä¸€è‡´ã€‚</p><p>2.å¦ä¸€ä¸ªåŠæ³•ï¼Œç”»å‡ºç­‰é«˜çº¿å›¾ã€‚</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/2012101621500549.png" alt=""></p><p>å¯ä»¥æ¯”è¾ƒæ¸…æ™°åœ°çœ‹å‡ºæ¥ï¼Œå½“$g(x,y)=0$ä¸ç­‰é«˜çº¿ç›¸åˆ‡çš„ç‚¹æ˜¯æå°å€¼ã€‚å› ä¸ºä½ åªèƒ½åœ¨$g(x,y)=0$çš„è¿™æ¡çº¿ä¸Šç§»åŠ¨ï¼Œé‚£ä¹ˆåˆ«çš„åœ°æ–¹æ€»ä¼šæ¯”å®ƒå¤§ï¼ˆæˆ–è€…å¤§ï¼‰ã€‚é‚£æ€ä¹ˆæ±‚ç›¸åˆ‡çš„éƒ¨åˆ†çš„ç‚¹å‘¢ï¼Ÿé¦–å…ˆï¼Œç­‰é«˜çº¿çš„æ–¹ç¨‹ï¼Œå®é™…ä¸Šæ˜¯æ›²çº¿åˆ°(x,y)å¹³é¢çš„æŠ•å½±ï¼Œæ–¹ç¨‹ä¸º$f(x,y)=C$,åŒæ ·çš„ï¼Œæ—¢ç„¶ç›¸åˆ‡ï¼Œé‚£ä¹ˆä»–ä»¬åœ¨è¯¥ç‚¹çš„æ³•å‘é‡ä¸€å®šæ˜¯å¹³è¡Œçš„ã€‚è®¡ç®—æ³•å‘é‡ï¼Œåˆå›åˆ°äº†å‰é¢çš„å†…å®¹ï¼š</p><script type="math/tex; mode=display">  \nabla f(x,y) = \lambda \nabla g(x,y)</script><p>è”ç«‹$g(x,y)=0$å³å¯å¾—åˆ°åŸæ¥çš„æ–¹ç¨‹ç»„ã€‚</p><p>ç°åœ¨æˆ‘ä»¬çŸ¥é“äº†æ¡ä»¶æå€¼å¦‚ä½•è§£å‡ºæ¥ï¼Œä½†æ˜¯è¿™åªæ˜¯æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•çš„ä¸€éƒ¨åˆ†ã€‚æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•æ˜¯ä¸€ç§å¯»æ‰¾å˜é‡å—ä¸€ä¸ªæˆ–å¤šä¸ªæ¡ä»¶æ‰€é™åˆ¶çš„å¤šå…ƒå‡½æ•°çš„æå€¼çš„æ–¹æ³•ã€‚é¦–å…ˆä¸Šé¢çš„æ¡ä»¶é™åˆ¶åªæœ‰ä¸€ä¸ªï¼Œå¦å¤–ä¸Šé¢çš„æ¡ä»¶ä¹Ÿå¾ˆç®€å•ï¼Œæ˜¯åœ¨$g(x,y)=0$ï¼Œå¦‚æœæˆ‘ä»¬è¦çš„æ˜¯$g(x,y) \leq 0$çš„å‘¢ï¼Ÿ</p><p>å¦‚æœ$f(x,y)$æå€¼ç‚¹æœ¬èº«å°±åœ¨ä¸Šé¢çš„çº¦æŸèŒƒå›´å†…ï¼Œé‚£ä¹ˆç›¸å½“äºæ²¡æœ‰çº¦æŸï¼Œä¹Ÿå°±æ˜¯$\lambda = 0$ï¼Œå¦åˆ™åœ¨è¾¹ç•Œä¸Šï¼Œåˆå›åˆ°äº†ä¸Šé¢çš„é—®é¢˜ï¼š$g(x,y)=0$.æ€»ä¹‹ï¼Œ$\lambda g(x,y) = 0$.å› æ­¤æˆ‘ä»¬éœ€è¦æ”¹å˜è”ç«‹æ¡ä»¶å³å¯ã€‚</p><p>ä¸Šé¢çš„å¼å­ï¼Œéƒ½æ˜¯$\lambda$ä¹Ÿä¼šå˜åŒ–çš„æƒ…å†µã€‚ä½†æ˜¯ï¼Œåœ¨æœºå™¨å­¦ä¹ çš„æ­£åˆ™åŒ–ä¸­ï¼Œæˆ‘ä»¬å¾€å¾€ç»™å‡ºçš„å¼å­æ˜¯å½¢å¦‚è¿™æ ·$E_{in} + \frac \lambda N ||W||^2$.è€Œä¸”è¿™ä¸ª$\lambda$å¯èƒ½æ˜¯ä¸ªå®šå€¼ã€‚å¦‚æœ$\lambda$å›ºå®šï¼Œå¾—åˆ°çš„åˆæ˜¯ä»€ä¹ˆå€¼ï¼Ÿ</p><p>æˆ‘ä»¬å¯ä»¥æ¨æ–­å‡ºæ¥çš„æ˜¯å¦‚æœ$E_{in}$æ‰¾åˆ°äº†æœ€å°çš„åœ°æ–¹ï¼Œé‚£ä¹ˆ$W$ä¹Ÿå°±ä¸º0ï¼Œå¦åˆ™$\nabla E_{in}$ä¸$W$çš„æ¯”å€¼æ˜¯ä¸€ä¸ªå®šå€¼ã€‚ä»è¿™äº›é‡Œå¾—ä¸åˆ°å¾ˆæœ‰ç”¨çš„ä¿¡æ¯ã€‚ä½†æ˜¯ä»å¦ä¸€ä¸ªè§’åº¦æ¥è¯´ï¼Œå®ƒç¡®å®é™åˆ¶äº†$W$çš„å¤§å°ï¼Œè™½ç„¶ç›®å‰ä¸çŸ¥é“å…·ä½“é™å®šåˆ°å“ªä¸ªèŒƒå›´ã€‚$||W_{reg}|| \leq ||W_{lin}||$æ˜¯ä¸€å®šçš„ï¼Œå¯ä»¥ä»åè¯æ³•è¯æ˜ï¼šå¦‚æœ$||W_{lin}||&lt;||W_{reg}||$,é‚£ä¹ˆE_{in}ä¹Ÿä¼šå°ï¼Œæ­£åˆ™é¡¹ä¹Ÿæ›´å°ï¼Œæ‰€ä»¥è¿™æ—¶å€™çš„$E_{reg}$æ¯”ä½¿ç”¨$W_{reg}$çš„æ›´ä½³ï¼Œä¹Ÿå°±æ˜¯å‰é¢æ‰¾åˆ°çš„ä¸å¯èƒ½æ˜¯æœ€å°å€¼ã€‚</p><p>å¦ä¸€ä¸ªæ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•é‡è¦çš„åœ¨æœºå™¨å­¦ä¹ ä¸­çš„åº”ç”¨ï¼Œæ˜¯åœ¨SVMä¸­ã€‚ </p>]]></content>
      
      
      <categories>
          
          <category> æ•°å­¦ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mathematics </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”ï¼ˆåŸºçŸ³ï¼‰ä½œä¸š4</title>
      <link href="/2018/10/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%EF%BC%88%E5%9F%BA%E7%9F%B3%EF%BC%89%E4%BD%9C%E4%B8%9A4/"/>
      <url>/2018/10/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%EF%BC%88%E5%9F%BA%E7%9F%B3%EF%BC%89%E4%BD%9C%E4%B8%9A4/</url>
      
        <content type="html"><![CDATA[<p>æœºå™¨å­¦ä¹ åŸºçŸ³çš„æœ€åä¸€æ¬¡ä½œä¸šï¼Œæ€»å…±20é“é¢˜ç›®ã€‚<br><a id="more"></a><br><strong>1. Deterministic noise depends on $\mathcal{H}$, as some models approximate $f$ better than others. Assume $\mathcal{H}â€™\subset \mathcal{H}$ and that $f$ is fixed. In general (but not necessarily in all cases), if we use $\mathcal{H}â€™$ instead of $\mathcal{H}$, how does deterministic noise behave?</strong></p><p>a. In general, deterministic noise will decrease.</p><p>b. In general, deterministic noise will increase.</p><p>c. In general, deterministic noise will be the same.</p><p>d. If $d_{\text{vc}}(\mathcal{H}â€™) \le \frac{1}{2} d_{\text{vc}}(\mathcal{H})$, deterministic noise will increase, else it will decrease.</p><p>e. If $d_{\text{vc}}(\mathcal{H}â€™) \le \frac{1}{2} d_{\text{vc}}(\mathcal{H})$, deterministic noise will decrease, else it will increase.</p><p>deterministic noiseå‡ºç°çš„åŸå› ï¼Œæ˜¯$H$æ— æ³•å®Œç¾çš„æ¨¡æ‹Ÿf.è€Œ$Hâ€™$æ˜¯$H$çš„å­é›†ï¼Œä¹Ÿå°±æ˜¯å®ƒçš„æ¨¡å‹å¤æ‚åº¦æ›´ä½ï¼Œä¸€èˆ¬æ¥è¯´æ›´æ— æ³•æ¨¡æ‹Ÿç›®æ ‡å‡½æ•°ï¼Œå®ƒçš„deterministic noiseåº”è¯¥æ˜¯ä¸Šå‡çš„ï¼Œé€‰bã€‚</p><p><strong>2. Consider the following hypothesis set for $\mathbf{x} \in \mathbb{R}^dx$ defined by the constraint:</strong></p><script type="math/tex; mode=display">\mathcal{H}(d, d_0) = \{ h ~|~ h(\mathbf{x}) = \mathbf{w}^\mathrm{T}\mathbf{x}; w_i = 0 \hspace{1 mm} \mbox{for} \hspace{1mm} i \geq d_0 \},</script><p><strong>which of the following statements is correct?</strong></p><p>a. $H(10,3)âŠ‚H(10,4)$</p><p>b. $H(10,3)âˆªH(10,4)=\{\}$</p><p>c. $H(10,3)âŠƒH(10,4)$</p><p>d. $H(10,3)âˆ©H(10,4)=\{\}$</p><p>e. none of the other choices</p><p>è¿™ä¸ªé¢˜ç›®çš„çº¦æŸä¸ç®—éš¾ï¼Œä¹Ÿå°±æ˜¯æœ‰ä¸€ä¸ªé¢å¤–çš„å‚æ•°$d_0(d_0 \leq d)$,wä¸‹æ ‡æ¯”$d_0$å¤§çš„å›ºå®šä¸º0.é€‰é¡¹ä¸­$d_0$åªæœ‰3ä¸4ä¸¤ä¸ªé€‰é¡¹ï¼Œè€Œ$d_0 = 3$çš„$H$æ˜¯åŒ…å«åœ¨$d_0=4$çš„$H$ä¸­çš„ï¼Œå› ä¸ºå¯¹äºå‰è€…æ¥è¯´ï¼Œ$w_3$çš„å€¼ä¹Ÿç¡®å®šäº†ï¼Œè‡ªç”±åº¦ä¸º3ï¼Œåè€…è‡ªç”±åº¦ä¸º4ï¼Œ$w_0$å¯ä»¥ä¸º0ä¹Ÿå¯ä»¥ä¸ºå…¶ä»–å€¼.å› æ­¤ç­”æ¡ˆé€‰a.</p><p>For Questions 3-4, consider the augmented error $E_{\text{aug}}(\mathbf{w}) = E_{\text{in}}(\mathbf{w}) + \frac{\lambda}{N} \mathbf{w}^T \mathbf{w}$ with some $\lambda &gt; 0$.</p><p><strong>3. If we want to minimize the augmented error $E_{\text{aug}}(\mathbf{w})$ by gradient descent with $\eta$ as learning rate, which of the following is a correct update rule?</strong></p><p>a. $w(t+1)âŸµw(t)+Î·Î»\nabla E \in (w(t))$.</p><p>b. $w(t+1)âŸµw(t)-Î·Î»\nabla E \in (w(t))$.</p><p>c. $w(t+1)âŸµ(1âˆ’ \frac {2\eta \lambda} {N})w(t)âˆ’Î·\nabla E \in (w(t))$.</p><p>d. $w(t+1)âŸµ(1+ \frac {2\eta \lambda} {N})w(t)âˆ’Î·\nabla E \in (w(t))$.</p><p>e. none of the other choices</p><p>è¿™ä¸ªé¢˜ç›®ä¹Ÿæ˜¯æ¯”è¾ƒç®€å•çš„ã€‚æ¢¯åº¦ä¸‹é™å°±æ˜¯æœç€æ¢¯åº¦çš„åæ–¹å‘å‰è¿›ï¼Œå› æ­¤åªç”¨æ±‚å‡ºæ¥æ¢¯åº¦å°±å¯ä»¥ã€‚$W^TW$çš„æ¢¯åº¦å¾ˆç®€å•æ˜¯$2W$ï¼Œå…¶ä»–çš„ä¸ä¹‹å‰çš„ä¸€è‡´ï¼Œå› æ­¤ç­”æ¡ˆé€‰cã€‚</p><p><strong>4. Let $\mathbf{w}_{\text{lin}}$ be the optimal solution for the plain-vanilla linear regression and $\mathbf{w}_{\text{reg}}(\lambda)$ be the optimal solution for minimizing $E_{\text{aug}}$ in Question 3, with $E_{\text{in}}$ being the squared error for linear regression. Which of the following is correct?</strong></p><p>a. none of the other choices</p><p>b. $||W_{reg}(\lambda)|| \leq ||W_{lin}||$ for any $\lambda &gt; 0$</p><p>c. $||W_{reg}(\lambda)|| \geq ||W_{lin}||$ for any $\lambda &gt; 0$</p><p>d. $||W_{reg}(\lambda)||$ is always a non-decreasing function of $\lambda$ for $\lambda \ge 0$</p><p>e. $||W_{reg}(\lambda)||$ is always a constant function of $\lambda$ for $\lambda \ge 0$</p><p>è¦æ˜ç™½è¿™ä¸ªé¢˜ç›®ï¼Œé¦–å…ˆè¦çŸ¥é“ä»€ä¹ˆæ˜¯$||W||$ï¼Œè¿™ä¸ªæ„æ€æ˜¯$W$å‘é‡çš„èŒƒæ•°ï¼Œä¹Ÿå°±ç­‰äº$W^TW$.å¯¹äºä¹‹å‰é¢˜ç›®æ·»åŠ çš„regularizationæ¥è¯´ï¼Œé™åˆ¶å®é™…ä¸Šæ˜¯$W^W \leq C$.å¦‚ä½•æ¨å¯¼ï¼Ÿ<br>æœ€ä½ç‚¹$\nabla E_{aug} = 0$ï¼Œä¹Ÿå°±æ˜¯$\nabla E_{in} = - \frac {2\lambda} {N} W$,å› æ­¤æœ€ä½ç‚¹$\nabla E_{in}$ä¸$W$çš„é•¿åº¦æ¯”å€¼æ˜¯ä¸€å®šçš„ï¼Œä¹Ÿå°±æ˜¯$W$å‘é‡çš„é•¿åº¦è¢«ç¡®å®šåˆ°äº†ä¸€ä¸ªå€¼ã€‚è€Œå› ä¸ºä¸€ç›´åœ¨æœçº¦æŸæ¡ä»¶ä¸‹æœ€ä½ç‚¹èµ°ï¼Œå› æ¬¡$\nabla E_{in}$ä¹Ÿæ˜¯æ¥è¿‘å¹³ç¼“çš„ä¹Ÿå°±æ˜¯å€¼æ¯”è¾ƒå°ï¼Œæ‰€ä»¥è¿™æ„å‘³ç€$W$æœ€åæ˜¯æ¯”è¾ƒå°çš„ã€‚</p><p>ä»å¦ä¸€ä¸ªè§’åº¦æ¥çœ‹ï¼ŒèŒƒæ•°ä¸€å®šæ˜¯å¤§äºé›¶çš„ï¼Œä¸ºäº†æ‰¾åˆ°æœ€ä½ç‚¹å½“ç„¶æ˜¯è®©èŒƒæ•°å°½é‡å°ï¼Œä¹Ÿå°±æ˜¯æ­£åˆ™åŒ–ç›¸å½“äºç»™å„ä¸ªå‚æ•°å¢åŠ äº†æƒ©ç½šï¼Œæƒ³è®©ä»–ä»¬å˜å¾—æ›´å°ã€‚</p><p>å› æ­¤è¿™ä¸ªé¢˜ç›®çš„ç­”æ¡ˆé€‰æ‹©b.å¦‚æœæ²¡æœ‰çº¦æŸæƒ…å†µä¸‹å¾—åˆ°çš„æœ€å¥½çš„$W$ä¹Ÿæ»¡è¶³çº¦æŸï¼Œä¹Ÿå°±æ˜¯ç­‰äºçš„æƒ…å†µï¼Œå…¶ä»–æ—¶å€™$||W_{lin}|| &gt; ||W_{reg}||$</p><p><strong>5. You are given the data points: $(-1,0)$, $(\rho,1)$, $(1,0)$,$ \rho \ge 0$, and a choice between two models:</strong></p><ul><li>constant $h_0(x)=b_0$ and</li><li>linear $h_1(x)=a_1x+b_1$</li></ul><p>For which value of $\rho$ would the two models be tied using leave-one-out cross-validation with the squared error measure?</p><p>a. $\sqrt{\sqrt {3}+4}$</p><p>b. $\sqrt{\sqrt{3} - 1}$</p><p>c. $\sqrt{9+4 \sqrt{6}}$</p><p>d. $\sqrt{9 - \sqrt{6}}$</p><p>e. none of the other choice</p><p>ä½¿ç”¨Leave-One-Out CrosséªŒè¯æ¥å¾—åˆ°$E_{val}$ã€‚å¯¹äºç¬¬ä¸€ç§æƒ…å†µï¼Œ$h(x)=b_0$æ˜¯ä¸ªå¸¸é‡ã€‚å¯¹äºç¬¬äºŒç§æƒ…å†µæ˜¯ä¸ªç›´çº¿ã€‚é¦–å…ˆè¦è®¡ç®—å‡ºä¸¤ç§æ¨¡å‹çš„$E_{val}$.</p><p>ç¬¬ä¸€ç§æ¨¡å‹ï¼š</p><ul><li>ç¬¬ä¸€ä¸ªä¸ºéªŒè¯é›†ï¼šåˆ™$E_{in} = \frac 1 2 ((b_0-1)^2 + b_0^2)$,åˆ™$E_{in}$æœ€å°çš„æ—¶å€™$b_0 = 0.5$ï¼Œ$err = 0.5 \times 0.5 = 0.25$.</li><li>ç¬¬äºŒä¸ªä¸ºéªŒè¯é›†ï¼šåˆ™$E_{in} = b_0^2$,å› æ­¤$b_0 = 0$,err = 1.</li><li>ç¬¬ä¸‰ä¸ªä¸ºéªŒè¯é›†ï¼Œæƒ…å†µä¸ç¬¬ä¸€ç§æƒ…å†µä¸€è‡´ï¼Œ$err =0.25$.</li></ul><p>å› æ­¤è¿™æ—¶å€™çš„$E_{val} = (1+0.25+0.25)/3 = 0.5$.</p><p>ç¬¬äºŒç§æ¨¡å‹ï¼š</p><ul><li>ç¬¬ä¸€ä¸ªä¸ºéªŒè¯é›†ï¼šåˆ™è®¡ç®—å‡ºæ¥å¾—åˆ°$a_1 = \frac 1 {p-1},b_1 = \frac 1 {1-p}$,åˆ™é¢„æµ‹å€¼æ˜¯$\frac 2 {1-p}$,$err = \frac 4 {(1-p)^2}$.</li><li>ç¬¬äºŒä¸ªä¸ºéªŒè¯é›†ï¼Œå¾—åˆ°çš„æ˜¯ä¸€ä¸ªå¸¸ç†Ÿï¼š$h(x) = 0$.è¿™ç§æƒ…å†µä¸‹$err = 1$.</li><li>ç¬¬ä¸‰ä¸ªä¸ºéªŒè¯é›†ï¼Œåˆ™è®¡ç®—å‡ºæ¥å¾—åˆ°$a_1 = \frac 1 {p+1},b_1 = \frac 1 {1+p}$,åˆ™é¢„æµ‹å€¼æ˜¯$\frac 2 {1+p}$,$err = \frac 4 {(1+p)^2}$.</li></ul><p>è¿™æ—¶å€™çš„$E_{val} =(1 + \frac 4 {(1-p)^2} + \frac 4 {(1+p)^2} )/3$.</p><p>é¢˜ç›®ä¸­è¯´äº†ï¼Œä¸¤ä¸ªæ¨¡å‹éƒ½é€‚ç”¨åˆ°è¯¥æ ·æœ¬é›†ã€‚é‚£ä¹ˆä¸Šé¢ä¸¤ä¸ªåº”è¯¥æ˜¯ç›¸ç­‰çš„ã€‚å¯ä»¥è§£å‡ºæ¥ï¼š$p^2 = 9Â±4\sqrt {6}$,è€Œ$9-4 \sqrt 6 &lt; 0$ï¼Œå› æ­¤æ­£ç¡®ç­”æ¡ˆæ˜¯ $9+ 4 \sqrt {6}$.ç­”æ¡ˆé€‰cã€‚ </p><p>For Questions 6-7, suppose that for 5 weeks in a row, a letter arrives in the mail that predicts the outcome of the upcoming Monday night baseball game.</p><p><strong>6. Assume there are no tie. You keenly watch each Monday and to your surprise, the prediction is correct each time. On the day after the fifth game, a letter arrives, stating that if you wish to see next weekâ€™s prediction, a payment of NTD $1000$ is required. Which of the following statement is true?</strong></p><p>a. There are 31 win-lose predictions for 5 games.</p><p>b. If the sender wants to make sure that at least one person receives correct predictions on all 5 games from him, the sender should target to begin with at least 5 people.</p><p>c. To make sure that at least one person receives correct predictions on all 5 games from the sender, after the first letter `predictsâ€™ the outcome of the first game, the sender should target at least 16 people with the second letter.</p><p>d. To make sure that at least one person receives correct predictions on all 5 games from him, at least 64 letters should be sent before the fifth game.</p><p>e. none of the other choice</p><p>è¿™ä¸ªé¢˜ç›®è®²çš„æ˜¯ä¸€ä¸ªå°æŠŠæˆã€‚5åœºæ¯”èµ›ï¼Œæ¯åœºæ¯”èµ›åªæœ‰æ­£è´Ÿä¸¤ä¸ªæƒ…å†µã€‚å› æ­¤ä¸€å…±å¯èƒ½å‡ºç°çš„æƒ…å†µæœ‰$2^5=32$ç§ã€‚aé”™è¯¯ã€‚32ç§æƒ…å†µï¼Œå½“ç„¶è¦32ä¸ªäººï¼Œå› æ­¤bé”™è¯¯ã€‚è‡³å°‘å‘å‡ºå»çš„ä¿¡æœ‰ï¼ˆ32+16+8+4+2=62ï¼‰å°ï¼Œdé”™è¯¯ã€‚è€Œcï¼Œç¬¬ä¸€ä¸ªç»“æœå‡ºæ¥åï¼Œä¼šæœ‰ä¸€åŠçš„äººæ”¶åˆ°é”™è¯¯çš„é¢„æµ‹ï¼Œå› æ­¤ç¬¬äºŒå°ä¿¡åªéœ€è¦å‘ç»™æ­£ç¡®çš„é‚£äº›äººå°±å¥½äº†ï¼Œä¹Ÿå°±æ˜¯16ä¸ªäºº.</p><p><strong>7. If the cost of printing and mailing out each letter is NTD 10. If the sender sends the minimum number of letters out, how much money can be made for the above `fraudâ€™ to succeed once? That is, one of the recipients does send him NTD 1000 to receive the prediction of the 6-th game?</strong></p><p>a. NTD 340</p><p>b. NTD 370</p><p>c. NTD 400</p><p>d. NTD 430</p><p>e. NTD 460</p><p>ä¸Šé¢ä¸€é“é¢˜ç›®æ¨æ–­å‡ºæ¥ï¼Œè‡³å°‘è¦å‘é€62å°ä¿¡æ‰èƒ½ä¿è¯æœ‰ä¸ªäººæ”¶åˆ°æ‰€æœ‰é¢„æµ‹ç»“æœã€‚è€Œæœ€åä¸€ä¸ªäººæ”¶åˆ°çš„å…¨éƒ¨æ­£ç¡®çš„é¢„æµ‹åè¿˜è¦åœ¨åŠ å‘ä¸€å°ï¼Œæ¥éª—é’±ã€‚ä¹Ÿå°±æ˜¯63å°ï¼Œæ‰€ä»¥ç­”æ¡ˆæ˜¯NTD 370ï¼Œé€‰b.</p><p>For Questions 8-10, please read the following story first. In our credit card example, the bank starts with some vague idea of what constitutes a good credit risk. So, as customers $\mathbf{x}_1, \mathbf{x}_2,â€¦,\mathbf{x}_N$ arrive, the bank applies its vague idea to approve credit cards for some of these customers based on a formula $a(\mathbf{x})$. Then, only those who get credit cards are monitored to see if they default or not.</p><p><strong>8. For simplicity, suppose that the first $N=10000$ customers were given credit cards by the credit approval function $a(\mathbf{x})$. Now that the bank knows the behavior of these customers, it comes to you to improve their algorithm for approving credit. The bank gives you the data $(\mathbf{x}_1, y_1), â€¦ , (\mathbf{x}_N, y_N)$. Before you look at the data, you do mathematical derivations and come up with a credit approval function. You now test it on the data and, to your delight, obtain perfect prediction.</strong></p><p>What is $M$, the size of your hypothesis set?</p><p>a. $1$</p><p>b. $N$</p><p>c. $2^N$</p><p>d. $N^2$</p><p>e. We have no idea about it.</p><p>åˆ©ç”¨æ•°å­¦æ¨ç†æƒ³åˆ°äº†ä¸€ä¸ªå‡½æ•°åšå‡ºäº†å¾ˆå¥½çš„é¢„æµ‹ï¼Œå› æ­¤è¿™ä¸ªvc dimensionæ˜¯æ— æ³•è®¡ç®—çš„ï¼Œä½†æ˜¯å› ä¸ºæ²¡æœ‰ç»è¿‡æ•°æ®çš„å­¦ä¹ ï¼Œè¿™ä¸ªHçš„å¤§å°åº”è¯¥æ˜¯1ï¼Œé€‰æ‹©a.</p><p><strong>9. With such an $M$, what does the Hoeffding bound say about the probability that the true average error rate of $g$ is worse than $1\%$ for $N=10,000$?</strong></p><p>a. $\leq 0.171$</p><p>b. $\leq 0.221$</p><p>c. $\leq 0.271$</p><p>d. $\leq 0.321$</p><p>e. none of the other choices</p><p>éœå¤«ä¸ä¸ç­‰å¼çš„ç®€å•åº”ç”¨ï¼š$P[\nu  - \upsilon|&gt; \epsilon ] \leq 2 e^{-2\epsilon ^2N}$ã€‚ä¸Šè¿°ä¸­$\epsilon = 0.01,N = 10000$,å¾—åˆ°ç­”æ¡ˆä¸º0.2706705664732ï¼Œç­”æ¡ˆé€‰c.</p><p><strong>10. You assure the bank that you have a got a system $g$ for approving credit cards for new customers, which is nearly error-free. Your confidence is given by your answer to the previous question. The bank is thrilled and uses your $g$ to approve credit for new customers. To their dismay, more than half their credit cards are being defaulted on. Assume that the customers that were sent to the old credit approval function and the customers that were sent to your g are indeed i.i.d. from the same distribution, and the bank is lucky enough (so the â€œbad luckâ€ that â€œthe true error of gg is worse than $1\%$â€™â€™ does not happen). Which of the following claim is true?</strong></p><p>a. By applying $a(\mathbf{x}) \mbox{ NOR } g(\mathbf{x})$ to approve credit for new customers, the performance of the overall credit approval system can be improved with guarantee provided by the previous problem.</p><p>b. By applying $a(\mathbf{x}) \mbox{ NAND } g(\mathbf{x})$ to approve credit for new customers, the performance of the overall credit approval system can be improved with guarantee provided by the previous problem.</p><p>c. By applying $a(\mathbf{x}) \mbox{ OR } g(\mathbf{x})$ to approve credit for new customers, the performance of the overall credit approval system can be improved with guarantee provided by the previous problem.</p><p>d. By applying $a(\mathbf{x}) \mbox{ AND } g(\mathbf{x})$ to approve credit for new customers, the performance of the overall credit approval system can be improved with guarantee provided by the previous problem.</p><p>e. none of the other choices</p><p>è¿™ä¸ªé¢˜ç›®ä¸­è¯´åˆ°ï¼Œåˆ©ç”¨ä¹‹å‰çš„æ¨æ–­å‡ºæ¥çš„gï¼Œæœ¬åº”è¯¥æœ‰å¾ˆå¥½çš„è¡¨ç°ï¼Œä½†æ˜¯å´å¾—åˆ°äº†å¾ˆå·®çš„è¡¨ç°ã€‚ä¸ºä»€ä¹ˆï¼Ÿæˆ‘ä»¬è¦æ³¨æ„ä¸€ä¸ªäº‹ï¼šSample Biasã€‚è™½ç„¶é¢˜ç›®ä¸­è¯´äº†ï¼Œæ–°çš„é¡¾å®¢å’Œä¹‹å‰ç³»ç»Ÿçš„é¡¾å®¢æ˜¯æ¥è‡ªäºåŒä¸€åˆ†å¸ƒçš„ï¼Œä½†æ˜¯æˆ‘ä»¬å¾—åˆ°çš„testæ•°æ®çš„åˆ†å¸ƒå¹¶ä¸æ˜¯åŸå…ˆçš„é¡¾å®¢åˆ†å¸ƒã€‚testæ•°æ®ä¸­ï¼Œé¡¾å®¢çš„ä¿¡æ¯å¹¶ä¸æ˜¯éšæœºå¾—åˆ°çš„ï¼Œè€Œæ˜¯å…ˆç»è¿‡äº†$a(x)$çš„ç­›é€‰ã€‚ä¸Šé¢çš„éœå¤«æ›¼ä¸ç­‰å¼çš„ç†è®ºä¿è¯æ˜¯åœ¨åŒä¸€åˆ†å¸ƒçš„å‰æä¸‹ï¼Œå› æ­¤é¦–å…ˆè¦ç»è¿‡$a(x)$çš„ç­›é€‰ï¼Œç„¶åå†ç”¨$g(x)$æ¥åˆ¤æ–­ã€‚å› æ­¤ç­”æ¡ˆé€‰d.</p><p>For Questions 11-12, consider linear regression with virtual examples. </p><p><strong>11. That is, we add $K$ virtual examples $(\tilde{\mathbf{x}}_1, \tilde{y}_1),(\tilde{\mathbf{x}}_2, \tilde{y}_2),\dots, (\tilde{\mathbf{x}}_K, \tilde{y}_K)$ to the training data set, and solve$<br>\min \limits _{\mathbf{w}} \frac{1}{N+K} \left(\sum_{n=1}^N (y_n - \mathbf{w}^T \mathbf{x}_n)^2 + \sum_{k=1}^K (\tilde{y}_k - \mathbf{w}^T \tilde{\mathbf{x}}_k)^2\right)$.We will show that using some â€œspecialâ€ virtual examples, which were claimed to be a possible way to combat overfitting in Lecture 9, is related to regularization, another possible way to combat overfitting discussed in Lecture 10. Let $\tilde{\mathbf{X}} = [\tilde{\mathbf{x}}_1 \tilde{\mathbf{x}}_2 \ldots \tilde{\mathbf{x}}_K]^T$, and $\tilde{\mathbf{y}} = [\tilde{y}_1, \tilde{y}_2, \ldots, \tilde{y}_K]^T$. What is the optimal $\mathbf{w}$ to the optimization problem above, assuming that all the inversions exist?</strong></p><p>a. $(\mathbf{X}^T\mathbf{X})^{-1}(\widetilde {\mathbf{X}}^T\widetilde{\mathbf{y}})$</p><p>b. $(\mathbf{X}^T\mathbf{X})^{-1}(\mathbf{X}^T \mathbf{y}+\widetilde {\mathbf{X}}^T\widetilde{y})$</p><p>c. $(\mathbf{X}^T\mathbf{X} + \widetilde{\mathbf{X}}^T\widetilde{\mathbf{X}})^{-1}(\widetilde {\mathbf{X}}^T\widetilde{\mathbf{y}})$</p><p>d. $(\mathbf{X}^T\mathbf{X} + \widetilde{\mathbf{X}}^T\widetilde{\mathbf{X}})^{-1}(\mathbf{X}^T \mathbf{y} +\widetilde {\mathbf{X}}^T\widetilde{\mathbf{y}} )$</p><p>e. none of the other choice</p><p>è¿™ä¸ªé¢˜ç›®è¯´èµ·æ¥ä¹Ÿå®¹æ˜“ã€‚æ—¢ç„¶æŠŠè™šæ‹Ÿæ•°æ®ä¹Ÿèåˆè¿›å»äº†ï¼Œå½“ç„¶å„éƒ¨åˆ†éƒ½è¦è®¡ç®—ï¼Œå¾ˆå®¹æ˜“æ’é™¤å…¶ä»–ç­”æ¡ˆ,é€‰æ‹©dã€‚</p><p><strong>12. For what $\tilde{\mathbf{X}} and $\tilde{\mathbf{y}}$ will the solution of the linear regression problem above equal to</strong></p><script type="math/tex; mode=display">\mathbf{w}_{\text{reg}} = \mathrm{argmin}_{\mathbf{w}} \frac{\lambda}{N} \|\mathbf{w}\|^2 + \frac{1}{N} \|\mathbf{X}\mathbf{w}-\mathbf{y}\|^2?</script><p>a. $\tilde{\mathbf{X}} = I, \tilde{\mathbf{y}} = 0$</p><p>b. $\tilde{\mathbf{X}} = \sqrt {\lambda}I, \tilde{\mathbf{y}} = 0$</p><p>c. $\tilde{\mathbf{X}} = \lambda I, \tilde{\mathbf{y}} = \mathbf{1}$</p><p>d. $\tilde{\mathbf{X}} = \sqrt{\lambda} \mathbf{X}, \tilde{\mathbf{y}} = \mathbf{y}$</p><p>e. none of the other choice</p><p>è¿™ä¸ªé—®é¢˜ä¹ä¸€çœ‹ï¼Œæ‘¸ä¸ç€å¤´è„‘ã€‚ä¸€ä¸ªæ˜¯çŸ©é˜µä¸­æ±‚é€†æ“ä½œï¼Œå¦ä¸€ä¸ªæ˜¯æœ€å°åŒ–ä¸€ä¸ªå‡½æ•°ï¼ˆargmin(f(x))çš„å®šä¹‰ä¹‹å‰å·²ç»è¯´è¿‡ï¼‰ã€‚ä½†æ˜¯æ¢ä¸ªåŠæ³•çš„è¯ï¼Œå…¶å®å¾ˆå®¹æ˜“è§£å†³ï¼Œæˆ‘ä»¬å¯ä»¥æƒ³è±¡ä¸€ä¸‹11é¢˜ä¸­éœ€è¦æœ€å°åŒ–çš„å‡½æ•°$E_{in}$ï¼Œåˆ™å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">W_{vir} = argmin_w \frac 1  {N+K} (||\tilde{\mathbf{X}}\mathbf{w} - \tilde{\mathbf{y}}||^2 +  ||\mathbf{X}\mathbf{w} - \mathbf{y}||^2).</script><p>æœ€å°åŒ–çš„è¯æ— è®ºå‰é¢æœ‰æ²¡æœ‰$\frac 1 N$æˆ–è€…å…¶ä»–å¸¸æ•°éƒ½æ˜¯æ— æ‰€è°“çš„ã€‚<br>æƒ³è¦è®©äºŒè€…æœ€åç»“æœç›¸ç­‰ï¼Œä½¿å¾—å»æ‰å¸¸æ•°ä¹‹åç›¸ç­‰å³å¯ï¼Œåˆ™ï¼Œ$\tilde{\mathbf{X}} = \sqrt {\lambda}I, \tilde{\mathbf{y}} = 0$.å› æ­¤è¿™é“é¢˜ç›®ç­”æ¡ˆé€‰bã€‚</p><p><strong>13. Consider regularized linear regression (also called ridge regression) for classification</strong></p><script type="math/tex; mode=display">\mathbf{w}_{\text{reg}} = \mbox{argmin}_{\mathbf{w}} \left(\frac{\lambda}{N} \|\mathbf{w}\|^2 + \frac{1}{N} \|\mathbf{X}\mathbf{w}-\mathbf{y}\|^2\right) .</script><p>Run the algorithm on the following data set as $\mathcal{D}$:<br><a href="https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_algo/hw4_train.dat" target="_blank" rel="noopener">https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_algo/hw4_train.dat</a></p><p>and the following set for evaluating $E_{out}$:<br><a href="https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_algo/hw4_test.dat" target="_blank" rel="noopener">https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_algo/hw4_test.dat</a></p><p>Because the data sets are for classification, please consider only the 0/1 error for all Questions below.</p><p>Let $\lambda = 10$, which of the followings is the corresponding $E_{in}$and $E_{out}$?</p><p>a. $E_{in} = 0.015,E_{out} = 0.020$</p><p>b. $E_{in} = 0.030,E_{out} = 0.015$</p><p>c. $E_{in} = 0.035,E_{out} = 0.020$</p><p>d. $E_{in} = 0.050,E_{out} = 0.045$</p><p>e. $E_{in} = 0.020,E_{out} = 0.010$</p><p>è¿™é“é¢˜ç›®æ˜¯çº¿æ€§å›å½’çš„ä¸€ä¸ªæ”¹è¿›ã€‚ç›¸å¯¹äºä¹‹å‰çš„ä»£ç ä¹Ÿåªè¦åšäº›è®¸çš„æ”¹è¿›å°±å¯ä»¥äº†ã€‚åˆ©ç”¨ä¹‹å‰çš„ç¬¬12é¢˜çš„ç»“è®ºï¼Œæˆ‘ä»¬ä¾ç„¶å¯ä»¥ä¸€æ­¥å¾—åˆ°ç»“æœã€‚<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sign</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> +<span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span><span class="params">(data,W=[])</span>:</span></span><br><span class="line">    nx = []</span><br><span class="line">    ny = []</span><br><span class="line">    ox = []</span><br><span class="line">    oy = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        <span class="keyword">if</span> data[i][<span class="number">-1</span>] == <span class="number">-1</span>:</span><br><span class="line">            nx.append(data[i][<span class="number">0</span>])</span><br><span class="line">            ny.append(data[i][<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ox.append(data[i][<span class="number">0</span>])</span><br><span class="line">            oy.append(data[i][<span class="number">1</span>])</span><br><span class="line">    plt.scatter(nx,ny,marker=<span class="string">"x"</span>,c=<span class="string">"r"</span>)</span><br><span class="line">    plt.scatter(ox,oy,marker=<span class="string">"o"</span>,c=<span class="string">"g"</span>)</span><br><span class="line">    <span class="keyword">if</span> len(W)!=<span class="number">0</span> :</span><br><span class="line">        print(W)</span><br><span class="line">        x = np.linspace(<span class="number">0</span>, <span class="number">1</span>, <span class="number">50</span>)</span><br><span class="line">        y = -W[<span class="number">1</span>] / W[<span class="number">2</span>] * x - W[<span class="number">0</span>] / W[<span class="number">2</span>]</span><br><span class="line">        plt.plot(x, y, color=<span class="string">"black"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridge_regression_one_step</span><span class="params">(data,_lambda)</span>:</span></span><br><span class="line">    X_matrix = []</span><br><span class="line">    Y_matrix = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        temp = [<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(data[i])<span class="number">-1</span>):</span><br><span class="line">            temp.append(data[i][j])</span><br><span class="line"></span><br><span class="line">        X_matrix.append(temp)</span><br><span class="line">        Y_matrix.append([data[i][<span class="number">-1</span>]])</span><br><span class="line">    X = np.mat(X_matrix)</span><br><span class="line">    hatX = math.sqrt(_lambda)*np.eye(len(data[<span class="number">0</span>]))</span><br><span class="line">    <span class="comment">#print(hatX)</span></span><br><span class="line">    hatY = np.mat([ <span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> data[<span class="number">0</span>]]).T</span><br><span class="line">    Y = np.mat(Y_matrix)</span><br><span class="line">    W = (X.T*X + hatX.T*hatX).I*(X.T*Y+hatX.T*hatY)</span><br><span class="line">    <span class="keyword">return</span> W.T.tolist()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Ein</span><span class="params">(data,W)</span>:</span></span><br><span class="line">    err_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        res = W[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,len(W)):</span><br><span class="line">            res += W[j]*data[i][j<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> sign(res) != data[i][<span class="number">-1</span>]:</span><br><span class="line">            err_num+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> err_num</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readDataFrom</span><span class="params">(path)</span>:</span></span><br><span class="line">    separator = re.compile(<span class="string">'\t|\b| |\n'</span>)</span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">with</span> open(path,<span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        s = f.readline()[:<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">while</span> s:</span><br><span class="line">            temp = separator.split(s)</span><br><span class="line">            result.append([float(x) <span class="keyword">for</span> x <span class="keyword">in</span> temp])</span><br><span class="line">            s = f.readline()[:<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    err = <span class="number">0</span></span><br><span class="line">    data = readDataFrom(<span class="string">"./train.dat"</span>)</span><br><span class="line">    <span class="comment">#print(data)</span></span><br><span class="line">    data_test =  readDataFrom(<span class="string">"./test.dat"</span>)</span><br><span class="line">    W = ridge_regression_one_step(data,<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"Ein"</span>,Ein(data,W)/len(data))</span><br><span class="line">    print(<span class="string">"Eout"</span>,Ein(data_test,W)/len(data_test))</span><br><span class="line">    visualize(data,W)</span><br></pre></td></tr></table></figure></p><p>å¯ä»¥å¾—åˆ°æœ€åçš„è¿è¡Œç»“æœå¦‚ä¸‹ï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Ein 0.05</span><br><span class="line">Eout 0.045</span><br></pre></td></tr></table></figure></p><p>å› æ­¤ç­”æ¡ˆé€‰æ‹©d.</p><p><strong>14. Following the previous Question, among $\log_{10} \lambda= \left\{2, 1, 0, -1, \ldots, -8, -9, -10 \right\}$. What is the $\lambda$ with the minimum $E_{in}$? Compute $\lambda$ and its corresponding $E_{in}$ and $E_{out}$ then select the closest answer. Break the tie by selecting the largest $\lambda$.</strong></p><p>a. $log_{10}^{\lambda} = -2,E_{in} = 0.030,E_{out} = 0.040$</p><p>b. $log_{10}^{\lambda} = -4,E_{in} = 0.015,E_{out} = 0.020$</p><p>c. $log_{10}^{\lambda} = -6,E_{in} = 0.030,E_{out} = 0.040$</p><p>d. $log_{10}^{\lambda} = -8,E_{in} = 0.015,E_{out} = 0.020$</p><p>e. $log_{10}^{\lambda} = -10,E_{in} = 0.030,E_{out} = 0.040$</p><p>è¿™ä¸ªé¢˜ç›®åªéœ€è¦å¯¹ä¸Šé¢é¢˜ç›®çš„æ‰§è¡Œå‡½æ•°åšä¸€äº›æ”¹åŠ¨å³å¯ï¼š<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    minEin = <span class="number">1</span></span><br><span class="line">    minEout = <span class="number">1</span></span><br><span class="line">    minEinI = <span class="number">-1</span></span><br><span class="line">    minEoutI = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">-10</span>,<span class="number">3</span>):</span><br><span class="line">        _lambda = math.pow(<span class="number">10</span>,i)</span><br><span class="line">        data = readDataFrom(<span class="string">"./train.dat"</span>)</span><br><span class="line">        <span class="comment"># print(data)</span></span><br><span class="line">        data_test = readDataFrom(<span class="string">"./test.dat"</span>)</span><br><span class="line">        W = ridge_regression_one_step(data, _lambda)</span><br><span class="line">        ein = Ein(data, W) / len(data)</span><br><span class="line">        eout = Ein(data_test, W) / len(data_test)</span><br><span class="line">        print(i,<span class="string">"Ein:"</span>, ein,<span class="string">"Eout:"</span>, eout)</span><br><span class="line">        <span class="keyword">if</span> ein&lt;=minEin:</span><br><span class="line">            minEin = ein</span><br><span class="line">            minEinI = i</span><br><span class="line">        <span class="keyword">if</span> eout &lt;= minEout:</span><br><span class="line">            minEout = eout</span><br><span class="line">            minEoutI = i</span><br><span class="line">    print(<span class="string">"minEin:"</span>,minEinI,<span class="string">"minEout:"</span>,minEoutI)</span><br></pre></td></tr></table></figure></p><p>å¾—åˆ°ç»“æœï¼š<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">-10</span> Ein: <span class="number">0.015</span> Eout: <span class="number">0.02</span></span><br><span class="line"><span class="number">-9</span> Ein: <span class="number">0.015</span> Eout: <span class="number">0.02</span></span><br><span class="line"><span class="number">-8</span> Ein: <span class="number">0.015</span> Eout: <span class="number">0.02</span></span><br><span class="line"><span class="number">-7</span> Ein: <span class="number">0.03</span> Eout: <span class="number">0.015</span></span><br><span class="line"><span class="number">-6</span> Ein: <span class="number">0.035</span> Eout: <span class="number">0.016</span></span><br><span class="line"><span class="number">-5</span> Ein: <span class="number">0.03</span> Eout: <span class="number">0.016</span></span><br><span class="line"><span class="number">-4</span> Ein: <span class="number">0.03</span> Eout: <span class="number">0.016</span></span><br><span class="line"><span class="number">-3</span> Ein: <span class="number">0.03</span> Eout: <span class="number">0.016</span></span><br><span class="line"><span class="number">-2</span> Ein: <span class="number">0.03</span> Eout: <span class="number">0.016</span></span><br><span class="line"><span class="number">-1</span> Ein: <span class="number">0.035</span> Eout: <span class="number">0.016</span></span><br><span class="line"><span class="number">0</span> Ein: <span class="number">0.035</span> Eout: <span class="number">0.02</span></span><br><span class="line"><span class="number">1</span> Ein: <span class="number">0.05</span> Eout: <span class="number">0.045</span></span><br><span class="line"><span class="number">2</span> Ein: <span class="number">0.24</span> Eout: <span class="number">0.261</span></span><br><span class="line">minEin: <span class="number">-8</span> minEout: <span class="number">-7</span></span><br></pre></td></tr></table></figure></p><p>å¯ä»¥çœ‹åˆ°Einæœ€å°çš„æ˜¯$\lambda = 10^{-8}$(ç›¸ç­‰å–æœ€å¤§çš„),å› æ­¤ç­”æ¡ˆé€‰d.</p><p><strong>15. Following the previous Question, among $\log_{10} \lambda= \left\{2, 1, 0, -1, \ldots, -8, -9, -10 \right\}$. What is the $\lambda$ with the minimum $E_{out}$? Compute $\lambda$ and its corresponding $E_{in}$ and $E_{out}$ then select the closest answer. Break the tie by selecting the largest $\lambda$.</strong></p><p>a. $log_{10}^{\lambda} = -1,E_{in} = 0.015,E_{out} = 0.015$</p><p>b. $log_{10}^{\lambda} = -3,E_{in} = 0.015,E_{out} = 0.015$</p><p>c. $log_{10}^{\lambda} = -5,E_{in} = 0.015,E_{out} = 0.030$</p><p>d. $log_{10}^{\lambda} = -7,E_{in} = 0.030,E_{out} = 0.015$</p><p>e. $log_{10}^{\lambda} = -9,E_{in} = 0.030,E_{out} = 0.030$</p><p>ç­”æ¡ˆåœ¨ä¸Šä¸ªé¢˜ç›®ä¸­å·²ç»å¾—åˆ°äº†ã€‚ç­”æ¡ˆé€‰d.</p><p><strong>16. Now split the given training examples in $\mathcal{D}$ to the first 120 examples for $\mathcal{D}_{\text{train}}$ and 80 for $\mathcal{D}_{\text{val}}$. $\textit{Ideally, you should randomly do the 120/80 split. Because the given examples are already randomly permuted, however, we would use a fixed split for the purpose of this problem.}$</strong></p><p><strong>Run the algorithm on $\mathcal{D}_{\text{train}}$ to get $g^-_\lambda$, and validate $g^-_\lambda$ with $\mathcal{D}_{\text{val}}$. Among $\log_{10} \lambda= \left\{2, 1, 0, -1, \ldots, -8, -9, -10 \right\}$. What is the $\lambda$ with the minimum $E_{train}(g^-_\lambda)$? Compute $\lambda$ and the corresponding $E_{train}(g^-_\lambda)$, $E_{val}(g^-_\lambda)$ and $E_{out}(g^-_\lambda)$ then select the closet answer. Break the tie by selecting the largest $\lambda$.</strong></p><p>a. $log _10^{\lambda} = 0,E_{train}(g_{\lambda}^-) = 0.000,E_{val}(g_{\lambda}^-) = 0.050,E_{out}(g_{\lambda}^-) = 0.025$</p><p>b. $log _10^{\lambda} = -2,E_{train}(g_{\lambda}^-) = 0.010,E_{val}(g_{\lambda}^-) = 0.050,E_{out}(g_{\lambda}^-) = 0.035$</p><p>c. $log _10^{\lambda} = -4,E_{train}(g_{\lambda}^-) = 0.000,E_{val}(g_{\lambda}^-) = 0.010,E_{out}(g_{\lambda}^-) = 0.025$</p><p>d. $log _10^{\lambda} = -6,E_{train}(g_{\lambda}^-) = 0.010,E_{val}(g_{\lambda}^-) = 0.010,E_{out}(g_{\lambda}^-) = 0.025$</p><p>e. $log _10^{\lambda} = -8,E_{train}(g_{\lambda}^-) = 0.000,E_{val}(g_{\lambda}^-) = 0.050,E_{out}(g_{\lambda}^-) = 0.025$</p><p>è¿™é“é¢˜ç›®ä¾ç„¶ç”¨ä¹‹å‰çš„ç¨‹åºå°±å¯ä»¥å®Œæˆï¼Œåªéœ€è¦ä¿®æ”¹ä¸»å‡½æ•°ã€‚è¿™é‡Œä½¿ç”¨åˆ°äº†éªŒè¯é›†ï¼Œéœ€è¦æ·»åŠ çš„å°±æ˜¯éªŒè¯ç›¸å…³çš„ä»£ç ã€‚<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    minEtrain = <span class="number">1</span></span><br><span class="line">    minEval = <span class="number">1</span></span><br><span class="line">    minEout = <span class="number">1</span></span><br><span class="line">    minEvalI = <span class="number">-1</span></span><br><span class="line">    minEtrainI = <span class="number">-1</span></span><br><span class="line">    minEoutI = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">-10</span>,<span class="number">3</span>):</span><br><span class="line">        _lambda = math.pow(<span class="number">10</span>,i)</span><br><span class="line">        data = readDataFrom(<span class="string">"./train.dat"</span>)</span><br><span class="line">        data_train = data[<span class="number">0</span>:<span class="number">120</span>]</span><br><span class="line">        data_val = data[<span class="number">120</span>:<span class="number">200</span>]</span><br><span class="line">        <span class="comment"># print(data)</span></span><br><span class="line">        data_test = readDataFrom(<span class="string">"./test.dat"</span>)</span><br><span class="line">        W = ridge_regression_one_step(data_train, _lambda)</span><br><span class="line">        etrain = Ein(data_train, W) / len(data_train)</span><br><span class="line">        eval = Ein(data_val,W)/len(data_val)</span><br><span class="line">        eout = Ein(data_test, W) / len(data_test)</span><br><span class="line">        print(i,<span class="string">"Etrain:"</span>, etrain,<span class="string">"Eval:"</span>,eval,<span class="string">"Eout:"</span>, eout)</span><br><span class="line">        <span class="keyword">if</span> etrain&lt;=minEtrain:</span><br><span class="line">            minEtrain = etrain</span><br><span class="line">            minEtrainI = i</span><br><span class="line">        <span class="keyword">if</span> eval &lt;= minEval:</span><br><span class="line">            minEval = eval</span><br><span class="line">            minEvalI = i</span><br><span class="line">        <span class="keyword">if</span> eout &lt;= minEout:</span><br><span class="line">            minEout = eout</span><br><span class="line">            minEoutI = i</span><br><span class="line">    print(<span class="string">"minEtrain:"</span>,minEtrainI,<span class="string">"minEval:"</span>,minEvalI,<span class="string">"minEout"</span>,minEoutI)</span><br></pre></td></tr></table></figure></p><p>æœ€åè¾“å‡ºå¦‚ä¸‹ï¼š<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">-10</span> Etrain: <span class="number">0.008333333333333333</span> Eval: <span class="number">0.125</span> Eout: <span class="number">0.04</span></span><br><span class="line"><span class="number">-9</span> Etrain: <span class="number">0.0</span> Eval: <span class="number">0.1</span> Eout: <span class="number">0.038</span></span><br><span class="line"><span class="number">-8</span> Etrain: <span class="number">0.0</span> Eval: <span class="number">0.05</span> Eout: <span class="number">0.025</span></span><br><span class="line"><span class="number">-7</span> Etrain: <span class="number">0.03333333333333333</span> Eval: <span class="number">0.0375</span> Eout: <span class="number">0.021</span></span><br><span class="line"><span class="number">-6</span> Etrain: <span class="number">0.03333333333333333</span> Eval: <span class="number">0.0375</span> Eout: <span class="number">0.021</span></span><br><span class="line"><span class="number">-5</span> Etrain: <span class="number">0.03333333333333333</span> Eval: <span class="number">0.0375</span> Eout: <span class="number">0.021</span></span><br><span class="line"><span class="number">-4</span> Etrain: <span class="number">0.03333333333333333</span> Eval: <span class="number">0.0375</span> Eout: <span class="number">0.021</span></span><br><span class="line"><span class="number">-3</span> Etrain: <span class="number">0.03333333333333333</span> Eval: <span class="number">0.0375</span> Eout: <span class="number">0.021</span></span><br><span class="line"><span class="number">-2</span> Etrain: <span class="number">0.03333333333333333</span> Eval: <span class="number">0.0375</span> Eout: <span class="number">0.021</span></span><br><span class="line"><span class="number">-1</span> Etrain: <span class="number">0.03333333333333333</span> Eval: <span class="number">0.0375</span> Eout: <span class="number">0.022</span></span><br><span class="line"><span class="number">0</span> Etrain: <span class="number">0.03333333333333333</span> Eval: <span class="number">0.0375</span> Eout: <span class="number">0.028</span></span><br><span class="line"><span class="number">1</span> Etrain: <span class="number">0.075</span> Eval: <span class="number">0.125</span> Eout: <span class="number">0.08</span></span><br><span class="line"><span class="number">2</span> Etrain: <span class="number">0.3416666666666667</span> Eval: <span class="number">0.4125</span> Eout: <span class="number">0.414</span></span><br><span class="line">minEtrain: <span class="number">-8</span> minEval: <span class="number">0</span> minEout <span class="number">-2</span></span><br></pre></td></tr></table></figure></p><p>å› æ­¤ç­”æ¡ˆé€‰æ‹©e.</p><p><strong>17. Following the previous Question, among $\log_{10} \lambda= \left\{2, 1, 0, -1, \ldots, -8, -9, -10 \right\}$.What is the $\lambda$ with the minimum $E_{val}(g^-_\lambda)$? Compute $\lambda$ and the corresponding $E_{train}(g^-_\lambda)$, $E_{val}(g^-_\lambda)$ and $E_{out}(g^-_\lambda)$ then select the closet answer. Break the tie by selecting the largest $\lambda$.</strong></p><p>a. $log _10^{\lambda} = 0,E_{train}(g_{\lambda}^-) = 0.033,E_{val}(g_{\lambda}^-) = 0.038,E_{out}(g_{\lambda}^-) = 0.028$</p><p>b. $log _10^{\lambda} = -3,E_{train}(g_{\lambda}^-) = 0.000,E_{val}(g_{\lambda}^-) = 0.028,E_{out}(g_{\lambda}^-) = 0.038$</p><p>c. $log _10^{\lambda} = -6,E_{train}(g_{\lambda}^-) = 0.066,E_{val}(g_{\lambda}^-) = 0.038,E_{out}(g_{\lambda}^-) = 0.038$</p><p>d. $log _10^{\lambda} = -9,E_{train}(g_{\lambda}^-) = 0.033,E_{val}(g_{\lambda}^-) = 0.028,E_{out}(g_{\lambda}^-) = 0.028$</p><p>e. $log _10^{\lambda} = -10,E_{train}(g_{\lambda}^-) = 0.066,E_{val}(g_{\lambda}^-) = 0.028,E_{out}(g_{\lambda}^-) = 0.028$</p><p>ç­”æ¡ˆåœ¨ä¸Šé¢å·²ç»ç»™å‡ºã€‚ç­”æ¡ˆé€‰æ‹©a.</p><p><strong>18. Run the algorithm with the optimal $\lambda$ of the previous Question on the whole $\mathcal{D}$ to get $g_\lambda$. Compute $E_{in}(g_\lambda)$ and $E_{out}(g_\lambda)$ then select the closet answer.</strong></p><p>a. $E_{in}(g_{\lambda}) = 0.015,E_{out}(g_{\lambda}) = 0.020$</p><p>b. $E_{in}(g_{\lambda}) = 0.025,E_{out}(g_{\lambda}) = 0.030$</p><p>c. $E_{in}(g_{\lambda}) = 0.035,E_{out}(g_{\lambda}) = 0.020$</p><p>d. $E_{in}(g_{\lambda}) = 0.045,E_{out}(g_{\lambda}) = 0.030$</p><p>e. $E_{in}(g_{\lambda}) = 0.055,E_{out}(g_{\lambda}) = 0.020$ </p><p>æ ¹æ®17é¢˜é€‰å‡ºæ¥æœ€ä½³çš„$\lambda = 0$ï¼Œå› æ­¤åœ¨å…¨æ•°æ®é›†ä¸Šå†æ¬¡è¿›è¡Œå­¦ä¹ ï¼Œå¾—åˆ°çš„ç»“æœåœ¨14é¢˜çš„åˆ†æä¸­å·²ç»å‘ˆç°ï¼Œç­”æ¡ˆæ˜¯c.</p><p>å¯ä»¥çœ‹åˆ°çš„æ˜¯åˆ©ç”¨éªŒè¯ï¼Œæˆ‘ä»¬é€‰å‡ºæ¥äº†ä¸€ä¸ªå¾ˆè´´è¿‘æœ€ä½³$E_{out}$çš„ç­”æ¡ˆäº†ã€‚</p><p>For Questions 19-20, split the given training examples in $\mathcal{D}$ to five folds, the first $40$ being fold 1, the next $40$ being fold 2, and so on. Again, we take a fixed split because the given examples are already randomly permuted.</p><p><strong>19.  What is the $Î»$ with the minimum $E_{cv}$, where $E_{cv}$ comes from the five folds defined above? Compute $\lambda$ and the corresponding $E_{cv}$ then select the closet answer. Break the tie by selecting the largest $\lambda$.</strong></p><p>a. $log_{10}^{\lambda} = 0,E_{cv} = 0.030$</p><p>b. $log_{10}^{\lambda} = -2,E_{cv} = 0.020$</p><p>c. $log_{10}^{\lambda} = -4,E_{cv} = 0.030$</p><p>e. $log_{10}^{\lambda} = -6,E_{cv} = 0.020$</p><p>d. $log_{10}^{\lambda} = -8,E_{cv} = 0.030$</p><p>è¿™ä¸ªé¢˜ç›®è¦åšäº¤å‰éªŒè¯ã€‚å› æ­¤éœ€è¦å†™ä¸€ä¸ªæ–°çš„äº¤å‰éªŒè¯çš„å‡½æ•°cvã€‚åŒæ—¶ä¹Ÿè¦éœ€è¦ä¿®æ”¹ä¸»å‡½æ•°ã€‚<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#æ·»åŠ çš„å‡½æ•°cv</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cv</span><span class="params">(data,fold_count,_lambda)</span>:</span></span><br><span class="line">    <span class="comment"># disorder data</span></span><br><span class="line">    ecv = <span class="number">0</span></span><br><span class="line">    each_c = len(data)/fold_count</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(fold_count):</span><br><span class="line">        val = data[int(i*each_c):int((i+<span class="number">1</span>)*each_c)]</span><br><span class="line">        train = data[<span class="number">0</span>:int(i*each_c)]</span><br><span class="line">        train.extend(data[int((i+<span class="number">1</span>)*each_c):<span class="number">-1</span>])</span><br><span class="line">        W = ridge_regression_one_step(train,_lambda)</span><br><span class="line">        ecv +=Ein(val,W)/len(val)</span><br><span class="line">    <span class="keyword">return</span> ecv/fold_count</span><br></pre></td></tr></table></figure></p><p>æœ€åå¾—åˆ°ç»“æœå¦‚ä¸‹ï¼š<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">-10</span> Ecv: <span class="number">0.05</span></span><br><span class="line"><span class="number">-9</span> Ecv: <span class="number">0.05</span></span><br><span class="line"><span class="number">-8</span> Ecv: <span class="number">0.03</span></span><br><span class="line"><span class="number">-7</span> Ecv: <span class="number">0.034999999999999996</span></span><br><span class="line"><span class="number">-6</span> Ecv: <span class="number">0.034999999999999996</span></span><br><span class="line"><span class="number">-5</span> Ecv: <span class="number">0.034999999999999996</span></span><br><span class="line"><span class="number">-4</span> Ecv: <span class="number">0.034999999999999996</span></span><br><span class="line"><span class="number">-3</span> Ecv: <span class="number">0.034999999999999996</span></span><br><span class="line"><span class="number">-2</span> Ecv: <span class="number">0.034999999999999996</span></span><br><span class="line"><span class="number">-1</span> Ecv: <span class="number">0.034999999999999996</span></span><br><span class="line"><span class="number">0</span> Ecv: <span class="number">0.034999999999999996</span></span><br><span class="line"><span class="number">1</span> Ecv: <span class="number">0.06</span></span><br><span class="line"><span class="number">2</span> Ecv: <span class="number">0.28500000000000003</span></span><br><span class="line">minEcv: <span class="number">-8</span></span><br></pre></td></tr></table></figure></p><p>å› æ­¤ç­”æ¡ˆé€‰æ‹©d.</p><p><strong>20. Run the algorithm with the optimal $\lambda$ of the previous problem on the whole $\mathcal{D}$ to get $g_\lambda$. Compute $E_{in}(g_\lambda)$ and $E_{out}(g_\lambda)$ then select the closet answer.</strong></p><p>a. $E_{in}(g_\lambda) = 0.005,E_{out}(g_\lambda) = 0.010$</p><p>b. $E_{in}(g_\lambda) = 0.015,E_{out}(g_\lambda) = 0.020$</p><p>c. $E_{in}(g_\lambda) = 0.025,E_{out}(g_\lambda) = 0.020$</p><p>d. $E_{in}(g_\lambda) = 0.035,E_{out}(g_\lambda) = 0.030$</p><p>e. $E_{in}(g_\lambda) = 0.045,E_{out}(g_\lambda) = 0.020$</p><p>ä¸Šé¢å¾—åˆ°çš„æœ€å¥½çš„$\lambda = 10^{-8}$ï¼Œå…¨éƒ¨æ•°æ®å»å­¦ä¹ çš„è¯ï¼Œå¾—åˆ°çš„$E_{in}$å’Œ$E_{out}$åœ¨14é¢˜ç›®è§£æä¸­ä¹Ÿèƒ½çœ‹åˆ°,ç­”æ¡ˆé€‰bã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> homework </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”ï¼ˆåŸºçŸ³ï¼‰æ€»ç»“</title>
      <link href="/2018/10/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%EF%BC%88%E5%9F%BA%E7%9F%B3%EF%BC%89%E6%80%BB%E7%BB%93/"/>
      <url>/2018/10/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%EF%BC%88%E5%9F%BA%E7%9F%B3%EF%BC%89%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<p>åˆ°äº†ç°åœ¨æœºå™¨å­¦ä¹ åŸºçŸ³çš„è¯¾ç¨‹å°±ç»“æŸäº†ã€‚æœ€åæœ‰ä¸€äº›å®é™…åˆ©ç”¨å­¦ä¹ çš„åŸåˆ™å’Œå°tipsï¼Œç”¨æ¥ä½œä¸ºæ€»ç»“ã€‚<br><a id="more"></a></p><h2 id="Occamâ€™s-Razor"><a href="#Occamâ€™s-Razor" class="headerlink" title="Occamâ€™s Razor"></a>Occamâ€™s Razor</h2><p>Entities must not be multiplied beyond necessity.</p><p>ä½¿ç”¨çš„æ¨¡å‹è¦å°½é‡ç®€å•ã€‚æˆ‘ä»¬çŸ¥é“è¶Šå¤æ‚çš„æ¨¡å‹å¯ä»¥è§£é‡Šçš„æ ·æœ¬æ›´å¤šï¼Œä½†æ˜¯ä¸€ä¸ªæ ·æœ¬é›†å¦‚æœè¢«ç®€å•çš„æ¨¡å‹æ‰€è§£é‡Šï¼Œé‚£ä¹ˆä»å¦ä¸€æ–¹é¢æ¥è¯´è¿™ä¸ªæ•°æ®é›†æ›´å¯èƒ½å­˜åœ¨ä¸€å®šçš„è§„å¾‹æ€§ã€‚å› ä¸ºå³ä½¿æ˜¯éšæœºäº§ç”Ÿçš„æ ·æœ¬ï¼Œå¤æ‚çš„æ¨¡å‹ä¾ç„¶å¯ä»¥è§£é‡Šã€‚è€Œå¯¹äºå®Œå…¨éšæœºçš„æ ·æœ¬æ˜¯æ²¡æœ‰å­¦ä¹ çš„å¿…è¦çš„ï¼ˆå¯è§No Free Lunchå®šç†ï¼‰ã€‚å› æ­¤æˆ‘ä»¬å°½é‡ä½¿ç”¨ç®€å•çš„æ¨¡å‹å»è§£é‡Šæ ·æœ¬ï¼Œåœ¨æ³›åŒ–èƒ½åŠ›ä¸Šç­‰ç­‰ç›¸å¯¹äºå¤æ‚æ¨¡å‹æ¥è¯´éƒ½ä¼šæ›´å¥½ã€‚è¶Šç®€å•è¶Šå¥½ã€‚</p><h2 id="Sampling-Bias"><a href="#Sampling-Bias" class="headerlink" title="Sampling Bias"></a>Sampling Bias</h2><p>1948å¹´ç¾å›½æ€»ç»Ÿå¤§é€‰ï¼ŒTruman versus Dewey.ä¸€å®¶æŠ¥ç¤¾ä¸ºäº†æå‰é¢„æµ‹å¤§é€‰ç»“æœï¼Œåˆ©ç”¨ç”µè¯è¿›è¡Œæ°‘æ„è°ƒæŸ¥ï¼Œå¾—åˆ°çš„ç»“æœæ˜¯Dewey Defeat Trumanï¼Œç„¶è€Œæœ€åçš„ç»“æœæ˜¯Trumanèµ¢å¾—äº†å¤§é€‰ã€‚è€Œæ°‘æ„è°ƒæŸ¥æ—¢æ²¡æœ‰å‡ºé”™ï¼Œä¹Ÿæ²¡æœ‰å‡ºç°è¿æ°”ä¸å¥½çš„æƒ…å†µï¼Œä¸ºä½•ä¼šè¿™æ ·ï¼Ÿå½“å¹´ç”µè¯æ˜¯æ¯”è¾ƒæ˜‚è´µçš„ï¼Œå› æ­¤æ¥å—æ°‘æ„è°ƒæŸ¥çš„éƒ½æ˜¯æ¯”è¾ƒå¯Œæœ‰çš„é˜¶çº§ï¼Œåœ¨è¿™äº›äººä¸­å¯¹Deweyçš„æ”¯æŒç‡æ›´é«˜ï¼Œä½†æ˜¯å äº†å¤§å¤šæ•°äººçš„ä¸­ä¸‹é˜¶çº§å¯¹Trumançš„æ”¯æŒæ›´å¥½ï¼Œå› æ­¤å‡ºç°äº†è¿™æ ·çš„ç»“æœã€‚</p><p>åœ¨æˆ‘ä»¬æ‰€æœ‰çš„ä¸Šè¿°çš„è¿™äº›ç†è®ºæ¨è®ºä¸­ï¼Œæˆ‘ä»¬éƒ½å‡è®¾è®­ç»ƒä¸æµ‹è¯•é›†ï¼Œéƒ½æ¥è‡ªåŒä¸€åˆ†å¸ƒã€‚è€Œç°å®ä¸­ï¼Œæ¯”å¦‚ä¸Šé¢çš„ä¾‹å­ï¼Œè¿™æ ·çš„æƒ…å†µå¹¶ä¸æ˜¯ç»å¸¸ä¼šç¢°åˆ°ã€‚ä¿è¯æˆ‘ä»¬è·å¾—çš„è®­ç»ƒé›†ä¸çœŸå®çš„æ•°æ®æ¥è‡ªåŒä¸€åˆ†å¸ƒæ˜¯æ¯”è¾ƒå›°éš¾çš„ã€‚å› æ­¤åœ¨å®é™…æ“ä½œä¸­æˆ‘ä»¬è¦æ³¨æ„å°½å¯èƒ½è®©è®­ç»ƒé›†ä¸æµ‹è¯•é›†æ¥è‡ªåŒä¸€åˆ†å¸ƒã€‚æ¯”å¦‚åœ¨é¢„æµ‹ç”µå½±æ—¶ï¼Œå¯èƒ½æ¯ä¸ªäººçœ‹çš„å‰7éƒ¨ç”µå½±æ˜¯è®­ç»ƒé›†ï¼Œåä¸‰æ­¥ç”¨æ¥æµ‹è¯•ï¼Œè¿™å°±ä¸æ˜¯åŒä¸€åˆ†å¸ƒï¼Œå› ä¸ºè§‚å½±é¡ºåºå¯èƒ½æ˜¯å¾ˆé‡è¦çš„,æœ‰ä¸€å®šçš„æ„å‘ï¼Œè€Œä¸æ˜¯éšæœºçš„ã€‚å› æ­¤åœ¨å­¦ä¹ æ—¶éœ€è¦é€‚å½“åŠ é‡åé¢å‡ éƒ¨ç”µå½±çš„æƒé‡ï¼Œæˆ–è€…åˆ©ç”¨åé¢å‡ éƒ¨æ¥åšéªŒè¯é›†ã€‚æ€»ä¹‹ï¼ŒSampling Biasä¹Ÿæ˜¯å¸¸è§çš„å½±å“å­¦ä¹ æ•ˆæœçš„åŸå› ã€‚</p><h2 id="Data-Snooping"><a href="#Data-Snooping" class="headerlink" title="Data Snooping"></a>Data Snooping</h2><p>æ•°æ®â€œå·çª¥â€ã€‚è¿™æ˜¯ä¹‹å‰ä¹Ÿä¸€ç›´æåˆ°çš„ï¼Œå¦‚ç”¨è‡ªå·±çš„çœ¼ç›æ¥å†³å®šå­¦ä¹ ç®—æ³•ã€‚è¿™æ—¶å€™å¯èƒ½ä¼šç®—ä¸Šæ„å¤–çš„VC dimensionï¼Œä½†æ˜¯æˆ‘ä»¬å´ä¸çŸ¥é“ã€‚ä½†æ˜¯å®é™…ä¸Šï¼Œä¸å…‰ç”¨çœ¼ç›ï¼Œå¾ˆå¤šæ—¶å€™Data snoopingå¾ˆéš¾ä»¥é¿å…ï¼Œä¸€äº›å°å°çš„åŠ¨ä½œå°±ä¼šå½±å“åˆ°è¿™äº›ä¸œè¥¿ã€‚æ¯”å¦‚ä¸€ç›´åœ¨å‰äººçš„æ¨¡å‹ä¸Šæ”¹è¿›å‘è¡¨è®ºæ–‡ï¼Œè™½ç„¶ä½ å¯èƒ½æ²¡æœ‰å»çœ‹æ•°æ®ï¼Œä½†æ˜¯ç¡®å®åˆ©ç”¨äº†ä¹‹å‰çš„æ¨¡å‹ï¼Œå®ƒå°±åœ¨ä¸€å®šç¨‹åº¦ä¸ŠåŒ…å«äº†æ•°æ®çš„ä¿¡æ¯ã€‚å·çª¥æ•°æ®çš„ç»“æœå°±æ˜¯ä½¿å¾—è‡ªå·±å­¦ä¹ çš„æ¨¡å‹è¿‡äºä¹è§‚ï¼Œå¯èƒ½ä¼šé€ æˆoverfittingï¼Œè¿™ä¹Ÿæ˜¯overfittingå¾ˆéš¾å¤„ç†çš„ä¸€ä¸ªåŸå› ã€‚</p><p>æœ‰å‡ ä¸ªå°çš„å»ºè®®ï¼Œå°±æ˜¯åœ¨ä¸çœ‹æ•°æ®ä¹‹å‰ï¼Œæƒ³å¥½éœ€è¦ç”¨åˆ°çš„ç‰¹å¾é‡ï¼›ä¿è¯æµ‹è¯•æ•°æ®çš„å°é”ï¼Œä½¿å…¶ä¸å—åˆ°æ±¡æŸ“ã€‚å¯¹äºé¿å…Data Snoopingçš„åšæ³•å¯èƒ½éœ€è¦å¤šå¹´çš„ç»éªŒï¼Œå…¶ä¸­çš„æœºåˆ¶å› ä¸ºç»„åˆå¤ªå¤šå¾ˆå¤æ‚ï¼Œå› æ­¤è¿™ä¸ªéœ€è¦æ…¢æ…¢ä½“ä¼šã€‚</p><p>æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½ï¼Œæ•°æ®æŒ–æ˜ï¼Œç»Ÿè®¡ç†è®ºçš„äº¤é›†ï¼Œå®ƒä¸ºä»Šå¤©äººç±»ç”Ÿæ´»å¸¦æ¥äº†å¾ˆå¤šä¾¿åˆ©ï¼Œå¦‚è¯­éŸ³å¤„ç†äººè„¸è¯†åˆ«ç­‰ç­‰ã€‚å¸Œæœ›è‡ªå·±å¯ä»¥å­¦å¥½è¿™é—¨è¯¾ç¨‹å¹¶ä¸”ä»˜è¯¸å®è·µã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> tips </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”Validation</title>
      <link href="/2018/09/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Validation/"/>
      <url>/2018/09/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Validation/</url>
      
        <content type="html"><![CDATA[<p>ä¸Šæ¬¡regularizationæœ€åç•™ä¸‹äº†ä¸€ä¸ªé—®é¢˜ï¼š$\lambda$çš„é€‰æ‹©ã€‚å…¶å®ä»”ç»†æƒ³æƒ³ï¼Œä»å­¦ä¹ æœºå™¨å­¦ä¹ åˆ°ç°åœ¨ï¼Œæˆ‘ä»¬é¢ä¸´çš„é€‰æ‹©ï¼Œå¯ä¸æ­¢ä¸€ä¸ª$\lambda$.<br><a id="more"></a></p><p>æ¨¡å‹çš„é€‰æ‹©ï¼ˆPLAï¼ŒPOKCETï¼ŒLinear Regressionï¼ŒLogistic Regressionï¼‰ï¼Œç‰¹å¾è½¬æ¢çš„æ–¹æ³•ï¼ˆç”¨ä»€ä¹ˆæ ·çš„å¤šé¡¹å¼è½¬æ¢ï¼‰ï¼ŒRegularizerçš„é€‰æ‹©ç­‰ç­‰ï¼Œè¿™äº›ç»„åˆèµ·æ¥è¶³å¤Ÿè®©äººå¤´å¤§ã€‚è€Œå®é™…ä¸Šï¼Œä¹Ÿæ²¡æœ‰ä¸€ç§æ°¸è¿œéƒ½è¡¨ç°å¾—å¾ˆå¥½çš„ç»„åˆï¼Œå¯¹äºä¸åŒçš„é—®é¢˜éœ€è¦ä¸åŒçš„æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯åšé€‰æ‹©æ˜¯å¿…é¡»çš„ã€‚</p><p>ä¸ºäº†ç®€åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬å°±ä»…ä»¥ä¸åŒçš„å‡è¯´çš„é€‰æ‹©ä¸ºä¾‹ã€‚å…¶ä»–çš„é€‰æ‹©ä¹Ÿä¸è¿™ä¸ªç±»ä¼¼ã€‚</p><p>å‡è®¾æˆ‘ä»¬ç›®å‰æœ‰2ä¸ª$H$ï¼Œä¸€ä¸ª$H_2$,å¦ä¸€ä¸ª$H_{10}$ï¼Œåº”è¯¥é€‰æ‹©å“ªä¸€ä¸ªï¼Ÿ</p><p>ä¸€ä¸ªç®€å•çš„æƒ³æ³•ï¼Œæ˜¯åˆ©ç”¨$E_{in}$å»åˆ¤æ–­ã€‚ä½†æ˜¯è¿™ä¸ªæƒ³æ³•å¤ªnaiveäº†ã€‚æˆ‘ä»¬ä¹‹å‰è®²è¿‡è¿‡æ‹Ÿåˆäº†ï¼Œå¦‚æœç”¨$E_{in}$å»åˆ¤æ–­ï¼Œé‚£å°±ä¸ç”¨æƒ³äº†ï¼Œå› ä¸º$H_{10}$ä¸€å®šæ¯”$H_2$è¦å¥½ï¼Œè€Œä¸”å¦‚æœä¸¤ä¸ªæ¨¡å‹ä¸€ä¸ªåŠ äº†$regularization$ï¼Œå®ƒçš„è¡¨ç°ä¸€å®šä¸å¦‚å¦ä¸€ä¸ª.è€Œä¸”å¦‚æœä¸¤ä¸ª$H$æ²¡æœ‰äº¤é›†ï¼Œä»ä¸¤ä¸ª$E_{in}$ä¸­é€‰æ‹©ä¸€ä¸ªå¥½çš„ï¼Œé‚£ä¹ˆå®é™…ä¸Šæ˜¯åœ¨ä¸¤ä¸ª$H$çš„å¹¶é›†ä¸­é€‰æ‹©ï¼Œè¿™ä¹Ÿå°±æ„å‘³ç€å¢å¤§äº†ä»£ä»·ï¼Œæ›´å®¹æ˜“å¾—åˆ°ä¸å¥½çš„$E_{out}$ã€‚</p><p>å¦ä¸€ç§ç®€å•çš„æƒ³æ³•ï¼Œç”¨æµ‹è¯•æ•°æ®æ¥åˆ¤æ–­ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„åŠæ³•ã€‚æˆ‘ä»¬çŸ¥é“$E_{test}$ä¸$E_{out}$æ˜¯æ»¡è¶³éœå¤«ä¸ä¸ç­‰å¼çš„ï¼Œå› æ­¤å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">E_{out} \leq E_{test}+O(\sqrt {\frac{\log M}{N_{test}}})</script><p>æ‰€ä»¥ç”¨æµ‹è¯•é›†æ¥ä»æ¨¡å‹ä¸­é€‰æ‹©ä¸€ä¸ªæœ€å¥½çš„æ˜¯å¯è¡Œçš„ã€‚ä½†æ˜¯æµ‹è¯•é›†ä»å“ªé‡Œæ¥ï¼Ÿ</p><p>ä¸€èˆ¬æ¥è¯´ï¼Œæµ‹è¯•é›†æ˜¯é”åœ¨è€æ¿çš„æŸœå­ä¸­ã€‚æµ‹è¯•é›†ç›¸å½“äºè€ƒè¯•è¯•å·ï¼Œç”¨æ¥è¯„åˆ¤æœ€ç»ˆçš„åˆ†æ•°.æˆ‘ä»¬æ— æ³•å¾—åˆ°æµ‹è¯•é›†ï¼Œè¿™å°±åƒè€ƒè¯•å‰ä½ æƒ³è®©è‡ªå·±åšåˆ°æœ€å¥½ï¼Œä½ æ²¡æ³•ç”¨è€ƒè¯•çš„å·å­æ¥æµ‹è¯•è‡ªå·±ï¼Œè¿™å«ä½œå¼Šã€‚</p><p>ä½†æ˜¯æˆ‘ä»¬å¯ä»¥è‡ªå·±æµ‹éªŒè‡ªå·±ã€‚è¿™å°±æ˜¯validationã€‚</p><p>ä»ç»™åˆ°çš„è®­ç»ƒé›†å½“ä¸­ï¼Œæˆ‘ä»¬éšæœºæŒ‘å‡ºä¸€éƒ¨åˆ†ï¼ˆä¿è¯iidï¼‰ï¼Œç”¨æ¥å½“ä½œvalé›†ï¼Œå…¶ä½™éƒ¨åˆ†ç”¨æ¥è®­ç»ƒæ¨¡å‹ã€‚ç„¶åé€šè¿‡valé›†æ¥é€‰å‡ºè¡¨ç°æœ€å¥½çš„gâ€™.ä¸ºä»€ä¹ˆä¸æ˜¯gï¼Ÿå› ä¸ºæ¯•ç«Ÿå®ƒçš„è®­ç»ƒé›†ç›¸å¯¹äºä¹‹å‰è¦å°‘äº†å¾ˆå¤šï¼Œæ‰€ä»¥åŠ ä¸ªæ ‡è¯†ä»¥åŒºåˆ†ã€‚</p><p>ä¸€èˆ¬æ¥è¯´ï¼Œå¾—åˆ°gâ€™ä»¥åï¼Œä¹Ÿå°±æ˜¯é€‰å‡ºäº†æˆ‘ä»¬æƒ³è¦çš„é‚£ä¸ª$H$ï¼Œç„¶åæˆ‘ä»¬è¦åšçš„å°±æ˜¯å°†éªŒè¯é›†å†æ¬¡èåˆå›å»ï¼Œç”¨è¿™ä¸ªæ•´ä½“çš„è®­ç»ƒé›†åœ¨è¯¥å‡è¯´ä¸Šè®­ç»ƒã€‚æ¯•ç«ŸæŸç§ç¨‹åº¦ä¸Šæ¥è¯´ï¼ŒNè¶Šå¤§ï¼Œå¾—åˆ°çš„æ¨¡å‹æ˜¯è¶Šå¥½çš„ã€‚è€Œä¸”</p><script type="math/tex; mode=display">E_{out}(g_{H_{chosen}}) < E_{out}(g'_{H_{chosen}})</script><p>ä¸Šå¼æ˜¯å®é™…ä¸­çš„ä¸€ä¸ªå¾ˆå¸¸è§çš„å¼å­ï¼Œä½†ç†è®ºä¸Šè¦è¿™ä¹ˆä¿è¯è¿˜éœ€è¦ä¸€å®šçš„é™åˆ¶æ¡ä»¶ã€‚</p><p>å‡è®¾æˆ‘ä»¬ä»Nä¸ªè®­ç»ƒæ ·æœ¬ä¸­æŒ‘å‡ºKä¸ªæ¥åšéªŒè¯é›†ï¼Œå½“ç„¶è¿™ä¸ªKçš„å¤§å°æ˜¯ä¼šå½±å“ç»“æœçš„ã€‚å¦‚æœKå¾ˆå¤§ï¼Œé‚£ä¹ˆå¾ˆå¼€å¿ƒï¼Œ$E_{val}$ä¸$E_{out}$æ›´æœ‰å¯èƒ½å¾ˆæ¥è¿‘ï¼Œè¿™å¯¹äºé€‰æ‹©gâ€™æ¥è¯´æ˜¯å¾ˆå¥½çš„ï¼Œä½†æ˜¯Kè¿‡å¤§æ„å‘³ç€ç•™ä¸‹æ¥çš„è®­ç»ƒæ ·æœ¬è¿‡å°‘ï¼Œgâ€™ä¸gå·®åˆ«å¾ˆå¤§ï¼Œå¯èƒ½å¯¼è‡´æˆ‘ä»¬æ— æ³•æ‰¾åˆ°åº”è¯¥é€‰æ‹©çš„é‚£ä¸ª$H$ï¼›å¦ä¸€æ–¹é¢ï¼ŒKå¾ˆå°ï¼Œgä¸gâ€™ç›¸å·®å¾ˆå°ï¼Œä½†æ˜¯$E_{val}$ä¸$E_{out}$å¯èƒ½å®é™…ä¸Šå·®çš„å¾ˆè¿œï¼Œä¹Ÿä¸èƒ½å¾—åˆ°å¥½çš„ç»“æœã€‚å› æ­¤è¿™åˆæ˜¯ä¸€ä¸ªéš¾é¢˜ã€‚<br>ä¸€èˆ¬æ¥è¯´æœ‰ä¸‹å›¾ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/%7ELN%7EMS5IB6G61%40DWBDM51OE.png" alt=""><br>å¯ä»¥çœ‹åˆ°å¦‚æœKè¿‡å¤§ï¼Œå¯¼è‡´è®­ç»ƒgâ€™çš„è®­ç»ƒé›†å¾ˆå°ï¼Œä½¿å¾—å®ƒçš„å­¦ä¹ æ•ˆæœå¾ˆå·®ï¼Œç”šè‡³ä¸å¦‚ä¸ç”¨éªŒè¯çš„æƒ…å†µã€‚</p><p>ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¼•å…¥ä¸€ç§æ–°çš„å·¥å…·ï¼šäº¤å‰éªŒè¯ã€‚</p><p>é¦–å…ˆæˆ‘ä»¬è€ƒè™‘ä¸€ç§å¾ˆæç«¯çš„æƒ…å†µï¼šK=1.æ¯æ¬¡ç•™å‡ºä¸€ä¸ªæ¥åšéªŒè¯ï¼Œå¯¹äºå•ä¸ªæ ·æœ¬æ¥è¯´å®ƒå½“ç„¶æ— æ³•ä»£è¡¨$E_{out}$.ä½†æ˜¯å¦‚æœæˆ‘ä»¬å¯¹è¿™ä¸ªè¿‡ç¨‹è¿›è¡ŒNæ¬¡ï¼Œæ‰€æœ‰çš„æ ·æœ¬éƒ½æ›¾åšè¿‡éªŒè¯é›†ï¼Œæœ€åæ±‚å‡ºæ¥$E_{val}$çš„å¹³å‡å€¼ï¼Œå¯ä»¥è¯æ˜å®ƒå°±èƒ½ä»£è¡¨gâ€™çš„$E_{out}$.ä¸Šé¢çš„åŠæ³•ï¼Œå«åšleave-one-out cross validation.å‡è®¾å®ƒå¾—åˆ°çš„é”™è¯¯æˆ‘ä»¬ç§°ä¸º$E_{looc}$ï¼Œåˆ™æœ‰ä¸‹é¢çš„è¯æ˜ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/VQVY7%5B%60%5BQA%40AT%7BQ_S24LDQ4.png" alt=""><br>(ä¸Šé¢çš„è¯æ˜æˆ‘æ˜¯çœ‹ä¸å¤§æ‡‚çš„)</p><p>è¿™ä¼¼ä¹æ˜¯ä¸ªå¾ˆå¥½çš„æ–¹æ³•ï¼Œä½†æ˜¯å®ƒæœ‰ä¸ªå¾ˆè‡´å‘½çš„ç¼ºé™·ï¼šè®¡ç®—é‡ï¼ˆNï¼‰å€çš„åŠ›æ°”å»è®¡ç®—gâ€™ã€‚</p><p>å› æ­¤ï¼Œå®é™…ä¸­æˆ‘ä»¬å¾ˆå°‘ç”¨leave one out crossï¼Œè€Œä½¿ç”¨V-Fold Crossã€‚å°†æ ·æœ¬åˆ†ä¸º10ï¼ˆæˆ–è€…å…¶ä»–æ•°ï¼‰ä»½ï¼Œç„¶åç•™ä¸€ä»½ä½œä¸ºvalé›†ï¼Œåƒä¸Šé¢ä¸€æ ·äº¤å‰éªŒè¯ã€‚è¿™æ ·éœ€è¦çš„åŠ›æ°”å°±æ˜¯10å€ï¼Œå¯ä»¥æ¥å—ï¼Œè€Œä¸”èƒ½å¾—åˆ°æ¯”éäº¤å‰éªŒè¯æ›´å¥½çš„ç»“æœã€‚</p><p>å¯¹äºå…¶ä»–æƒ…å†µçš„é€‰æ‹©ï¼Œä¹Ÿå¯ä»¥ç”¨è¿™æ ·çš„åŠæ³•ï¼Œå› ä¸ºæˆ‘ä»¬æœ€ç»ˆç›®çš„æ˜¯å¾—åˆ°å°½é‡å¥½çš„$E_{out}$ã€‚</p><p>æœ€åï¼Œvalidationä¾ç„¶æ˜¯ä¸ºäº†åšé€‰æ‹©ï¼Œå› æ­¤å®ƒçš„ç»“æœä¾ç„¶æ˜¯æ¯”è¾ƒoptimisticï¼Œç®—æ³•æœ€ç»ˆçš„è¡¡é‡è¿˜æ˜¯è¦é€šè¿‡æµ‹è¯•é›†ï¼Œè€Œä¸æ˜¯å°†æœ€å¥½çš„validationç»“æœä½œä¸ºè¡¡é‡æ ‡å‡†ï¼Œè¿™æ˜¯è‡ªæ¬ºæ¬ºäººçš„è¡¨ç°ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> validation </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”Regularization</title>
      <link href="/2018/09/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Regularization/"/>
      <url>/2018/09/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Regularization/</url>
      
        <content type="html"><![CDATA[<p>ä¸Šç¯‡åšå®¢è¯´äº†overfittingçš„æƒ…å†µï¼Œæœ‰ä¸€äº›æ¯”è¾ƒé«˜çº§çš„å¤„ç†overfittingçš„åŠæ³•ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªå°±æ˜¯regularizationï¼Œä¸­æ–‡ä¸­å«åšæ­£åˆ™åŒ–ã€‚<br><a id="more"></a></p><p>ä»å‰é¢çš„nonlinear transformä¸­ä¹Ÿè¯´æ˜äº†ï¼Œå¤æ‚çš„å‡è¯´ä¸€èˆ¬ä¼šåŒ…æ‹¬äº†ç®€å•çš„å‡è¯´ã€‚ä¾‹å¦‚ä¸€ä¸ª2æ¬¡çš„å‡è¯´ï¼Œä¸10æ¬¡çš„å‡è¯´ï¼Œä»–ä»¬çš„åŒºåˆ«ï¼Œå°±æ˜¯10æ¬¡çš„æ¯”äºŒæ¬¡çš„å¤šäº†æ›´å¤šä¸‰æ¬¡åŠä»¥ä¸Šçš„ç‰¹å¾ã€‚ä¹Ÿå°±æ˜¯äºŒæ¬¡å‡è¯´å®é™…ä¸Šæ˜¯åæ¬¡å‡è¯´åŠ ä¸Šäº†ä¸€ä¸ªé™åˆ¶ï¼šä¸‰æ¬¡åŠä»¥ä¸Šçš„ç‰¹å¾å‰é¢çš„æƒé‡ï¼ˆwï¼‰ä¸º0.è¿™æ ·å°±ä½¿å¾—å‡è¯´å˜å¾—ç®€å•äº†ä¸å°‘.</p><p>å¦‚æœæ”¾å®½è¿™ä¸ªé™åˆ¶ï¼Œå‡å¦‚è¾ƒä¸ºç®€å•çš„æ¨¡å‹ç‰¹å¾é‡æœ‰rä¸ªï¼Œåœ¨å¤æ‚æ¨¡å‹ä¸­ï¼Œæœ€å¤šæœ‰rä¸ªç‰¹å¾çš„æƒé‡ä¸ä¸º0ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šï¼Œä¹Ÿå¯ä»¥å¾ˆå¥½çš„å‡å°‘è¿™ä¸ªå¤æ‚åº¦ã€‚åªä¸è¿‡è®©äººé—æ†¾çš„æ˜¯ï¼Œåœ¨è¿™æ ·çš„å‡è¯´ä¸­é€‰å‡ºæœ€ä½³çš„$\mathbf w$ï¼ˆæƒé‡å‘é‡ï¼‰è¢«è¯æ˜äº†æ˜¯ä¸€ä¸ªNP-hardé—®é¢˜ï¼Œæ²¡æœ‰ä¸€ä¸ªå¾ˆå¥½çš„è§£å†³åŠæ³•.</p><p>å¦‚æœä¸ªæ•°æ˜¯æ•´æ•°ï¼Œè¦æŒ‘å‡ºæœ€å¥½çš„ï¼Œå¾ˆå®¹æ˜“æœ‰NP-hardé—®é¢˜ï¼Œä½†æ˜¯å°†è¿™ä¸ªæ‹“å®½åˆ°å®æ•°é¢†åŸŸï¼Œæˆ‘ä»¬å¾€å¾€å¯ä»¥é€šè¿‡æ•°å­¦å·¥å…·å¾—åˆ°æœ€ä½³è§£ã€‚ä¾‹å¦‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç»§ç»­æ‹“å®½è¿™ä¸ªé™åˆ¶ï¼šè®©è¿™äº›$w^2$å’Œå°äºä¸€ä¸ªå¸¸æ•°$C$ï¼Œä¼¼ä¹ä¹Ÿå¯ä»¥èµ·åˆ°ç±»ä¼¼çš„æ•ˆæœã€‚</p><p>ä¸ºäº†ç®€åŒ–é—®é¢˜ï¼Œä¸¾ä¸ªå¾ˆç®€å•çš„ä¾‹å­å¦‚ï¼šå¯¹äºåªæœ‰ä¸€ä¸ªç‰¹å¾é‡çš„æ ·æœ¬é›†ï¼Œ$H_2 = w_0+w_1x+w_2X^2$ï¼Œè€Œ$H_{10} = w_0+w_1x+w_2x^2+â€¦+w_{10}x^{10}$.</p><p>å¯¹äº$H_2$æ¥è¯´ï¼Œ$H_{10}$ä¸­é™åˆ¶ä¸º$w_3 = w_4 = w_5 = â€¦= w_{10} = 0$.</p><p>å¯¹äºä¸Šé¢è¯´çš„ç¬¬äºŒç§å‡è®¾ï¼Œ$H_{10}$é™åˆ¶ä¸º:$\sum _{n = 0} ^{10} [[w_n \ne 0]] \leq 3$.</p><p>å¯¹äºç¬¬ä¸‰ç§å‡è®¾ï¼Œ$H_{10}$é™åˆ¶ä¸º:$\sum_{n = 0}^{10} w_n^2 \leq C$ å³$W^TW \leq C$.</p><p>é‚£ä¹ˆï¼Œæˆ‘ä»¬å·²ç»çŸ¥é“äº†æœ€åä¸€ä¸ªæ‰æœ‰å¯èƒ½æ±‚å¾—æœ€ä½³è§£ã€‚å¦‚ä½•å»åšï¼Ÿ</p><p>é«˜ç»´åº¦çš„figureæˆ‘ä»¬æ— æ³•æƒ³è±¡ï¼Œæˆ‘ä¹Ÿä¸çŸ¥é“æ€ä¹ˆå»ç§°å‘¼ï¼Œä½†æ˜¯å¦‚æœæ˜¯äºŒç»´ï¼Œè¿™ä¸ªé™åˆ¶æ˜¯ä¸€ä¸ªåœ†ï¼Œå¦‚æœæ˜¯ä¸‰ç»´ï¼Œè¿™ä¸ªé™åˆ¶æ˜¯ä¸€ä¸ªçƒã€‚å‡è®¾æˆ‘ä»¬ä¾ç„¶ç§°è¿™ä¸ªé™åˆ¶ä¸ºä¸€ä¸ªçƒï¼Œè€Œæ²¡æœ‰é™åˆ¶çš„æœ€ä½ç‚¹ä¸åœ¨è¿™ä¸ªçƒå†…ã€‚å› æ­¤æ¢¯åº¦ä¸‹é™çš„ç»“æœå°±æ˜¯è¾¾åˆ°äº†çƒçš„è¾¹ç¼˜ï¼Œä½†æ˜¯ä¾ç„¶æƒ³è¦èµ°ä¸‹å¡è·¯ã€‚æ— è·¯å¯èµ°çš„æƒ…å†µï¼Œæ˜¯æ¢¯åº¦ä¸è¯¥æ³•å‘é‡çš„æ–¹å‘å¹³è¡Œäº†ï¼Œè€Œåªè¦æ¢¯åº¦ä¸è¯¥æ³•å‘é‡çš„æ–¹å‘ä¸ä¸å¹³è¡Œï¼Œæˆ‘ä»¬æ€»æ˜¯å¯ä»¥æœç€æŸä¸ªæ–¹å‘èµ°ä½¿å¾—$E_{in}$ç»§ç»­å‡å°‘ã€‚å› æ­¤è¿™ä¸ªè¿‡ç¨‹ç»ˆæ­¢çš„æ—¶å€™ï¼Œå°±æ˜¯è¯¥ç‚¹çš„æ³•å‘é‡ä¸$E_{in}$çš„æ¢¯åº¦å¹³è¡Œäº†ï¼Œè€Œå€¼å¾—æ³¨æ„çš„æ˜¯è¾¹ç¼˜æŸç‚¹çš„æ³•å‘é‡å®é™…ä¸Šå°±æ˜¯$W$.å¦‚æœæˆ‘ä»¬ç§°åšè¿™ä¸ªç»“æœ$W$ä¸º$W_{REG}$ï¼Œé‚£ä¹ˆæœ‰ä¸ªç»“æœï¼š$W_{REG} = \lambda â–½E_{in}$.</p><p>å…¶ä¸­è¿™ä¸ª$lambda$æ˜¯ä¸€ä¸ªå¸¸ç†Ÿ.æˆ‘ä»¬çŸ¥é“ï¼Œçº¿æ€§å›å½’ä¸­æ¢¯åº¦ä¸º$â–½E_{in} = \frac 2 N (X^TX - Y^TXW)$ï¼Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å°†$\lambda$å†™ä¸º$ \frac {2\lambda} N$,æœ€åå¾—åˆ°ï¼š</p><script type="math/tex; mode=display">â–½E_{in}+ \frac 2 N \lambda W_{REG} = 0.</script><p>å¦‚æœ$\lambda$æå‰çŸ¥é“ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¯ä»¥æ±‚å¾—$W_{REG}$çš„å€¼.</p><p>å¦‚æœå¯¹ä¸Šå¼å·¦è¾¹æ±‚ç§¯åˆ†å¯ä»¥å¾—åˆ°ï¼š </p><script type="math/tex; mode=display">f(W) = E_{in} + \frac \lambda N W_{REG}</script><p>æ‰€ä»¥å¯ä»¥å¾ˆç¥å¥‡åœ°å‘ç°ï¼Œå¯¹äºåŸæ¥çš„é—®é¢˜çš„æ±‚è§£å¯ä»¥å¾ˆæœ‰æ•ˆåœ°è½¬å˜æˆäº†æ±‚$f(W)$çš„æœ€å°å€¼ï¼Œå®ƒå°±æ˜¯æ­£åˆ™åŒ–åçš„$E_{in}$ï¼Œå› æ­¤æ–°çš„$cost-function$å˜æˆäº†ä¸‹é¢çš„æ ·å­ï¼š</p><script type="math/tex; mode=display">min_{W \in R^{Q+1}} \frac 1 N \sum _{n = 0} ^N (\mathbf{w}^T \theta(X_n) - y_n)^2 + \frac {\lambda} N \sum {q=0} ^Q w_q^2</script><p>tipsï¼šå¯¹äºå¤šé¡¹å¼æ­£åˆ™åŒ–ï¼Œå› ä¸ºä¸€èˆ¬æ¥è¯´æˆ‘ä»¬ä¼šå°†ç‰¹å¾å€¼çš„èŒƒå›´é™å®šåˆ°$[-1,1]$(åŸå› ä»¥åå†æ¢è®¨),è¿™å¯¼è‡´é«˜æ¬¡é¡¹çš„å½±å“å¯èƒ½å˜å¾—éå¸¸å°ï¼Œä¸ºäº†å¤„ç†è¿™ç§æƒ…å†µéœ€è¦ç”¨åˆ°ä¸€ä¸ªæ­£äº¤åŒ–å¤„ç†ï¼Œå…³é”®è¯â€œLegendre polynomialâ€ã€‚æ•ˆæœæ›´å¥½ã€‚éœ€è¦äº†è§£æ›´å¤šçš„è¯å¯ä»¥å»æœç´¢.</p><p>$\lambda$ç”±$C$ç¡®å®šï¼ˆè¿™æ˜¯ä¸ä¸¥è°¨çš„è¯´è¯ã€‚ä½†æ˜¯å®é™…ä¸­ç»™å®š\lambdaå°±å¯ä¸å°†$W$é™å®šåˆ°ä¸€ä¸ªèŒƒå›´ï¼Œå› æ­¤ç»™å‡ºCçš„äººä¹Ÿæ›´å®¹æ˜“ç»™å‡ºä¸€ä¸ª$\lambda$ï¼‰ï¼Œå®é™…åº”ç”¨æ—¶ï¼Œç»™$\lambda$ä¸€ä¸ªå¾ˆå°çš„å€¼å°±å¯ä¸å¾ˆå¥½åœ°å¤„ç†è¿‡æ‹Ÿåˆçš„æƒ…å½¢ï¼Œå¦‚æœè¿‡å¤§ï¼Œå¯èƒ½ä¼šå‡ºç°æ¬ æ‹Ÿåˆçš„æƒ…å†µ.</p><p>æ¥ä¸‹æ¥éœ€è¦ç»§ç»­è¯´æ˜çš„æ˜¯regularizationï¼Œä¸vcç†è®ºä¹‹é—´çš„å…³ç³»ã€‚å®é™…ä¸Šï¼Œå³ä½¿åŠ ä¸Šäº†regularizationï¼Œå¯¹äºä¸€ä¸ªå‡è¯´æ¥è¯´ï¼Œåœ¨æ•°å­¦è®¡ç®—ä¸Šå®ƒçš„vc dimentionä¾ç„¶å¾ˆå¤§ï¼Œä¾ç„¶ä¼šä»˜å‡ºå¾ˆå¤§çš„ä»£ä»·ã€‚ä½†æ˜¯regularizationçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿå®ƒå°†æˆ‘ä»¬éœ€è¦å¯»æ‰¾çš„èŒƒå›´å±€é™åœ¨äº†ä¸€å®šèŒƒå›´å†…ï¼Œåœ¨è¿™ä¸ªèŒƒå›´å†…ï¼Œå¯èƒ½éƒ½æ˜¯æ¯”è¾ƒå¥½çš„$h$,å› æ­¤æœ‰æ•ˆçš„vc dimensionå‡å°‘äº†ï¼Œä¹Ÿå°±æ›´æœ‰å¯èƒ½å¾—åˆ°æ¯”è¾ƒå¥½çš„$E_{out}$ã€‚</p><p>å¦‚ä½•é€‰æ‹©regularizerï¼Ÿ</p><p>1.é¦–å…ˆï¼Œå¦‚æœæˆ‘ä»¬çŸ¥é“ç›®æ ‡å‡½æ•°çš„ä¸€äº›ç‰¹ç‚¹ï¼Œå°±å¯ä»¥æŒ‡å¼•æˆ‘ä»¬é€‰æ‹©ä¸€äº›å¥½çš„regularizerï¼Œæ¯”å¦‚ï¼šå¦‚æœçŸ¥é“ç›®æ ‡å‡½æ•°æ˜¯å¶å‡½æ•°ï¼Œå¯ä»¥åªå¯¹å¥‡æ¬¡é¡¹çš„ç‰¹å¾è¿›è¡Œæ­£åˆ™åŒ–ã€‚</p><p>2.é€‰æ‹©å¹³æ»‘çš„ï¼Œå¦‚$\sum _{q=0} ^ Q |w_q|$.è¿™ä¸ªä¹Ÿå«L1 regularizerï¼ˆL1æ­£åˆ™åŒ–ï¼‰.ç›¸å¯¹äºL2æ¥è¯´å®ƒæ•ˆæœå¾€å¾€æ›´å¥½ä¸€ç‚¹ï¼Œå› ä¸ºæ›´åŠ å¹³æ»‘ï¼Œä½†æ˜¯ä¸å¥½è§£ã€‚</p><p>3.é€‰æ‹©å¥½ä¼˜åŒ–çš„ï¼Œå¦‚L2ï¼Œä¹Ÿå°±æ˜¯ä¸Šæ–‡æåˆ°çš„ã€‚</p><p>é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰$\lambda$çš„é€‰æ‹©ã€‚å¦‚å›¾ï¼Œä¸åŒçš„noiseçº§åˆ«éœ€è¦çš„$lambda$ä¹Ÿä¸åŒï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/QWA_KMVHC_%7DZWJF3%607C%5DQYA.png" alt=""></p><p>å¦‚ä½•é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„$\lambda$ä¹Ÿéå¸¸é‡è¦ï¼Œè¿™å°±éœ€è¦ç”¨åˆ°ä¸‹ä¸€èŠ‚æ‰€è®²çš„Validationã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> overfitting </tag>
            
            <tag> regularization </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”Overfitting</title>
      <link href="/2018/09/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Overfitting/"/>
      <url>/2018/09/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Overfitting/</url>
      
        <content type="html"><![CDATA[<p>Overfittingï¼ˆè¿‡æ‹Ÿåˆï¼‰æ˜¯æœºå™¨å­¦ä¹ ä¸­å¯èƒ½æœ€è®©äººå¤´ç–¼çš„é—®é¢˜äº†ã€‚å¯¹åº”Overfittingçš„æ˜¯Underfittingï¼ˆæ¬ æ‹Ÿåˆï¼‰ï¼Œç›¸æ¯”ä¹‹ä¸‹æˆä»½å°±å°‘äº†å¾ˆå¤šã€‚<a id="more"></a></p><p>ç®€å•æ¥è¯´ï¼ŒUnderfittingï¼Œæ˜¯$E_{in}$é«˜ï¼Œ$E_{out}$ä¹Ÿå¾ˆé«˜ã€‚äºæ˜¯äººä»¬ä¼šæƒ³æ–¹è®¾æ³•åœ°å‡å°‘$E_{in}$ï¼Œè®¤ä¸ºè¿™æ ·å°±å¯ä»¥å¾—åˆ°è¾ƒå¥½åœ°ç»“æœã€‚ä½†æ˜¯ä¸å¹¸çš„æ˜¯ï¼Œæœ‰æ—¶å€™$E_{in}$å·²ç»å¾ˆä½äº†ï¼Œè¿™ä¸ªæ¨¡å‹ä¾ç„¶æœ‰å¾ˆé«˜çš„$E_{out}$.è¿™å°±å¾ˆè®©äººå¤´ç–¼ã€‚è¿™å°±æ˜¯overfittingã€‚æƒ³è¦æ›´å¥½çš„è§£å†³Overfittingï¼Œç†è§£ä¸€äº›æ•°å­¦ç†è®ºå¦‚VC dimensionæ˜¯å¾ˆæœ‰å¸®åŠ©çš„ï¼Œç»™æˆ‘ä»¬æä¾›äº†æ›´å¤šå‡ºç°è¿™ç§æƒ…å†µçš„åŸå› å’Œè§£å†³çš„æ€è·¯ã€‚</p><p>å…¶å®Overfittingæˆ‘ä»¬ä¹‹å‰ä¹Ÿæ—©æœ‰æåŠè¿‡ã€‚</p><p>é¦–å…ˆæ¥çœ‹ä¸€ä¸‹overfittingçš„ç®€å•çš„ä¾‹å­ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/RPA%5DK%7D%5DD%5BU%251EL%7B1EM%29I%24W4.png" alt=""></p><p>å¯¹äºç›®æ ‡å‡½æ•°äº§ç”Ÿçš„èµ„æ–™ï¼ŒåŠ ä¸Šäº†ä¸€ç‚¹noiseï¼Œåœ¨è®­ç»ƒé›†æ ·æœ¬å¾ˆå°‘çš„æƒ…å†µä¸‹ï¼Œå‡ºç°äº†ä¸Šé¢çš„æƒ…å†µï¼šç›®æ ‡å‡½æ•°çš„$E_{in}$ï¼Œæ¯”æˆ‘ä»¬å¾—åˆ°çš„è¿™ä¸ªä¸ç›®æ ‡å‡½æ•°å·®äº†åä¸‡å…«åƒé‡Œçš„å‡½æ•°çš„$E_{in}$æ›´å¤§ã€‚æˆ‘ä»¬çš„ç®—æ³•é€‰æ‹©çš„æ˜¯$E_{in}$æœ€å°çš„ï¼Œå› æ­¤å°±é€‰æ‹©äº†è¡¨ç°å¾ˆå·®çš„æ¨¡å‹ã€‚</p><p>ä»ä¸Šé¢çš„æ ·ä¾‹æˆ‘ä»¬æƒ³åˆ°äº†nonlinear transformï¼Œå½“æˆ‘ä»¬è¿›è¡Œç‰¹å¾è½¬æ¢çš„æ—¶å€™ï¼Œvc dimensionå¤§å¤§å¢åŠ ï¼Œä½¿å¾—ä»˜å‡ºçš„ä»£ä»·å˜å¾—å¾ˆé«˜ï¼Œåœ¨æ ·æœ¬ä¸å¤Ÿçš„æƒ…å†µä¸‹ï¼Œå¾ˆå®¹æ˜“å¾—åˆ°å¾ˆå·®çš„$E_{out}$ï¼Œè¿™å°±æ˜¯ä¸€ç§overfitting.äºæ˜¯åˆä¸€æ¬¡çœ‹åˆ°äº†è¿™å¼ å›¾ã€‚</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/M%29P32DW%29EE9%7BWB%246A08T8%29X.png" alt=""></p><p>å½“ç„¶ï¼Œé€ æˆä¸Šé¢çš„ç»“æœæœ‰ä¸€ç‚¹åŸå› æ˜¯å™ªå£°ï¼Œä½†æ˜¯å³ä½¿æ²¡æœ‰å™ªå£°ï¼Œæœ€å¤šæœ€å¤šï¼Œä»–ä»¬çš„$E_{in}$ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œè€Œä¸”å®é™…ä¸­ï¼Œæ²¡æœ‰å™ªå£°çš„æƒ…å†µæ˜¯å¾ˆå°‘å‡ºç°çš„ã€‚è¿™è¯´æ˜äº†é€ æˆoverfittingçš„ä¸¤ä¸ªåŸå› ï¼š1.noiseè¿‡å¤šã€‚å¾ˆå¥½ç†è§£ï¼Œæ›´å¥½çš„é€‚åº”äº†noiseï¼Œå®ƒçš„æ³›åŒ–èƒ½åŠ›å½“ç„¶ä¸è¡Œï¼›2.ä½¿ç”¨è¿‡äºå¤æ‚çš„æ¨¡å‹ï¼Œå»ä¼°è®¡è¾ƒä¸ºç®€å•çš„ç›®æ ‡å‡½æ•°ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œç®€å•çš„å‡½æ•°åªæ˜¯å¤æ‚æ¨¡å‹çš„ç‰¹ä¾‹ï¼Œè€Œä¸”å› ä¸ºå™ªå£°çš„åŸå› å¾€å¾€ç›®æ ‡å‡½æ•°ä¸èƒ½å®Œç¾æ‹Ÿåˆï¼Œä½†æ˜¯å¤æ‚çš„æ¨¡å‹å°±èƒ½åšçš„éå¸¸å®Œç¾ã€‚ä½†æ˜¯å¦ä¸€æ–¹é¢ï¼Œå®ƒçš„æ³›åŒ–èƒ½åŠ›ä¹Ÿå¤§å¤§ä¸‹é™äº†ã€‚</p><p>å¦‚æœæˆ‘ä»¬é‡‡ç”¨å¤æ‚çš„æ¨¡å‹ä¼°è®¡å¤æ‚çš„å‡½æ•°å‘¢ï¼Ÿåœ¨æˆ‘ä»¬å¿ƒé‡Œå¯èƒ½ä¼šæƒ³ï¼Œè¿™ä¸‹æ€»ä¼šå¥½ç‚¹äº†å§ã€‚å› ä¸ºå¤æ‚çš„ç›®æ ‡å‡½æ•°ï¼Œä½ ä¸ç”¨å¤æ‚çš„æ¨¡å‹ï¼Œæ ¹æœ¬å°±ä¸å¯èƒ½å®Œç¾åœ°ä¼°è®¡å‡ºæ¥ã€‚ä¼¼ä¹æœ‰ç‚¹é“ç†ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸‹é¢åœ°ä¾‹å­ï¼š</p><p>ç”¨10æ¬¡çš„å¤šé¡¹å¼äº§ç”Ÿä¸€äº›æ•°æ®ï¼ŒåŠ ä¸Šå™ªå£°ã€‚æˆ‘ä»¬åˆ†å¸ƒç”¨10æ¬¡çš„$H$ä¸2æ¬¡çš„$H$æ¥å¯¹å®ƒè¿›è¡Œæ‹Ÿåˆï¼š</p><p>ç›®æ ‡å‡½æ•°ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/%29L%29Y%40YB1U%7DTN%5BLRL%7BJ%7E_N%7E9.png" alt=""></p><p>æ‹Ÿåˆç»“æœï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/Z%7DIA11%7BHHKFR%7EOFL6VV%60YH3.png" alt=""></p><p>é€šè¿‡å¯¹æ¯”ï¼Œæˆ‘ä»¬æƒŠå¥‡åœ°å‘ç°ï¼ŒäºŒæ¬¡çš„æ‹Ÿåˆç»“æœï¼Œè™½ç„¶$E_{in}$åšå¾—ä¸å¦‚ï¼Œä½†æ˜¯$E_{out}$æ¯”10æ¬¡çš„æ›´å¥½ï¼</p><p>å¦‚æœæˆ‘ä»¬ä½¿ç”¨äºŒæ¬¡çš„å¤šé¡¹å¼ï¼Œé‚£é¦–å…ˆæˆ‘ä»¬ä¸å¯èƒ½åšåˆ°å®Œç¾ï¼Œä½†æ˜¯æˆ‘ä»¬å‘ç°ï¼Œæœ‰æ—¶å€™å®ƒçš„è¡¨ç°æ¯”æ›´å¤æ‚åœ°æ¨¡å‹æ¨¡æ‹Ÿçš„æ›´å¥½ï¼Œå°½ç®¡åŸæ¥çš„æ¨¡å‹éå¸¸åœ°å¤æ‚ã€‚</p><p>æƒ³è¦äº†è§£è¿™å…¶ä¸­çš„åŸå› ï¼Œæˆ‘ä»¬æ¥è§‚å¯Ÿä¸€ä¸‹ä¸¤ä¸ª$H$çš„learning curveï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/9RVH8F%297JG%40RGRIUHHFJ%7EV2.png" alt=""></p><p>å¯¹äºå³ä¾§çš„æˆ‘ä»¬æ˜¯ç†Ÿæ‚‰çš„ï¼Œè§‚å¯Ÿå³ä¾§çš„$H_{10}$çš„å­¦ä¹ æ›²çº¿ï¼Œæˆ‘ä»¬å‘ç°ï¼Œè™½ç„¶å½“NåŒºåŸŸæ— ç©·çš„æ—¶å€™ï¼Œå®ƒæœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œä½†æ˜¯åœ¨ç°è‰²åŒºåŸŸé‡Œï¼Œå®ƒæ˜¯Overfittingçš„ã€‚å› æ­¤ï¼Œoverfittingçš„æœ€å…³é”®çš„åŸå› ï¼šæ•°æ®é‡ä¸å¤Ÿå¤šã€‚</p><p>å› ä¸ºæ¨¡å‹è¶Šå¤æ‚ï¼Œå¯ä»¥èµ°çš„è·¯å°±è¶Šå¤šï¼Œåœ¨èµ„æ–™é‡å°çš„æ—¶å€™ï¼Œå¯èƒ½å¾ˆå¤šæ¡è·¯éƒ½ä¼šå®Œç¾èµ°è¿‡è¿™æ¡é“ï¼Œè€Œå…¶ä»–çš„éƒ¨åˆ†å¯èƒ½å·®çš„å¾ˆè¿œã€‚è¿™ä¾ç„¶å¯ä»¥ç”¨VC dimensionæ¥è§£é‡Šï¼Œä»£ä»·å¤ªå¤§äº†ï¼Œä¸ºäº†é™ä½ä»£ä»·ï¼Œå¿…é¡»éœ€è¦æ›´å¤§çš„N.</p><p>å½“ç„¶ï¼Œä¸Šé¢çš„ä¾‹å­ä¸­ä¾ç„¶æœ‰noiseçš„å­˜åœ¨ã€‚Noiseæˆ–å¤šæˆ–å°‘å½±å“äº†å¤æ‚æ¨¡å‹çš„æ€§èƒ½ï¼Œè€Œä¸”è¶Šå¤æ‚æ¨¡å‹å®ƒçš„å½±å“å¯èƒ½è¶Šå¤§ã€‚å¯¹äºæ²¡æœ‰noiseçš„æ¨¡å‹ï¼Œæ˜¯å¦è¿˜æœ‰ä¸Šé¢çš„ç»“æœï¼Ÿ</p><p>åˆ©ç”¨50æ¬¡ç›®æ ‡å‡½æ•°äº§ç”Ÿçš„æ•°æ®ï¼Œä¾ç„¶ç”¨$H_2$ä¸$H_{10}$å»æ¨¡æ‹Ÿï¼š</p><p>ç›®æ ‡å‡½æ•°ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/UTID8V4V%28O3TP%5BTE%24J%5BT_%24U.png" alt=""></p><p>æ‹Ÿåˆç»“æœï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/T%7EY9M6%24%605S_DUHD8CPA0B9X.png" alt=""></p><p>å¯ä»¥çœ‹åˆ°äºŒæ¬¡å‡½æ•°ä¾ç„¶æ¯”10æ¬¡å‡½æ•°è¡¨ç°å¾—æ›´å¥½ã€‚åŸå› å’Œä¸Šé¢ä¸€æ ·ã€‚æ‰€ä»¥æ²¡æœ‰å¾ˆå¤šæ•°æ®é‡çš„æ”¯æ’‘ï¼Œä½¿ç”¨è¾ƒä¸ºç®€å•çš„æ¨¡å‹å¾€å¾€æ•ˆæœæ›´å¥½ã€‚</p><p>è¿™æ—¶å€™æˆ‘ä»¬ä¼šçº³é—·ï¼Œè¿™æ˜æ˜æ²¡æœ‰noiseå•Šï¼Œä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿè¿‡äºå¤æ‚çš„æ¨¡å‹ä¸ç®€å•çš„å‡è®¾ä¼¼ä¹å¸¦æ¥äº†å’Œnoiseç±»ä¼¼çš„ç»“æœã€‚</p><p>åœ¨æœºå™¨å­¦ä¹ é¢†åŸŸä¸­ï¼Œå¯¹äºè¿‡äºå¤æ‚çš„æ¨¡å‹æœ¬èº«å¸¦æ¥çš„ç±»ä¼¼äºnoiseçš„æ•ˆæœï¼Œè¢«ç§°ä¸ºDeterministic Noise.</p><p>å¯¹äºæ™®é€šçš„noiseï¼Œæˆ‘ä»¬å‡è®¾ä¸ºé«˜æ–¯å™ªå£°ï¼ˆGaussian Noiseï¼‰ã€‚å¯¹äºGaussian Noiseä¸Deterministic Noiseå¯¹äºæ¨¡å‹çš„å½±å“ï¼Œ$Q_f$ä¸ºç›®æ ‡å‡½æ•°çš„æ¬¡æ•°ï¼Œé‚£ä¹ˆå¯ä»¥ç”¨ä¸‹é¢çš„å›¾æ¥å½¢è±¡å±•ç¤ºå‡ºæ¥ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/%28%25V%25Z8QJM%29V8%5D2NS%240%254IZK.png" alt=""></p><p>å¯ä»¥çœ‹å‡ºæ¥ï¼Œä¸¤ç§å™ªå£°å¸¦æ¥çš„æ•ˆæœç›¸è¿‘ï¼Œå¯ä»¥é€šè¿‡å¢åŠ Næ¥é¿å…è¿‡æ‹Ÿåˆçš„æƒ…å†µã€‚</p><p>å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä¾§å›¾ä¸­ï¼Œå·¦ä¸‹è§’ä¾ç„¶æœ‰ä¸€å—ä¼šè¿‡æ‹Ÿåˆã€‚éœ€è¦æ³¨æ„ï¼Œä¸Šè¿°å›¾ä¸­$H$çš„æ¬¡æ•°æ˜¯ä¸å˜çš„ï¼Œå› æ­¤ï¼Œå¦‚æœ$Q_f$å°äº$H$çš„æ¬¡æ•°ï¼Œä¼šå‡ºç°ç¬¬ä¸€ç§æƒ…å†µï¼Œpowerè¿‡å¼ºçš„æƒ…å†µ.</p><p>å®é™…ä¸Šï¼Œdeterministic noiseä¸ç”µè„‘äº§ç”Ÿä¼ªéšæœºæ•°çš„åŸç†å¾ˆç›¸ä¼¼ï¼Œè¿‡äºå¤æ‚çš„æ¨¡å‹ï¼Œé€ æˆäº†éšæœºå™ªå£°çš„æ•ˆæœã€‚</p><p>æ€»ç»“ä¸€ä¸‹ï¼ŒOverfittingå‡ºç°çš„åŸå› ï¼š<br>1.Nå¤ªå°<br>2.Stochastic(Gaussian) Noise<br>3.Deterministic Noise<br>4.Too Much Power</p><p>å¦‚ä½•å¯¹ä»˜overfittingæ˜¯ä¸ªå¾ˆå¤æ‚çš„è¯é¢˜ã€‚é¦–å…ˆç›´è§‚æ¥è¯´ï¼Œé™å™ªï¼Œå¢åŠ æ ·æœ¬ã€‚é™å™ªï¼Œå¯ä»¥é€šè¿‡ä¿®æ­£labelä¸å»é™¤é”™è¯¯çš„æ ·æœ¬æ¥å®ç°ï¼Œè€Œå¢åŠ æ ·æœ¬å¾€å¾€æ²¡æœ‰é‚£ä¹ˆå®¹æ˜“ï¼ŒæŸäº›æƒ…å†µä¸‹æˆ‘ä»¬å¯ä»¥è‡ªå·±åˆ›é€ data.å¦å¤–è¿˜æœ‰ä¸¤ç§æ¯”è¾ƒå¤æ‚çš„åšæ³•ï¼Œä¹Ÿæœ‰å¾ˆå¥½çš„æ•ˆæœï¼šRegularization ä¸ Validation.ä»¥åä¼šä¸“é—¨å†™åšå®¢æ¥ä»‹ç»ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> overfitting </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”ï¼ˆåŸºçŸ³ï¼‰ä½œä¸š3</title>
      <link href="/2018/09/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%EF%BC%88%E5%9F%BA%E7%9F%B3%EF%BC%89%E4%BD%9C%E4%B8%9A3/"/>
      <url>/2018/09/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%EF%BC%88%E5%9F%BA%E7%9F%B3%EF%BC%89%E4%BD%9C%E4%B8%9A3/</url>
      
        <content type="html"><![CDATA[<p>æ€»å…±20é“é¢˜ç›®ã€‚<a id="more"></a><br><strong>1. Consider a noisy target $y = {\bf w}_f^T{\bf x} + \epsilon$, where $\mathbf{x} \in \mathbb{R}^d$ (with the added coordinate $x_0=1$), $y\in\mathbb{R}$, $\mathbf{w}_f$ is an unknown vector, and $\epsilon$ is a noise term with zero mean and $\sigma^2$ variance. Assume $\epsilon$ is independent of ${\bf x}$ and of all other $\epsilon$â€™s. If linear regression is carried out using a training data set $\mathcal{D} = \{(\mathbf{x}_1, y_1), \ldots, ({\bf x}_N, y_N)\}$, and outputs the parameter vector $\mathbf{w}_{\rm lin}$, it can be shown that the expected in-sample error $E_{\rm in}$ with respect to $\mathcal{D}$ is given by:</strong></p><script type="math/tex; mode=display">\mathbb{E}_{\mathcal{D}}[E_{\rm in}(\mathbf{w}_{\rm lin})] = \sigma^2\left(1 - \frac{d + 1}{N}\right)</script><p>For $\sigma = 0.1$ and $d = 8$, which among the following choices is the smallest number of examples $N$ that will result in an expected $E_{\rm in}$ greater than 0.008?</p><p>a. 10</p><p>b. 25</p><p>c. 100</p><p>d. 500</p><p>e. 1000</p><p>è¿™é“é¢˜ç›®ä¸­ï¼Œå·²ç»ç»™å‡ºäº†$E_{in}$çš„æœŸæœ›å€¼å¦‚ä½•è®¡ç®—ï¼Œåªéœ€è¦å°†$\sigma = 0.1,d = 8$å¸¦å…¥ä¸Šå¼å³å¯ï¼Œç®—å‡ºæ¥çš„æ˜¯$N = 45$çš„æ—¶å€™ï¼Œ$E_{in}$çš„æœŸæœ›å€¼ä¸º0.008ï¼Œè€Œä¸ºäº†ä½¿å¾—æœŸæœ›å€¼å˜å¾—æ›´å¤§ï¼Œ$N$çš„å€¼ä¹Ÿè¦å˜å¾—æ›´å¤§ï¼Œå› æ­¤ä¸Šé¢é€‰é¡¹ä¸­æœ€å°çš„å¤§äº45çš„$N$æ˜¯100ï¼Œé€‰cï¼ˆNote:Greateræ„æ€æ˜¯æ›´å¤§ï¼Œè€Œä¸æ˜¯æ›´å¥½ï¼‰.</p><p><strong>2. Recall that we have introduced the hat matrix $\mathrm{H} = \mathrm{X}(\mathrm{X}^T\mathrm{X})^{-1}\mathrm{X}^T$ in class, where $\mathrm{X} \in \mathbb{R}^{N\times (d+1)}$ containing $N$ examples with $d$ features. Assume $\mathrm{X}^T\mathrm{X}$ is invertible and $N &gt; d+1$, which statement of $\mathrm{H}$ is true?</strong></p><p>a. none of the other choices</p><p>b. $\mathrm{H}^{1126} = \mathrm{H}$</p><p>c. $(d+1)$ eigenvalues of $\mathrm{H}$ are bigger than 1.</p><p>d. $N - (d+1)$ eigenvalues of $\mathrm{H}$ are 1</p><p>e. $\mathrm{H}$ is always invertible</p><p>ä»<a href="https://wlsdzyzl.top/2018/08/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94linear-regression/#more" target="_blank" rel="noopener">linear regression</a>ä¸­ï¼Œæˆ‘ä»¬ç›´åˆ°$H$çŸ©é˜µçš„ä½œç”¨æ˜¯åœ¨$X$ç©ºé—´åš$Y$çš„æŠ•å½±ï¼Œæ¥å¾—åˆ°$Yâ€™$ï¼Œè€ŒæŠ•å½±ä¸€æ¬¡ä¸æŠ•å½±10æ¬¡æ²¡ä»€ä¹ˆå¤ªå¤§åŒºåˆ«ï¼Œå› æ­¤bæ˜¯æ­£ç¡®çš„ã€‚$H$çš„è‡ªç”±åº¦æ˜¯$N - (d+1)$ï¼Œè€Œå®ƒçš„ç‰¹å¾å€¼åº”è¯¥æ˜¯æœ‰(N - d+1)ä¸ªä¸ä¸º0ï¼Œè€Œä¸”ç‰¹å¾å€¼ä¹Ÿä¸æ˜¯ä¸€å®šçš„ï¼Œåªæ˜¯ä¸ç‰¹å¾å‘é‡æˆæ¯”ä¾‹ï¼Œå› æ­¤cï¼Œdæ˜¯ä¸å¯¹çš„ã€‚</p><p><strong>3. Which of the following is an upper bound of $[[sign(w^Tx)â‰ y]]$ for $y \in \{-1, +1\}$?</strong></p><p>a. $err(W) = \frac 1 2 e ^{(-yW^TX)}$</p><p>b. $err(W) = [W^TX \geq y]$</p><p>c. $err(W) = max(0,1 - yW^TX)$</p><p>d. $err(W) = max(0, -yW^TX)$</p><p>e. none of the other choices</p><p>è¿™ä¸ªé¢˜ç›®æœ€ç›´è§‚çš„çœ‹æ³•ä¾ç„¶æ˜¯ç”»å›¾ï¼Œa-red,b-blue,c-yellow,d-green,$[sign(W^TX) \neq y]$-black,æŒ‰ç…§ä¸Šé¢çš„é¢œè‰²ç”»å›¾å¦‚ä¸‹ï¼š<br>å½“y = 1 æ—¶ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/8%5B%24IX%25Y9N86%5DB%2856AA%243TEE.png" alt=""><br>å½“ y = -1æ—¶ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/%7BAK_J8U02WEX%247%5BMUS3O1%24B.png" alt=""><br>ä»ä¸Šé¢çš„å›¾ä¸­æˆ‘ä»¬å¯ä»¥å¾ˆç›´è§‚çœ‹åˆ°åªæœ‰yellowçº¿æ˜¯ç¬¦åˆçš„ï¼Œå› æ­¤è¿™ä¸ªé¢˜ç›®ç­”æ¡ˆä¸ºc.</p><p><strong>4. Which of the following is a differentiable function of $\mathbf{w}$ everywhere?</strong></p><p>a. $err(W) = \frac 1 2 e ^{(-yW^TX)}$</p><p>b. $err(W) = [W^TX \geq y]$</p><p>c. $err(W) = max(0,1 - yW^TX)$</p><p>d. $err(W) = max(0, -yW^TX)$</p><p>e. none of the other choices</p><p>differentiable function æ„æ€æ˜¯å¯å¾®å‡½æ•°ã€‚æ‰€ä»¥å¾ˆæ˜æ˜¾ç­”æ¡ˆæ˜¯a.</p><p><strong>5. When using SGD on the following error functions and `ignoringâ€™ some singular points that are not differentiable, which of the following error function results in PLA?</strong></p><p>a. $err(W) = \frac 1 2 e ^{(-yW^TX)}$</p><p>b. $err(W) = [W^TX \geq y]$</p><p>c. $err(W) = max(0,1 - yW^TX)$</p><p>d. $err(W) = max(0, -yW^TX)$</p><p>e. none of the other choices</p><p>PLAæ›´æ–°ç­–ç•¥ï¼š</p><script type="math/tex; mode=display">W_{n+1} = W_{n} + yX</script><p>ä½¿ç”¨SGDï¼Œä¹Ÿå°±æ˜¯éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬æ±‚$err(W)$çš„å¯¼æ•°æ¥æ›´æ–°ã€‚å¯¹äºæ¢¯åº¦ä¸‹é™ä¸­ï¼Œå¯¹äºä¸‹ä¸€æ­¥çš„åšæ³•æ˜¯ï¼š</p><script type="math/tex; mode=display">W_{n+1} = W_{n} - \alpha \frac {d_{err{W}}}{d_W}</script><p>å…¶ä¸­$\alpha$æ˜¯å­¦ä¹ ç‡ï¼Œè¿™é“é¢˜ç›®ä¸­åº”è¯¥ä¸º1.</p><p>å› æ­¤ä¸»è¦æ˜¯è¦æ±‚å¾—å¯¼æ•°ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬ä»ä¸Šé¢çš„é¢˜ç›®ä¸­å¯ä»¥è½»æ˜“çš„çœ‹å‡ºï¼Œå¯¹äº a,bä¸€å®šæ˜¯ä¸æ­£ç¡®çš„ï¼Œè€Œcï¼Œdçš„å¯¼æ•°æ˜¯ä¸€æ ·çš„ï¼š$\frac {d_{err{W}}}{d_W}  =  -yX$ï¼Œå› æ­¤å°±æ›´æ–°çš„æ­¥éª¤æ¥è¯´ï¼Œå®ƒä»¬ä¸¤ä¸ªéƒ½æ˜¯åˆé€‚çš„ã€‚</p><p>ä½†æ˜¯æˆ‘ä»¬ä¸èƒ½éƒ½é€‰ä¸Šï¼Œè¦æ³¨æ„PLAç®—æ³•åšæ›´æ–°çš„æ—¶å€™æ˜¯æ‰¾åˆ°é”™è¯¯çš„ç‚¹ï¼Œå¦‚æœç‚¹æ˜¯æ­£ç¡®çš„ï¼Œæˆ‘ä»¬åˆ™ä¸åº”è¯¥æ›´æ–°ã€‚è€Œcé€‰é¡¹æ˜¯PLAçš„Upper boundï¼Œä»ä¸Šé¢é¢˜ç›®çš„å›¾ä¸­ä¹Ÿå¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœ$y = -1ï¼ŒW^TX \in [-1,1]$ä¹‹é—´PLAæ˜¯ä¸ä¼šæ›´æ–°çš„ï¼Œè€Œcåœ¨è¿™ä¸ªæ—¶å€™ä¾ç„¶ä¼šé€‰æ‹©æ›´æ–°ã€‚æ‰€ä»¥è¿™ä¸ªé¢˜ç›®çš„æ­£ç¡®ç­”æ¡ˆæ˜¯d.</p><p>For Questions 6-10, consider a function $E(u,v) = e^u + e^{2v} + e^{uv} + u^2 - 2 u v + 2 v^2 - 3 u - 2 v$.</p><p><strong>6. What is the gradient $\nabla E(u, v)$ around $(u, v)=(0, 0)$?</strong></p><p>a. $(0,-2)$</p><p>b. none of the other choices</p><p>c. $(-3,1)$</p><p>d. $(3,-1)$</p><p>e. $(-2,0)$</p><p>è¿™é“é¢˜ç›®ä¸ç®—éš¾ã€‚ $\nabla E(u,v) = (e^u+ve^{uv}+2u - 2v - 3, 2e^{2v}+ue^{uv}-2u +4v - 2)$ï¼Œä»£å…¥$(0ï¼Œ0)$å¾—åˆ°$(-2,0)$ï¼Œç­”æ¡ˆä¸ºe.</p><p><strong>7. In class, we have taught that the update rule of the gradient descent algorithm is $(u_{t+1}, v_{t+1}) = (u_t, v_t) - \eta \nabla E(u_t, v_t)$. Please start from $(u_0, v_0) = (0, 0)$, and fix $\eta=0.01$. What is $E(u_{5}, v_{5})$ after five updates?</strong></p><p>a. 4.904</p><p>b. 3.277</p><p>c. 2.825</p><p>d. 2.361</p><p>e. 1.436</p><p>è¿™ä¸ªé¢˜ç›®è™½ç„¶è¿­ä»£æ¬¡æ•°ä¸å¤šï¼Œä½†æ˜¯å› ä¸ºæœ‰$\exp$å‡½æ•°è®¡ç®—ï¼Œè¿˜æ˜¯æ¯”è¾ƒå¤æ‚çš„ã€‚å› æ­¤æˆ‘ç¼–å†™æ¢¯åº¦ä¸‹é™çš„ç¨‹åºæ¥è®¡ç®—è¿™é“é¢˜ç›®ã€‚</p><p>å‡½æ•°gradient_decentè¦ä¼ å…¥4ä¸ªå‚æ•°ï¼šstartå¼€å§‹å€¼ï¼Œget_gradicent ä¸€ä¸ªè®¡ç®—æ¢¯åº¦çš„å‡½æ•°ï¼Œetaå­¦ä¹ ç‡ï¼Œi_timesä¸ºè¿­ä»£æ¬¡æ•°ã€‚</p><p>å…¶ä»–çš„å‡½æ•°åŒ…æ‹¬è®¡ç®—æ¢¯åº¦ï¼Œæ›´æ–°Wï¼ˆupdateï¼‰ï¼Œä»¥åŠè®¡ç®—$err$ï¼Œç¨‹åºä»£ç å¦‚ä¸‹ï¼š<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_gradient</span><span class="params">(para)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [math.exp(para[<span class="number">0</span>])+para[<span class="number">1</span>]*math.exp(para[<span class="number">0</span>]*para[<span class="number">1</span>])+<span class="number">2</span>*para[<span class="number">0</span>]<span class="number">-2</span>*para[<span class="number">1</span>]<span class="number">-3</span>,<span class="number">2</span>*math.exp(<span class="number">2</span>*para[<span class="number">1</span>])+para[<span class="number">0</span>]*math.exp(para[<span class="number">0</span>]*para[<span class="number">1</span>] )- <span class="number">2</span>*para[<span class="number">0</span>]+<span class="number">4</span>*para[<span class="number">1</span>]<span class="number">-2</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_decent</span><span class="params">(start,get_gradient,eta,i_times)</span>:</span></span><br><span class="line">    last = copy.deepcopy(start)</span><br><span class="line">    <span class="keyword">while</span>(i_times&gt;<span class="number">0</span>):</span><br><span class="line">        i_times-=<span class="number">1</span></span><br><span class="line">        update(last,get_gradient(last),eta)</span><br><span class="line">    <span class="keyword">return</span> last</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(last,gradient,eta)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span>  range(len(last)):</span><br><span class="line">        last[i] = last[i] - eta * gradient[i]</span><br><span class="line">    <span class="keyword">return</span> last</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Ein</span><span class="params">(para)</span>:</span></span><br><span class="line">    u = para[<span class="number">0</span>]</span><br><span class="line">    v = para[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> math.exp(u)+math.exp(<span class="number">2</span>*v)+ math.exp(u*v)+math.pow(u,<span class="number">2</span>)<span class="number">-2</span>*u*v+<span class="number">2</span>*math.pow(v,<span class="number">2</span>)<span class="number">-3</span>*u - <span class="number">2</span>*v</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    print(gradient_decent([<span class="number">0</span>,<span class="number">0</span>],get_gradient,<span class="number">0.01</span>,<span class="number">5</span>))</span><br><span class="line">    print(Ein(gradient_decent([<span class="number">0</span>,<span class="number">0</span>],get_gradient,<span class="number">0.01</span>,<span class="number">5</span>)))</span><br></pre></td></tr></table></figure></p><p>æœ€åè¾“å‡ºï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[0.09413996302028127, 0.0017891105951028273]</span><br><span class="line">2.8250003566832635</span><br></pre></td></tr></table></figure></p><p>å› æ­¤ç­”æ¡ˆé€‰c.</p><p><strong>8. Continuing from Question 7. If we approximate the $E(u + \Delta u, v + \Delta v)$ by $\hat{E}_2(\Delta u, \Delta v)$, where $\hat{E}_2$ is the second-order Taylorâ€™s expansion of $E$ around $(u,v)$. Suppose $\hat{E}_2(\Delta u, \Delta v) = b_{uu} (\Delta u)^2 + b_{vv} (\Delta v)^2 + b_{uv} (\Delta u)(\Delta v) + b_u \Delta u + b_v \Delta v + b $, What are the values of $(b_{uu}, b_{vv}, b_{uv}, b_u, b_v, b)$ around $(u, v) = (0, 0)$?</strong></p><p>a. none of the other choices</p><p>b. $(3,8,-1,-2,0,3)$</p><p>c. $(3,8,-0.5,-1,-2,0)$</p><p>d. $(1.5,4,-0.5,-1,-2,0)$</p><p>e. $(1.5,4,-1,-2,0,3)$</p><p>è¿™ä¸ªé¢˜ç›®ä¸ç®—éš¾ï¼Œå®é™…ä¸Šæ˜¯ä¸€ä¸ªå¤šç»´å‡½æ•°çš„äºŒé˜¶æ³°å‹’å±•å¼€ï¼Œè€Œ$b{_uu}$åˆ™æ˜¯å¯¹$u$æ±‚äºŒé˜¶åå¯¼å†é™¤ä»¥$2!$ã€‚åªè¦ä¸€é¡¹é¡¹è®¡ç®—ä¸Šé¢çš„å€¼å°±å¯ä»¥äº†ã€‚<br>$b_{uu} = 3,b_{vv} = 8,b_{uv} = -1,b_{u} = -2,b_{v} = 0,b = 3$ï¼ŒäºŒå…ƒæ³°å‹’å±•å¼€å› æ­¤ç­”æ¡ˆæ˜¯e.   </p><p><strong>9. Continue from Question 8 and denote the Hessian matrix to be $\nabla^2 E(u, v)$, and assume that the Hessian matrix is positive definite. What is the optimal $(\Delta u, \Delta v)$ to minimize $\hat{E}_2(\Delta u, \Delta v)$? (The direction is called the Newton Direction.)</strong></p><p>a. $+(\nabla^2E(u,v))^{-1}\nabla E(u,v)$</p><p>b. $-(\nabla^2E(u,v))^{-1}\nabla E(u,v)$</p><p>c. none of the other choices</p><p>d. $+\nabla ^2 E(u,v) \nabla E(u,v)$</p><p>e. $-\nabla ^2 E(u,v) \nabla E(u,v)$</p><p>å…¶å®è¿™ä¸ªé¢˜ç›®æˆ‘ä¸æ˜¯å¾ˆæ¸…æ¥šã€‚è¿™å…¶ä¸­æ¶‰åŠåˆ°äº†æµ·æ£®çŸ©é˜µä»¥åŠç‰›é¡¿æ–¹å‘ã€‚é€šè¿‡ç®€å•çš„äº†è§£ï¼Œå¦‚æœä¸€ä¸ªç‚¹çš„æ¢¯åº¦ä¸º0å‘é‡ï¼Œè€Œä¸”åœ¨è¯¥ç‚¹çš„æµ·æ£®çŸ©é˜µä¸ºæ­£å®šçŸ©é˜µï¼Œé‚£ä¹ˆè¯¥ç‚¹ä¸ºæå°å€¼ç‚¹ã€‚é¢˜ç›®ä¸­è¯´æ˜äº†æµ·æ£®çŸ©é˜µä¸ºæ­£å®šçŸ©é˜µï¼Œå› æ­¤æ‰¾è¿™ä¸ªæå€¼ç‚¹ï¼Œå¦‚æœä¸çŸ¥é“ç‰›é¡¿æ–¹å‘ï¼Œæˆ‘å°±ä¼šæŒ‰ç…§æ¢¯åº¦ä¸‹é™çš„æ¥ã€‚è€ŒæŸ¥é˜…äº†ç‰›é¡¿æ–¹å‘ä¹‹åï¼Œç‰›é¡¿æ–¹å‘å°±æ˜¯eé€‰é¡¹ã€‚æ‰€ä»¥é€‰æ‹©b.</p><p>å½“ç„¶ï¼Œè¿™ä¸ªé¢˜ç›®å®é™…ä¸Šå°±æ˜¯æœå‡ºæ¥çš„ç­”æ¡ˆï¼ŒçœŸæ­£æƒ³è¦ç†è§£éœ€è¦æ›´åŠ æ·±å…¥åœ°å»çœ‹çº¿æ€§ä»£æ•°ã€‚</p><p><strong>10. Use the Newton direction (without \eta) for updating and start from $(u_0, v_0) = (0, 0)$. What is $E(u_{5}, v_{5})$ after five updates?</strong></p><p>a. 4.904</p><p>b. 3.277</p><p>c. 2.825</p><p>d. 2.361</p><p>e. 1.436</p><p>è¿™é“é¢˜ç›®ï¼Œå½“ç„¶æˆ‘è¿˜æ˜¯æƒ³å€ŸåŠ©ç¨‹åºæ¥å®Œæˆï¼Œä¸è¿‡è¿™æ¬¡çš„ç¨‹åºç›¸æ¯”ä¹‹å‰ä¼šæ›´éº»çƒ¦ä¸€ç‚¹ã€‚å› ä¸ºä¹‹å‰è®¾ç½®å¥½äº†æ¥å£ï¼Œä¹Ÿå°±æ˜¯åœ¨æ¢¯åº¦ä¸‹é™çš„æ—¶å€™ä¼ å…¥æ±‚å¾—æ¢¯åº¦çš„å‡½æ•°ï¼Œå› æ­¤è¿™é‡Œæˆ‘ä»¬åªéœ€è¦å†™å‡ºè®¡ç®— $\nabla ^2 E(u,v) \nabla E(u,v)$çš„å‡½æ•°ï¼š<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Newton_direction</span><span class="params">(para)</span>:</span></span><br><span class="line">    u = para[<span class="number">0</span>]</span><br><span class="line">    v = para[<span class="number">1</span>]</span><br><span class="line">    hs = [[math.exp(u)+math.pow(v,<span class="number">2</span>)*math.exp(u*v)+<span class="number">2</span>,math.exp(u*v)+v*u*math.exp(u*v)<span class="number">-2</span>],</span><br><span class="line">          [math.exp(u*v)+v*u*math.exp(u*v)<span class="number">-2</span>,<span class="number">4</span>*math.exp(<span class="number">2</span>*v)+math.pow(u,<span class="number">2</span>)*math.exp(u*v)+<span class="number">4</span>]]</span><br><span class="line">    m = mat(hs)</span><br><span class="line">    <span class="keyword">return</span> (m.I*mat([get_gradient([u,v])]).T).T.tolist()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p><p>æœ€ååœ¨ä¸»å‡½æ•°ä¸­ä¿®æ”¹ä¼ å…¥çš„å­¦ä¹ ç‡ä¸æ±‚æ¢¯åº¦çš„å‡½æ•°ï¼š<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(Ein(gradient_decent([<span class="number">0</span>,<span class="number">0</span>],Newton_direction,<span class="number">1</span>,<span class="number">5</span>)))</span><br></pre></td></tr></table></figure></p><p>å¾—åˆ°ç»“æœï¼š<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">å› æ­¤ç­”æ¡ˆé€‰d.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">**11. Consider six inputs $\mathbf&#123;x&#125;_1 = (1, 1)$, $\mathbf&#123;x&#125;_2 = (1, -1)$, $\mathbf&#123;x&#125;_3 = (-1, -1)$, $\mathbf&#123;x&#125;_4 = (-1, 1)$, $\mathbf&#123;x&#125;_5 = (0, 0)$, $\mathbf&#123;x&#125;_6 = (1, 0)$. What is the biggest subset of those input vectors that can be shattered by the union of quadratic, linear, or constant hypotheses of $\mathbf&#123;x&#125;$?**</span><br><span class="line"></span><br><span class="line">a. $\mathbf&#123;x&#125;_1,\mathbf&#123;x&#125;_2,\mathbf&#123;x&#125;_3$</span><br><span class="line"></span><br><span class="line">b. $\mathbf&#123;x&#125;_1,\mathbf&#123;x&#125;_2,\mathbf&#123;x&#125;_3,\mathbf&#123;x&#125;_4,\mathbf&#123;x&#125;_5,\mathbf&#123;x&#125;_6$</span><br><span class="line"></span><br><span class="line">c. $\mathbf&#123;x&#125;_1,\mathbf&#123;x&#125;_2,\mathbf&#123;x&#125;_3,\mathbf&#123;x&#125;_4$</span><br><span class="line"></span><br><span class="line">d. $\mathbf&#123;x&#125;_1,\mathbf&#123;x&#125;_3$</span><br><span class="line"></span><br><span class="line">e. $\mathbf&#123;x&#125;_1,\mathbf&#123;x&#125;_2,\mathbf&#123;x&#125;_3,\mathbf&#123;x&#125;_4,\mathbf&#123;x&#125;_5$</span><br><span class="line"></span><br><span class="line">é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç‰¹å¾é‡çš„ä¸ªæ•°æ˜¯2ï¼ˆ$d=2$ï¼‰ï¼Œè€Œç”±å‰é¢çš„æ¨å¯¼å¯ä»¥çŸ¥é“ï¼Œæœ€é«˜æ¬¡ä¸º2æ¬¡æ—¶å€™ï¼Œå®ƒå¯ä»¥ç»„æˆçš„æ–°çš„ç‰¹å¾å€¼çš„ä¸ªæ•°ä¸º5ï¼Œè€Œå®ƒçš„vc dimensionæ˜¯å°äºç­‰äº6çš„.ä¸€ç§å¯è¡Œçš„åŠæ³•æ˜¯å°†xç©ºé—´æŠ•å½±åˆ°zç©ºé—´ä»¥åï¼Œç”¨çº¿æ€§çš„åŠæ³•æ¥shatterè¿™6ä¸ªç‚¹ï¼Œå†è¿”å›çœ‹åŸæ¥çš„xç»„åˆè€Œæˆçš„ç‰¹å¾æ˜¯å¦èƒ½å¤Ÿæ»¡è¶³è¿™6ä¸ªç‰¹å¾é‡ã€‚ä½†æ˜¯å®é™…ä¸Šè¿™è¿˜æ˜¯å¤æ‚çš„ã€‚ä¹Ÿå¯ä»¥åˆ©ç”¨æ¢¯åº¦ä¸‹é™ï¼Œè·‘ç¨‹åºæ¥å¯¹ä¸Šè¿°é€‰é¡¹æ¥è¿›è¡Œæ’é™¤ã€‚å…·ä½“ç­”æ¡ˆæ˜¯å¤šå°‘è¿˜éœ€è¦è¿›ä¸€æ­¥è¿›è¡Œæ’é™¤ã€‚è¿™ä¸ªç­”æ¡ˆæ˜¯b.å¯ä»¥shatter6ä¸ªç‚¹.</span><br><span class="line"></span><br><span class="line">**12. Assume that a transformer peeks the data and decides the following transform $\boldsymbol&#123;\Phi&#125;$ &quot;intelligently&quot; from the data of size $N$. The transform maps $\mathbf&#123;x&#125; \in \mathbb&#123;R&#125;^d$ to $\mathbf&#123;z&#125; \in \mathbb&#123;R&#125;^N$, where**</span><br><span class="line">$$</span><br><span class="line">(\boldsymbol&#123;\Phi&#125;(\mathbf&#123;x&#125;))_n = z_n = \left[\left[ \mathbf&#123;x&#125; = \mathbf&#123;x&#125;_n \right]\right]</span><br><span class="line">$$</span><br><span class="line">Consider a learning algorithm that performs PLA after the feature transform.Assume that all $\mathbf&#123;x&#125;_n$ are different, $30%$ of the $y_n$&apos;s are positive, and $sign(0)=+1$. Then, estimate the $E_&#123;out&#125;$ of the algorithm with a test set with all its input vectors $\mathbf&#123;x&#125;$ different from those training $\mathbf&#123;x&#125;_n$&apos;s and $30%$ of its output labels $y$ to be positive. Which of the following is not true?</span><br><span class="line"></span><br><span class="line">a. PLA will halt after enough iterations.</span><br><span class="line"></span><br><span class="line">b. $E_&#123;out&#125; = 0.7$</span><br><span class="line"></span><br><span class="line">c. $E_&#123;in&#125; = 0.7$</span><br><span class="line"></span><br><span class="line">d. All $\mathbf&#123;Z&#125;_n$&apos;s are orthogonal to each other.</span><br><span class="line"></span><br><span class="line">e. The transformed data set is always linearly separable in the $\mathcal&#123;Z&#125;$ space.</span><br><span class="line"></span><br><span class="line">è¦æ˜ç™½è¿™ä¸ªé¢˜ç›®é¦–å…ˆè¦è¯»æ‡‚é¢˜æ„ã€‚</span><br><span class="line">é¢˜ç›®ä¸­å¯¹åŸæ¥çš„å‘é‡è¿›è¡Œäº†ç‰¹å¾è½¬æ¢ï¼ŒåŸæ¥å‘é‡ä¸ºdç»´ï¼Œæ–°çš„å‘é‡ä¸ºNç»´ï¼Œæ¯ä¸€ä¸ªæ ·æœ¬å¯¹åº”çš„æ–°çš„Zå‘é‡æ˜¯ $[ [[x_i = x_1]], [[x_i = x_2]],...,[[x_i = x_n]]]^T$,é¢˜ç›®ä¸­å·²ç»å‘ŠçŸ¥ï¼Œ$X_n$æ˜¯å„ä¸ç›¸åŒçš„ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç›´åˆ°$Z$å‘é‡ä¼šç±»ä¼¼äºï¼š$[1,0,...,0]^T,[0,1,...,0]^T,...,[0,0,...,1]$.åœ¨è¿™æ ·çš„æ ·æœ¬ä¸Šè¿›è¡ŒPLAç®—æ³•ã€‚æˆ‘ä»¬å¯ä»¥çŸ¥é“è¿™ç§æƒ…å†µæ˜¯æœ€å¥½çš„æƒ…å†µï¼Œæ˜¯å¯ä»¥è¢«shatterçš„ï¼ŒPLAä¸€å®šä¼šåœæ­¢ï¼Œè€Œä¸”ä»–ä»¬æ˜¯äº’ç›¸æ­£äº¤çš„ã€‚å› æ­¤ç­”æ¡ˆå°±åœ¨bï¼Œcä¸­é€‰æ‹©ã€‚è€Œæ—¢ç„¶PLAä¼šåœæ­¢è¯´æ˜æ‰¾åˆ°äº†æœ€å°çš„$E_&#123;in&#125;$ï¼Œæ­¤æ—¶$E_&#123;in&#125;$ä¸º0.å› æ­¤ç­”æ¡ˆé€‰c.</span><br><span class="line"></span><br><span class="line">è‡³äºcä¸ºä½•æ˜¯æ­£ç¡®çš„ï¼Œé¦–å…ˆï¼Œå› ä¸ºæµ‹è¯•é›†çš„æ ·æœ¬ä¸åŸæ¥çš„è®­ç»ƒé›†æ ·æœ¬ä¹Ÿæ²¡æœ‰ä¸€è‡´çš„ï¼Œå› æ­¤ä»–ä»¬å¾—åˆ°çš„è½¬æ¢åçš„zå‘é‡ä¸ºnç»´0å‘é‡.è€Œé¢˜ç›®ä¸­è¯´æ˜äº†$sign(0)=+1$ï¼Œå› æ­¤å®ƒä¼šå°†æ‰€æœ‰çš„æµ‹è¯•æ ·æœ¬éƒ½åˆ¤åˆ«ä¸ºpositiveï¼Œè€Œå®é™…ä¸Šåªæœ‰30%ä¸ºpositiveï¼Œå› æ­¤æµ‹è¯•å¾—åˆ°çš„$E_&#123;out&#125;$ä¸º70%.</span><br><span class="line"></span><br><span class="line">For Questions 13-15, consider the target function:</span><br><span class="line">$$</span><br><span class="line">f(x_1, x_2) = \mbox&#123;sign&#125;(x_1^2 + x_2^2 - 0.6)</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">**13. Generate a training set of N = 1000N=1000 points on $X=[âˆ’1,1]$ with uniform probability of picking each $\mathbf&#123;x&#125; \in \mathcal&#123;X&#125;$. Generate simulated noise by flipping the sign of the output in a random $10\%$ subset of the generated training set.**</span><br><span class="line"></span><br><span class="line">Carry out Linear Regression without transformation, i.e., with feature vector:</span><br><span class="line">$$</span><br><span class="line">(1,x_1,x_2)</span><br><span class="line">$$</span><br><span class="line"></span><br><span class="line">to find the weight $\mathbf&#123;w&#125;_&#123;\rm lin&#125;$, and use $\mathbf&#123;w&#125;_&#123;\rm lin&#125;$ directly for classification. What is the closest value to the classification $(0/1)$ in-sample error ($E_&#123;\rm in&#125;$)? Run the experiment $1000$ times and take the average $E_&#123;\rm in&#125;$ in order to reduce variation in your results.</span><br><span class="line"></span><br><span class="line">a. 0.1</span><br><span class="line"></span><br><span class="line">b. 0.3</span><br><span class="line"></span><br><span class="line">c. 0.5</span><br><span class="line"></span><br><span class="line">d. 0.7</span><br><span class="line"></span><br><span class="line">e. 0.9</span><br><span class="line"></span><br><span class="line">è¿™æ¬¡ä½œä¸šä¸ä¹‹å‰ä¸ä¸€æ ·äº†ï¼Œå¾ˆæ—©å°±é‡åˆ°ç¼–ç¨‹é¢˜ç›®äº†ï¼Œè€Œä¸”å‰é¢æœ‰å‡ é“é¢˜ç›®è™½ç„¶ä¸æ˜¯ç¼–ç¨‹é¢˜ç›®ï¼Œæˆ‘ä¹Ÿä¾ç„¶æ˜¯ç”¨ç¨‹åºè®¡ç®—å‡ºæ¥çš„ã€‚</span><br><span class="line"></span><br><span class="line">è¿™ä¸ªé¢˜ç›®æƒ³è¦ç”¨ç¨‹åºè§£ç­”å…¶å®ä¹Ÿä¸æ˜¯éå¸¸éš¾ã€‚åœ¨è¿™é‡Œæˆ‘ä½¿ç”¨çš„æ˜¯ä¸€æ­¥æ±‚æ³•ï¼Œè€Œä¸æ˜¯æ¢¯åº¦ä¸‹é™é‚£æ ·è¿­ä»£ã€‚é¦–å…ˆæ˜¯è¦ç”Ÿæˆæ•°æ®ï¼š</span><br><span class="line">```py</span><br><span class="line">def make_data():</span><br><span class="line">    result = []</span><br><span class="line">    for i in range(1000):</span><br><span class="line">        x1 = random.random()*2-1</span><br><span class="line">        x2 = random.random()*2-1</span><br><span class="line">        result.append([x1,x2,sign(pow(x1,2)+pow(x2,2)-0.6)])</span><br><span class="line">    return result</span><br></pre></td></tr></table></figure></p><p>ä¸ºäº†æ£€æŸ¥åšçš„æ•°æ®æ˜¯å¦æ­£ç¡®ï¼Œå¯¹ç”Ÿæˆçš„æ•°æ®è¿›è¡Œäº†å¯è§†åŒ–ï¼ˆå› ä¸ºè¾“å‡ºé•¿å®½æ¯”ä¾‹ä¸åŒï¼Œæ‰€ä»¥è¿™ä¸ªåœ†çœ‹ä¸Šå»ä¸æ˜¯æ­£åœ†ï¼‰ï¼š<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span><span class="params">(data,W=[])</span>:</span></span><br><span class="line">    nx = []</span><br><span class="line">    ny = []</span><br><span class="line">    ox = []</span><br><span class="line">    oy = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        <span class="keyword">if</span> data[i][<span class="number">-1</span>] == <span class="number">-1</span>:</span><br><span class="line">            nx.append(data[i][<span class="number">1</span>])</span><br><span class="line">            ny.append(data[i][<span class="number">2</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ox.append(data[i][<span class="number">1</span>])</span><br><span class="line">            oy.append(data[i][<span class="number">2</span>])</span><br><span class="line">    plt.scatter(nx,ny,marker=<span class="string">"x"</span>,c=<span class="string">"r"</span>)</span><br><span class="line">    plt.scatter(ox,oy,marker=<span class="string">"o"</span>,c=<span class="string">"g"</span>)</span><br><span class="line">    theta = np.linspace(<span class="number">0</span>, <span class="number">2</span> * np.pi, <span class="number">800</span>)</span><br><span class="line">    x, y = np.cos(theta) * <span class="number">0.77459666924148</span>, np.sin(theta) * <span class="number">0.77459666924148</span><span class="comment"># sqrt 0.6</span></span><br><span class="line">    plt.plot(x, y, color=<span class="string">'blue'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(W)!=<span class="number">0</span>:</span><br><span class="line">        print(W)</span><br><span class="line">        x = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">50</span>)</span><br><span class="line">        y = -W[<span class="number">1</span>] / W[<span class="number">2</span>] * x - W[<span class="number">0</span>] / W[<span class="number">2</span>]</span><br><span class="line">        plt.plot(x, y, color=<span class="string">"black"</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/RE%40%60F%29JRPQT_KE%7EFIVEC17K.png" alt="ç”Ÿæˆæ•°æ®"></p><p>ç„¶åé€šè¿‡çº¿æ€§å›å½’å¾—åˆ°$W = (X^TX)^{-1}X^TY$:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_regression_one_step</span><span class="params">(data)</span>:</span></span><br><span class="line">    X_matrix = []</span><br><span class="line">    Y_matrix = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        temp = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(data[i])<span class="number">-1</span>):</span><br><span class="line">            temp.append(data[i][j])</span><br><span class="line">        X_matrix.append(temp)</span><br><span class="line">        Y_matrix.append([data[i][<span class="number">-1</span>]])</span><br><span class="line">    X = np.mat(X_matrix)</span><br><span class="line">    Y = np.mat(Y_matrix)</span><br><span class="line"></span><br><span class="line">    W = (X.T*X).I*X.T*Y</span><br><span class="line">    <span class="keyword">return</span> W.T.tolist()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p><p>æœ€åé€šè¿‡åˆ†ç±»è®¡ç®—$E_{in}$ï¼š<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Ein</span><span class="params">(data,W)</span>:</span></span><br><span class="line">    err_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        res = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(W)):</span><br><span class="line">            res += W[j]*data[i][j]</span><br><span class="line">        <span class="keyword">if</span> sign(res) != data[i][<span class="number">-1</span>]:</span><br><span class="line">            err_num+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> err_num</span><br></pre></td></tr></table></figure></p><p>è¿è¡Œ1000æ¬¡å–è¾“å‡ºä¸ºï¼š<code>0.50614</code></p><p>å› æ­¤ç­”æ¡ˆé€‰c.å…¶å®æˆ‘ä»¬ä¹Ÿå¯ä»¥æƒ³è±¡å¾—åˆ°è¿™ä¸ªé”™è¯¯ç‡æ˜¯æ¥è¿‘0.5çš„ï¼Œå¯è§†åŒ–çš„ç»“æœå¦‚ä¸‹ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/_%7BA%60P%28N%255%7B4U8%25%25NQ%24H%408O9.png" alt=""></p><p>ä¸ç®¡æ€ä¹ˆç”»è¿™æ¡çº¿ï¼Œæ€»ä¼šæœ‰æ¥è¿‘ä¸€åŠçš„åŒºåŸŸè¢«åˆ†é”™.</p><p><strong>14. Now, transform the training data into the following nonlinear feature vector:$(1,x_1,x_2,x_1x_2,x_1^2,x_2^2)$.Find the vector $\tilde{\mathbf{w}}$ that corresponds to the solution of Linear Regression, and take it for classification. Which of the following hypotheses is closest to the one you find using Linear Regression on the transformed input? Closest here means agrees the most with your hypothesis (has the most probability of agreeing on a randomly selected point).</strong></p><p>a. $g(x_1,x_2) = sign(-1-1.5x_1+0.08x_2+0.13x_1x_2+0.05x_1^2+1.5x_2^2)$</p><p>b. $g(x_1,x_2) = sign(-1-1.5x_1+0.08x_2+0.13x_1x_2+0.05x_1^2+0.05x_2^2)$</p><p>c. $g(x_1,x_2) = sign(-1-0.05x_1+0.08x_2+0.13x_1x_2+1.5x_1^2+15x_2^2)$</p><p>d. $g(x_1,x_2) = sign(-1-0.05x_1+0.08x_2+0.13x_1x_2+1.5x_1^2+1.5x_2^2)$</p><p>e. $g(x_1,x_2) = sign(-1-0.05x_1+0.08x_2+0.13x_1x_2+15x_1^2+1.5x_2^2)$</p><p>ç»è¿‡ç‰¹å¾è½¬æ¢åè¿›è¡Œçº¿æ€§å›å½’ï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„Wå‘é‡ã€‚å®é™…ä¸Šï¼Œæ¯æ¬¡è¾“å‡ºçš„å‘é‡ä¸å®¹æ˜“çœ‹å‡ºæ¥ä»–ä»¬çš„ç›¸ä¼¼æ€§ï¼Œæœ€å¥½çš„åŠæ³•å°±æ˜¯ä»zç©ºé—´è½¬å›å»çœ‹å®é™…çš„åˆ†ç±»æ•ˆæœã€‚<br>ä¸ºäº†å®Œæˆè¿™é“é¢˜ç›®ï¼Œéœ€è¦æ·»åŠ ä¸€äº›æ–°çš„å‡½æ•°ã€‚é¦–å…ˆæ˜¯ä¸€ä¸ªç‰¹å¾è½¬æ¢çš„å‡½æ•°ï¼š<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform_data</span><span class="params">(data)</span>:</span></span><br><span class="line">    t_data = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        temp = [<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        temp.append(data[i][<span class="number">1</span>])</span><br><span class="line">        temp.append(data[i][<span class="number">2</span>])</span><br><span class="line">        temp.append(data[i][<span class="number">1</span>]*data[i][<span class="number">2</span>])</span><br><span class="line">        temp.append(data[i][<span class="number">1</span>]*data[i][<span class="number">1</span>])</span><br><span class="line">        temp.append(data[i][<span class="number">2</span>]*data[i][<span class="number">2</span>])</span><br><span class="line">        temp.append(data[i][<span class="number">-1</span>])</span><br><span class="line">        t_data.append(temp)</span><br><span class="line">    <span class="keyword">return</span> t_data</span><br></pre></td></tr></table></figure></p><p>å¦å¤–ï¼Œæˆ‘é‡æ–°å†™äº†ä¸€ä¸ªå¯è§†åŒ–çš„å‡½æ•°ï¼Œæ¥ç”»å‡ºå­¦ä¹ ååˆ†ç±»çš„ç»“æœï¼Œå¹¶ä¸”ä¸åŸå§‹çš„ç•Œé™åšå¯¹æ¯”ã€‚å®é™…ä¸Šï¼Œå¦‚æœä¸é€šè¿‡å¯è§†åŒ–ï¼Œå•å•ä»Wå‘é‡æˆ‘ä»¬ä¸æ˜¯å¾ˆå®¹æ˜“èƒ½çœ‹å‡ºæ¥ä»–ä»¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ï¼Œå¦‚ï¼š</p><script type="math/tex; mode=display">W = [-1.233493970901365, 0.0289716107001069, -0.04249028735529122, -0.06563216707578017, 1.8906604891218894, 2.014485177474217]</script><p>å¾—åˆ°çš„åˆ†ç±»æ•ˆæœå¦‚ä¸‹ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/%24NF51V%5BFC0ELUWH8QXJ%7D%5BDE.png" alt=""></p><p>è€Œå…¶ä»–å‡ ä¸ªé€‰é¡¹æŒ‰ç…§a,b,c,d,eçš„é¡ºåºåˆ†åˆ«å¾—åˆ°ä¸‹é¢çš„åˆ†ç±»æ•ˆæœï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/XZSCVFU97%28EH4V5%29X%5DDT%7E70.png" alt=""><br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/V%28%7D%7BZ%5BQ2L%402145%5D%7D%5D9FW%7E_B.png" alt=""><br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/R67TAG1%7BEXKD2%406Y_WS%28SZA.png" alt=""><br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/%5BI13AGR586%24DR9%5BG3%5D038%40F.png" alt=""><br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/2HM1%5D84NO9J%29%7BUA%7DNI%7E2%5D90.png" alt=""></p><p>å¯ä»¥å¾ˆæ˜æ˜¾çš„çœ‹å‡ºæ¥ï¼Œç­”æ¡ˆé€‰æ‹©d.</p><p><strong>15. Following Question 14, what is the closest value to the classification out-of-sample error $E_{\rm out}$ of your hypothesis? Estimate it by generating a new set of 1000 points and adding noise as before. Average over 1000 runs to reduce the variation in your results.</strong></p><p>a. 0.1</p><p>b. 0.3</p><p>c. 0.5</p><p>d. 0.7</p><p>e. 0.9</p><p>è¿™é“é¢˜ç›®éœ€è¦æ¥æµ‹é‡$E_{out}$ï¼Œå¹¶ä¸”è¦åŠ ä¸Šå™ªå£°ã€‚å…¶å®è¿˜æ˜¯å¾ˆç®€å•çš„ï¼Œåªéœ€è¦åšç®€å•çš„æ”¹åŠ¨ï¼Œæ¥è®¡ç®—$E_{out}$.</p><p>æ—¢ç„¶æ˜¯$E_{out}$ï¼Œé‚£å°±éœ€è¦æ¯æ¬¡éƒ½é‡æ–°ç”Ÿæˆæ•°æ®ï¼Œè¿™å°±æ˜¯å’Œ$E_{in}$çš„åŒºåˆ«ï¼Œå†åŠ ä¸ªè½¬æ¢ï¼Œé™¤æ­¤ä¹‹å¤–æ˜¯ä¸€æ ·çš„ï¼Œå› æ­¤å¯ä»¥å¤ç”¨$E_{in}$å‡½æ•°ï¼š<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Eout</span><span class="params">(W)</span>:</span></span><br><span class="line">    data = make_data()</span><br><span class="line">    t_data = transform_data(data)</span><br><span class="line">    <span class="keyword">return</span> Ein(t_data,W)</span><br></pre></td></tr></table></figure></p><p>ç„¶åè¿è¡Œ1000æ¬¡å°±å¯ä»¥äº†ï¼Œæœ€åå¾—åˆ°çš„é”™è¯¯ç‡ï¼š<code>0.126856</code>,ç­”æ¡ˆé€‰a.</p><p>For Questions 16-17, you will derive an algorithm for the multinomial (multiclass) logistic regression model.<br><strong>16. For a $K$-class classification problem, we will denote the output space $\mathcal{Y} = \{1, 2, \ldots, K\}$. The hypotheses considered by the model are indexed by a list of weight vectors $(\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_K)$, each weight vector of length $d+1$. Each list represents a hypothesis</strong></p><script type="math/tex; mode=display">h_y( X) = \frac {e^{W_y^TX}} {\sum _{k=1} ^{K} e^{W_k^TX}}</script><p>that can be used to approximate the target distribution $P(y|\mathbf{x})$P. The model then seeks for the maximum likelihood solution over all such hypotheses.(Note:$X$ = $\mathbf{x}$)</p><p>For general $K$, derive an $E_{\text{in}}(\mathbf{w}_1, \cdots, \mathbf{w}_K)$ like page 11 of Lecture 10 slides by minimizing the negative log likelihood. What is the resulting $E_{\text{in}}$?</p><p>a. \frac 1 N \sum _{n = 1}^N \left ( \sum _{k=1} ^K (W_k^TX_n - W_{y_n}^TX_n)\right)</p><p>b. \frac 1 N \sum _{n = 1}^N \left  (\sum _{k=1} ^K W_k^TX_n - W_{y_n}^TX_n\right)</p><p>c. \frac 1 N \sum _{n = 1}^N \left ( \ln (\sum _{k=1} ^K e^{W_k^TX_n}) - W_{y_n}^TX_n\right)</p><p>d. none of the other choices</p><p>e. \frac 1 N \sum _{n = 1}^N \left ( \ln (\sum _{k=1} ^K e^{W_k^TX_n} - e^{W_{y_n}^TX_n})\right)</p><p>è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰æ„æ€çš„æ¦‚ç‡çš„å®šä¹‰ï¼Œå®ƒä¿è¯äº†åˆ†åˆ°å„ä¸ªåˆ†ç±»çš„æ¦‚ç‡å’ŒåŠ èµ·æ¥ä¸º1ï¼Œè€Œä¹‹å‰çš„å‡è®¾æ˜¯ä¸èƒ½ä¿è¯è¿™ä¸ªé—®é¢˜çš„ã€‚</p><p>å¯¹äº$E_{in}$çš„å¤„ç†ï¼Œä¹Ÿæ˜¯ä¸€è‡´çš„ã€‚æŒ‰ç…§ä¹‹å‰çš„æƒ³æ³•æ¥ä¸€æ­¥ä¸€æ­¥åšã€‚é¦–å…ˆï¼Œè®¡ç®—å‡ºåœ¨è¯¥$W$çš„æƒ…å†µä¸‹ï¼Œå‡ºç°å½“å‰åˆ†ç±»çš„æ¦‚ç‡ï¼Œå¯ä»¥çŸ¥é“çš„æ˜¯äº’ç›¸è¿ä¹˜ï¼Œæœ€åä¸ºäº†è®©ä»–å˜æˆåŠ æ³•ï¼Œä»¥åŠæœ€å°åŒ–ï¼ˆè€Œéæœ€å¤§åŒ–ï¼‰ï¼Œéœ€è¦åŠ ä¸€ä¸ªè´Ÿå·ï¼Œå¿½ç•¥P(X=X_i)ã€‚</p><p>å¯ä»¥å¾—åˆ°ä¸‹é¢çš„è¿‡ç¨‹ï¼š</p><script type="math/tex; mode=display">\prod_{i=n}^{N} \frac {e^{W_{y_n}^TX_{n}}} {\sum _{k=1} ^{K} e^{W_k^TX_n}}</script><script type="math/tex; mode=display">=> -\sum_{n = 1}^N( \ln (e^{W_{y_n}^TX_n}) - \ln (\sum _{k = 1}^K e^{W_{k}^TX_n}))</script><script type="math/tex; mode=display">  =>\sum _{n = 1}^N \left ( \ln (\sum _{k=1} ^K e^{W_k^TX_n} - W_{y_n}^TX_n)\right)</script><p>æœ€åé™¤ä»¥Nï¼Œå¾—åˆ°é€‰é¡¹c.</p><p><strong>17. For the $E_{\text{in}}$ derived above, its gradient $\nabla E_{\text{in}}$ can be represented by $\left(\frac{\partial E_{\text{in}}}{\partial\mathbf{w}_1}, \frac{\partial E_{\text{in}}}{\partial\mathbf{w}_2}, \cdots, \frac{\partial E_{\text{in}}}{\partial\mathbf{w}_K}\right)$, write down $\frac{\partial E_{\text{in}}}{\partial\mathbf{w}_i}$ .</strong></p><p>a. \frac 1 N \sum _{n=1}^N \left(\sum _{i = 1} ^K(e^{W_i^TX_n} - [[y_n=i]])X_n\right)</p><p>b. \frac 1 N \sum_{n=1}^N ((h_i(x_n)-1)x_n)</p><p>c. none of the other choices</p><p>d. \frac 1 N \sum _{n=1}^N \left((h_i(x_n) - [[y_n=i]])X_n\right)</p><p>e. \frac 1 N \sum _{n=1}^N \left( \sum _(i = 1)^K(e^{W_i^TX_n}-1)X_n\right)</p><p>è¿™ä¸ªé¢˜ç›®å°±æ˜¯å¯¹ä¸Šé¢çš„é¢˜ç›®çš„é€‰æ‹©ç»“æœè¿›è¡Œæ±‚å¯¼å³å¯ã€‚è™½ç„¶æ¯”è¾ƒå¤æ‚ï¼Œä½†æ˜¯ä¸ç®—éš¾ã€‚ä¸€æ­¥æ­¥ç®—ä¸‹æ¥å°±æˆï¼Œæ­£ç¡®ç­”æ¡ˆæ˜¯d.</p><p>For Questions 18-20, you will play with logistic regression. Please use the following set for training:</p><p><a href="https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_algo/hw3_train.dat" target="_blank" rel="noopener">https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_algo/hw3_train.dat</a></p><p>and the following set for testing:</p><p><a href="https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_algo/hw3_test.dat" target="_blank" rel="noopener">https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_algo/hw3_test.dat</a></p><p><strong>18. Implement the fixed learning rate gradient descent algorithm for logistic regression. Run the algorithm with $\eta = 0.001$ and $T = 2000$. What is $E_{out}(g)$ from your algorithm, evaluated using the $0/1$ error on the test set?</strong></p><p>a. 0.475</p><p>b. 0.412</p><p>c. 0.322</p><p>d. 0.220</p><p>e. 0.103</p><p><strong>19. Implement the fixed learning rate gradient descent algorithm for logistic regression. Run the algorithm with $\eta = 0.01$ and $T = 2000$. What is $E_{out}(g)$ from your algorithm, evaluated using the $0/1$ error on the test set?</strong></p><p>a. 0.475</p><p>b. 0.412</p><p>c. 0.322</p><p>d. 0.220</p><p>e. 0.103</p><p><strong>20. Implement the fixed learning rate stochastic gradient descent algorithm for logistic regression. Instead of randomly choosing nn in each iteration, please simply pick the example with the cyclic order $n = 1, 2, \ldots, N, 1, 2, \ldots$,Run the algorithm with $\eta = 0.001$ and $T = 2000$. What is $E_{out}(g)$ from your algorithm, evaluated using the $0/1$ error on the test set?</strong></p><p>a. 0.475</p><p>b. 0.412</p><p>c. 0.322</p><p>d. 0.220</p><p>e. 0.103</p><p>è¿™3é“é¢˜ç›®ï¼Œå¯ä»¥ç”¨ä¸€å¥—æ–¹æ³•åšå‡ºæ¥ï¼Œå¯ä»¥å¼•ç”¨ç¬¬7é¢˜çš„ç¨‹åºä¸­çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸13é¢˜ç¨‹åºä¸­çš„å¯¹äº$E_{in}$çš„ä¼°è®¡ã€‚å½“ç„¶ï¼Œå¯¹äºä¹‹å‰ä»£ç çš„è¿˜éœ€è¦è¿›è¡Œä¸€ä¸ä¿®æ”¹ã€‚<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gradient_decent_7 <span class="keyword">as</span> gd</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span>  numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> _linear_regression_13 <span class="keyword">as</span> lr</span><br><span class="line">now = <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+math.exp(-x))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_logistic_gradient</span><span class="params">(lastW,data)</span>:</span></span><br><span class="line">    W = np.mat([lastW]).T</span><br><span class="line">    X = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        temp = data[i][<span class="number">0</span>:<span class="number">-1</span>]</span><br><span class="line">        temp.append(<span class="number">1</span>)</span><br><span class="line">        X.append(np.mat([temp]).T)</span><br><span class="line">    gra = logistic(-data[<span class="number">0</span>][<span class="number">-1</span>]*(W.T*X[<span class="number">0</span>])[<span class="number">0</span>][<span class="number">0</span>])*(-data[<span class="number">0</span>][<span class="number">-1</span>]*X[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(data)):</span><br><span class="line">        gra = gra + logistic(-data[i][<span class="number">-1</span>]*(W.T*X[i])[<span class="number">0</span>][<span class="number">0</span>])*(-data[i][<span class="number">-1</span>]*X[i])</span><br><span class="line">    <span class="comment">#print(gra)</span></span><br><span class="line">    <span class="keyword">return</span> ((<span class="number">1</span>/len(data))*gra).T.tolist()[<span class="number">0</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stochastic_gradient_init</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span>  now</span><br><span class="line">    now = <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stochastic_gradient</span><span class="params">(lastW,data)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> now</span><br><span class="line">    <span class="keyword">if</span> now == len(data):</span><br><span class="line">        now = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    W = np.mat([lastW]).T</span><br><span class="line">    temp = data[now][<span class="number">0</span>:<span class="number">-1</span>]</span><br><span class="line">    temp.append(<span class="number">1</span>)</span><br><span class="line">    X = np.mat([temp]).T</span><br><span class="line">    gra = logistic(-data[now][<span class="number">-1</span>]*(W.T*X)[<span class="number">0</span>][<span class="number">0</span>])*(-data[now][<span class="number">-1</span>]*X)</span><br><span class="line">    now += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> gra.T.tolist()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readDataFrom</span><span class="params">(filename)</span>:</span></span><br><span class="line">    result = []</span><br><span class="line">    separator = re.compile(<span class="string">'\t|\b| |\n'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(filename,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        line = f.readline()[<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">        <span class="comment">#print(line)</span></span><br><span class="line">        <span class="keyword">while</span> line:</span><br><span class="line">            temp = separator.split(line)</span><br><span class="line">            <span class="comment">#print(temp)</span></span><br><span class="line"></span><br><span class="line">            abc = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> temp]</span><br><span class="line">            <span class="comment">#print(abc)</span></span><br><span class="line">            result.append(abc)</span><br><span class="line">            <span class="comment">#print(result)</span></span><br><span class="line">            line = f.readline()[<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure></p><p>ä¸Šé¢å±•ç¤ºäº†5ä¸ªå‡½æ•°ï¼Œç¬¬ä¸€ä¸ªlogisticå‡½æ•°å°±æ˜¯ä¸ºäº†è®¡ç®—logisticå‡½æ•°çš„å€¼ï¼Œç„¶ååˆ†åˆ«æœ‰ä¸¤ä¸ªè®¡ç®—æ¢¯åº¦çš„å‡½æ•°ï¼Œä¸ºäº†å¯ä»¥å¤ç”¨ä¹‹å‰çš„ä»£ç ï¼Œé‡‡ç”¨äº†ä¸€æ ·çš„å½¢å¼ï¼Œä¸è¿‡å€¼å¾—æ³¨æ„çš„æ˜¯å¤šå¢åŠ äº†ä¸€ä¸ªdataå‚æ•°ï¼Œè¿™æ˜¯ä¹‹å‰æ²¡æœ‰çš„ã€‚ä¸ºäº†å®ç°é¢˜ç›®ä¸­è¦æ±‚çš„é¡ºåºé‡‡å–ä¸€ä¸ªæ ·æœ¬æ¥åšä¿®æ­£ï¼Œä½¿ç”¨äº†ä¸€ä¸ªå…¨å±€å˜é‡æ¥è®°å½•å½“å‰çš„æ ·æœ¬ç´¢å¼•ï¼ŒåŒæ—¶æœ‰ä¸€ä¸ªåˆå§‹åŒ–å‡½æ•°ï¼Œå¯ä»¥è®©æ ·æœ¬ç´¢å¼•å½’é›¶ã€‚</p><p>è¿˜æœ‰ä¸€ä¸ªreadDataFromä¸ç”¨å¤šè¯´ï¼Œæ ¹æ®èµ„æ–™çš„è¾“å…¥å½¢å¼è¿›è¡Œé€‚å½“è°ƒæ•´ã€‚</p><p>æœ€ååœ¨ä¸»å‡½æ•°ä¸­æ•´åˆè¾“å‡ºç»“æœï¼š<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    data = readDataFrom(<span class="string">"./hw3_train.dat"</span>)</span><br><span class="line">    start = [<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>,len(data[<span class="number">0</span>]))]</span><br><span class="line">    i_times = <span class="number">2000</span></span><br><span class="line">    stochastic_gradient_init()</span><br><span class="line">    W1 = gd.gradient_decent(start,get_logistic_gradient,<span class="number">0.001</span>,i_times,data)</span><br><span class="line">    W2 = gd.gradient_decent(start,get_logistic_gradient,<span class="number">0.01</span>,i_times,data)</span><br><span class="line">    W3 = gd.gradient_decent(start,stochastic_gradient,<span class="number">0.001</span>,i_times,data)</span><br><span class="line">    err1 = lr.Ein(data,W1)</span><br><span class="line">    err2 = lr.Ein(data,W2)</span><br><span class="line">    err3 = lr.Ein(data,W3)</span><br><span class="line">    out_data = readDataFrom(<span class="string">"./hw3_test.dat"</span>)</span><br><span class="line">    err_out1 = lr.Ein(out_data,W1)</span><br><span class="line">    err_out2 = lr.Ein(out_data,W2)</span><br><span class="line">    err_out3 = lr.Ein(out_data,W3)</span><br><span class="line">    print(<span class="string">"Ein:"</span>,err1,err1/len(data),err2,err2/len(data),err3,err3/len(data))</span><br><span class="line">    print(<span class="string">"Eout:"</span>,err_out1,err_out1/len(out_data),err_out2,err_out2/len(out_data),err_out3,err_out3/len(out_data))</span><br></pre></td></tr></table></figure></p><p>æˆ‘å°†èµ·å§‹çš„Wè®¾å®šä¸º0å‘é‡ï¼Œä¸åŒçš„èµ·å§‹å‘é‡å¯èƒ½å¾—åˆ°ä¸åŒçš„æ€§èƒ½ã€‚</p><p>æœ€åçš„ç»“æœï¼Œåˆ†åˆ«æ˜¯3é“é¢˜ç›®çš„${E_in}$ä¸$E_{out}$.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Ein: 430 0.43 204 0.204 427 0.427</span><br><span class="line">Eout: 1354 0.4513333333333333 678 0.226 1352 0.45066666666666666</span><br></pre></td></tr></table></figure><p>å› æ­¤æˆ‘è®¤ä¸ºç­”æ¡ˆæ˜¯a,d,a. å¯ä»¥çœ‹åˆ°å­¦ä¹ ç‡è¿‡ä½çš„è¯å¯èƒ½ä¼šä½¿å¾—ä¸‹é™é€Ÿåº¦è¿‡æ…¢ï¼Œè€Œä¸”éšæœºæ¢¯åº¦ä¸‹é™çš„æ€§èƒ½æœ‰æ—¶å€™ä¸è§å¾—æ¯”æ™®é€šçš„æ¢¯åº¦ä¸‹é™å·®ï¼Œä½†æ˜¯é€Ÿåº¦å´å¤§å¤§æé«˜ã€‚</p><p>å°†è¿­ä»£æ¬¡æ•°æ›´æ–°ä¸º10000ï¼Œ3ä¸ªå­¦ä¹ æ–¹æ³•éƒ½å–å¾—äº†ä¸é”™çš„æ•ˆæœï¼Œè€Œæ­¥é•¿å¤§çš„æ€§èƒ½åè€Œä¸å¦‚ä¹‹å‰ï¼Œè¯´æ˜å› ä¸ºæ­¥é•¿è¿‡å¤§ï¼Œå®ƒåªèƒ½å†æœ€ä½ç‚¹å¾˜å¾Šå¾ˆéš¾å–å¾—æ›´ä½çš„ä½ç½®ï¼Œè€Œæ­¥é•¿å°çš„ä¾ç„¶æ²¡æœ‰è¾¾åˆ°æé™ï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Ein: 236 0.236 206 0.206 235 0.235</span><br><span class="line">Eout: 779 0.25966666666666666 695 0.23166666666666666 776 0.25866666666666666</span><br></pre></td></tr></table></figure></p><h2 id="p-s"><a href="#p-s" class="headerlink" title="p.s."></a>p.s.</h2><p>1.pythonä¸­çš„å…¨å±€å˜é‡ï¼Œlistä¸ºå¼•ç”¨ï¼Œè€Œèµ‹å€¼ä¸€ä¸ªå¸¸æ•°åˆ™ä¼šé‡æ–°å£°æ˜ï¼Œä¸ºäº†é¿å…æ­§ä¹‰ã€‚å› æ­¤æƒ³è¦æ”¹å˜å…¨å±€å˜é‡çš„å¸¸æ•°ï¼Œéœ€è¦æ·»åŠ å…³é”®è¯globalã€‚</p><p>2.ç‰›é¡¿æ–¹å‘ä¸æµ·æ£®çŸ©é˜µã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> homework </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”Nonlinear Transformation</title>
      <link href="/2018/09/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Nonlinear-Transformation/"/>
      <url>/2018/09/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Nonlinear-Transformation/</url>
      
        <content type="html"><![CDATA[<p>ä¹‹å‰æˆ‘ä»¬å­¦ä¹ çš„æ‰€æœ‰æ¨¡å‹ï¼Œéƒ½æ˜¯linear modelã€‚æˆ‘ä»¬åšçš„éƒ½æ˜¯ç”¨ä¸€æ¡ç›´çº¿ï¼ˆæˆ–è€…å¹³é¢ç­‰ï¼‰æ¥åˆ†ç±»ï¼Œæˆ–è€…æ‹Ÿåˆï¼Œæˆ–è€…æ˜¯é€šè¿‡è¯¥ç›´çº¿æ¥é¢„æµ‹æ¦‚ç‡ã€‚ä½†æ˜¯ç°å®ä¸­å¾ˆå¤šæ—¶å€™çº¿æ€§ä¸ä¸€å®šèƒ½åšå¾—å¾ˆå¥½ã€‚<a id="more"></a>å¦‚ä¸‹å›¾ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/5%4014N%288G7K0JNSKXZF%29DU_7.png" alt=""></p><p>ä¼¼ä¹ç”¨ä¸€ä¸ªåœ†ï¼Œèƒ½æ›´å¥½åœ°å¯¹è¿™äº›æ ·æœ¬è¿›è¡Œåˆ†ç±»ã€‚å®é™…ä¸Šï¼Œä¸Šå›¾çš„åˆ†ç±»çš„ä¾æ®æ˜¯ï¼š</p><script type="math/tex; mode=display">h(X) = sign(0.6-x_1^2-x_2^2)</script><p>å› æ­¤ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¼•å…¥äº†äºŒæ¬¡å‡è®¾ï¼ˆquadraticï¼‰ã€‚<br>åœ¨ä¸Šå¼ä¸­ï¼Œå¦‚æœæˆ‘ä»¬ä»¤$z_0 = x_0 = 1, z_1 = x_1^2 ,z_2 = x_2^2$ï¼Œé‚£ä¹ˆå°±å‘ç°ï¼Œå®é™…ä¸Šä¸Šé¢ä¸­å¯¹äº$x$çš„äºŒæ¬¡å‡è®¾å®é™…ä¸Šè½¬æ¢ä¸ºäº†å¯¹$Z$çš„çº¿æ€§å‡è®¾ã€‚ä¸€ä¸ªæ ·æœ¬åœ¨xç©ºé—´ä¸Šæ˜¯å¾ˆéš¾çº¿æ€§åˆ†å‰²çš„ï¼Œæ— è®ºæ€æ ·éƒ½ä¸èƒ½å¾—åˆ°è¾ƒå¥½çš„$E_{in}$ï¼Œä½†æ˜¯å±•å¼€åˆ°zç©ºé—´ä¸Šä¹Ÿè®¸å°±æ˜¯çº¿æ€§å¯åˆ†çš„äº†ã€‚</p><p>å› æ­¤æ€ä¹ˆåšå®é™…ä¸Šå¾ˆç®€å•ï¼Œä¹‹å‰çš„è®­ç»ƒé›†æ˜¯$D$ï¼Œæˆ‘ä»¬é€šè¿‡å±•å¼€åˆ°zç©ºé—´æ¥å¾—åˆ°è®­ç»ƒé›†$Dâ€™$ï¼Œç”¨è®­ç»ƒé›†æ¥è¿›è¡Œä¹‹å‰çš„ç”¨äºçº¿æ€§æ¨¡å‹çš„è®­ç»ƒã€‚</p><p>é€šè¿‡è¿™ä¸ªï¼Œæˆ‘ä»¬ä»¿ä½›è¿›å…¥äº†æ–°ä¸–ç•Œï¼šæˆ‘ä»¬å¯ä»¥ä»»æ„æ¥æ„å»ºå¤šé¡¹å¼(ç‰¹å¾è½¬æ¢)ï¼Œè¾¾åˆ°éçº¿æ€§å­¦ä¹ çš„ç»“æœã€‚</p><p>ä½†æ˜¯ä¸–ç•Œä¸Šä¸ä¼šå¤©ä¸Šæ‰é¦…é¥¼ã€‚æˆ‘ä»¬çŸ¥é“ä¸ä¼šæœ‰è¿™ä¹ˆç™½ç™½çš„å¥½äº‹å‘ç”Ÿã€‚æ—¢ç„¶å¤šé¡¹å¼è¿™ä¹ˆå¼ºå¤§ï¼Œå®ƒå¿…ç„¶ä¼šä»˜å‡ºæ›´å¤šçš„ä»£ä»·ï¼Œä¸‹é¢æˆ‘ä»¬æ¥ç®€å•è¯´æ˜ä¸‹å®ƒçš„priceã€‚</p><p>å‡å¦‚åŸå§‹ç‰¹å¾æœ‰dä¸ªï¼Œæˆ‘ä»¬æ„å»ºçš„æ˜¯æœ€é«˜æ¬¡ä¸ºQçš„Qæ¬¡å¤šé¡¹å¼($Q \leq d$)ã€‚é‚£ä¹ˆï¼Œæ„é€ çš„æ–°çš„æ ·æœ¬é›†ä¼šæœ‰å¤šå°‘ä¸ªç»´åº¦ï¼Ÿ</p><script type="math/tex; mode=display">\Phi _Q(X) = \left ( \begin{matrix}     1,\\    x_1,x_2,...,x_d,\\    x_1^2,x_1x_2,...,x_d^2,\\    ...,\\    x_1^Q,x_1^{Q-1}x_2,...,x_d^Q\\        \end{matrix}   \right )</script><p>å‡å¦‚æˆ‘ä»¬ä¹˜è½¬æ¢åçš„ç»´åº¦æ˜¯$\overline d$ï¼Œåˆ™ $\overline d = C_{Q+d}^d-1 =&gt;O(Q^d)$(å¦‚ä½•å¾—åˆ°è¿™ä¸ªå€¼éœ€è¦æ’åˆ—ç»„åˆçŸ¥è¯†ï¼Œåœ¨æ­¤å¤„çŸ¥é“å³å¯).</p><p>è¿™æ„å‘³ç€$\overline W$æ˜¯ï¼ˆ$1+\overline d$ï¼‰ç»´çš„å‘é‡ï¼Œè€Œè½¬æ¢åçš„$vc dimension \leq \overline d+1$(ä¹‹å‰çš„VC boundä¸­è¯æ˜è¿‡äº†äºŒå…ƒåˆ†ç±»ç»´åº¦ä¸ºdçš„æ—¶å€™ï¼Œvc dimensionä¸ºd+1).vc dimensionå¯èƒ½ä¼šå¢åŠ å¾ˆå¤šï¼Œä¹Ÿå°±æ„å‘³ç€æˆ‘ä»¬éœ€è¦éå¸¸å¤šçš„èµ„æ–™å¯èƒ½æ‰èƒ½å¾—åˆ°è¾ƒå¥½çš„å‡†ç¡®åº¦ï¼ŒåŒæ—¶ä¹Ÿä¼šæå¤§åœ°å¢åŠ å­˜å‚¨ç©ºé—´ï¼Œä»¥åŠç®—æ³•çš„å­¦ä¹ é€Ÿåº¦ã€‚</p><p>æˆ‘ä»¬åˆ—å‡ºæ¬¡æ•°ä¸º1-Qæ—¶å€™çš„ç»´åº¦å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display">\Phi _0(x) = (1)</script><script type="math/tex; mode=display">\Phi _1(x) = (\Phi _0(x) , x_1,x_2,...,x_d)</script><script type="math/tex; mode=display">\Phi _2(x) = (\Phi _1(x) , x_1^2,x_1x_2,...,x_d^2)</script><p>â€¦</p><script type="math/tex; mode=display">\Phi _Q(x) = (\Phi _{Q-1}(x),x_1^Q,x_1^{Q-1}x_2,...,x_d^Q)</script><p>å› æ­¤ï¼šæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š</p><p>$H_0 \subset H_1 \subset H_2 \subset H_3 \subset â€¦ \subset H_Q$;</p><p>ä»è€Œå¾—åˆ°ï¼š</p><p>$ d_{vc}(H_0) \leq d_{vc}(H_1) \leq d_{vc}(H_2) \leq â€¦ \leq d_{vc}(H_Q)$;</p><p>è€Œå¯¹äº$E_in$æ¥è¯´ï¼Œä¸é”™çš„æ¶ˆæ¯æ˜¯$E_{in}$æ˜¯åœ¨ä¸ä¼šå˜å¤§çš„ã€‚å› ä¸ºå®ƒæœ€å¤šæœ€å¥½çš„çº¿ä¸ä¹‹å‰ä¸€æ ·ï¼š</p><p>$ E_{in} (g_0) \geq  E_{in} (g_1) \geq E_{in} (g_2) \geq â€¦ \geq E_{in}(g_Q)$;</p><p>æ›´é‡è¦çš„ä¸€ä»¶äº‹ï¼Œä¸çŸ¥é“ä½ æ˜¯å¦è¿˜è®°å¾—è¿™ä¸ªå­¦ä¹ æ›²çº¿ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/M%29P32DW%29EE9%7BWB%246A08T8%29X.png" alt=""></p><p>ä»ä¸Šé¢å¯ä»¥çœ‹å‡ºæ¥ï¼Œç»´åº¦ä¸ªæ•°å¢åŠ ï¼Œ$E_{out}$å¹¶ä¸æ˜¯ä¸€ç›´ä¸‹é™çš„ï¼Œæœ€å¥½çš„ç»´åº¦ä¸ªæ•°æ˜¯åœ¨ä¸­é—´æŸä¸ªåœ°æ–¹ã€‚è€Œ$E_{in}$ä»æ¥å°±ä¸æ˜¯æˆ‘ä»¬å…³æ³¨çš„é‡ç‚¹ï¼Œæˆ‘ä»¬å°½å¯èƒ½å‡å°‘ä»–ï¼Œæ˜¯ä¸ºäº†å¾—åˆ°æ›´å¥½çš„$E_{out}$,è€Œè¿‡åˆ†å¢åŠ ç»´åº¦ä¸ªæ•°ï¼Œå¯èƒ½ä¼šæœ¬æœ«å€’ç½®ã€‚$E_{out}$æ‰æ˜¯æˆ‘ä»¬æƒ³è¦çš„ã€‚</p><p>å®é™…ä¸­ï¼Œæˆ‘ä»¬åœ¨å­¦ä¹ æ—¶å€™åº”è¯¥å…ˆä»ä½çš„ç»´åº¦å¼€å§‹ï¼Œå†æ…¢æ…¢å‘ä¸Šå¢åŠ ï¼Œç›´åˆ°å¾—åˆ°æƒ³è¦çš„$E_{in}$ï¼Œå¾€å¾€çº¿æ€§çš„å­¦ä¹ å¹¶ä¸åƒäººä»¬æƒ³è±¡ä¸­çš„é‚£æ ·æ•ˆæœå¾ˆå·®ï¼Œå¯èƒ½ä»è¾ƒä½çš„ç»´åº¦å°±å¯ä»¥å¾—åˆ°è¾ƒå¥½çš„ç»“æœã€‚å¦‚æœä¸€å¼€å§‹å°±ç”¨å¾ˆå¤šçš„ç»´åº¦ï¼Œå¯èƒ½ç›´æ¥å¾—åˆ°äº†å¾ˆå¥½çš„$E_{in}$ï¼Œä½†æ˜¯æ³›åŒ–èƒ½åŠ›å´å¾ˆå·®ã€‚</p><p>æœ€åï¼Œè¦æ³¨æ„ï¼Œæœ‰çš„äººè¯´ï¼Œåˆšå¼€å§‹ä¸¾å¾—ä¾‹å­ä¸­ï¼Œå¦‚æœé€šè¿‡å¯è§†åŒ–ï¼Œæˆ‘ä»¬ç›´æ¥ç”¨ $s(X) = w_0 + W_1 x_1^2 + W_2x_2 ^2$ï¼Œè¿™æ ·çœ‹èµ·æ¥ï¼Œä¼¼ä¹ç»´åº¦åªæœ‰3ä¸ªï¼Œå®é™…ä¸Šç±»ä¼¼çš„è¿˜æœ‰$s(x) = w_0 + W_1( x_1^2 + x_2 ^2)$,ä¼¼ä¹ç»´åº¦åªæœ‰ä¸¤ä¸ªï¼Œç”šè‡³æ˜¯å¯ä»¥é™åˆ°0ä¸ªã€‚ä½†æ˜¯è¿™å®é™…ä¸Šæ˜¯äººè„‘å­¦ä¹ çš„è¿‡ç¨‹ï¼Œæˆ‘ä»¬å·²ç»æ›¿æœºå™¨å­¦ä¹ äº†ä¸å°‘ï¼Œå› æ­¤æœ€åçš„æ³›åŒ–ç»“æœå¯èƒ½å¹¶ä¸æ˜¯æˆ‘ä»¬æƒ³è±¡çš„é‚£ä¹ˆå¥½ï¼Œå› ä¸ºçœŸå®çš„ä»£ä»·è¿˜æ˜¯å­˜åœ¨çš„ã€‚è€Œä¸”ï¼Œé«˜çº¬åº¦çš„å¯è§†åŒ–æ˜¯å¾ˆéš¾åšåˆ°çš„ã€‚</p><p>å› æ­¤ï¼Œä½¿ç”¨å¤šé¡¹å¼å¹¶ä¸è§å¾—å°±æ˜¯å¥½çš„ï¼Œæˆ‘ä»¬è¦ç»“åˆå®é™…æƒ…å†µæ¥è¿›è¡Œå­¦ä¹ ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> polynomial </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”Multiclassï¼ˆOVA and OVOï¼‰</title>
      <link href="/2018/09/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Multiclass/"/>
      <url>/2018/09/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Multiclass/</url>
      
        <content type="html"><![CDATA[<p>ç›®å‰ï¼Œæˆ‘ä»¬å¯¹äºŒå…ƒåˆ†ç±»å·²ç»æœ‰äº†ä¸å°‘çš„äº†è§£ï¼Œå¯ä»¥ç”¨å¤šä¸ªçº¿æ€§æ¨¡å‹å»å®ç°äºŒå…ƒåˆ†ç±»ã€‚ä½†æ˜¯ç”Ÿæ´»ä¸­é‡åˆ°çš„å¾€å¾€ä¸æ˜¯æ˜¯éé¢˜ï¼Œè€Œæ˜¯é€‰æ‹©é¢˜ï¼Œå°¤å…¶æ˜¯å›¾åƒè¯†åˆ«é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬å¾€å¾€éœ€è¦è¯†åˆ«å¤šä¸ªç‰©ä½“ã€‚å¦‚ä½•é€šè¿‡ä¹‹å‰å®ç°çš„äºŒå…ƒåˆ†ç±»ï¼Œæ¥å®ç°å¤šå…ƒåˆ†ç±»å‘¢ï¼Ÿ<br><a id="more"></a><br>è¿™é‡Œä»‹ç»ä¸¤ç§æ€è·¯ï¼Œä¸€ä¸ªæ˜¯OVAï¼ˆOne Vesus Allï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯OVOï¼ˆOne Vesus Oneï¼‰ã€‚</p><h2 id="OVA-via-Logistic-Regression"><a href="#OVA-via-Logistic-Regression" class="headerlink" title="OVA(via Logistic Regression)"></a>OVA(via Logistic Regression)</h2><p>è¦æƒ³è¿›è¡Œå¤šå…ƒåˆ†ç±»ï¼Œæˆ‘ä»¬é¦–å…ˆæƒ³åˆ°çš„æ˜¯å¯¹æ¯ä¸€ç§ç±»å‹è¿›è¡Œæ˜¯éåˆ¤æ–­ã€‚ç†æƒ³ä¸­ï¼Œè¿™æ ·ä¼¼ä¹ä¸é”™ï¼Œæ‰¾ä¸ªç‰©ä½“ï¼Œå“ªä¸€ç§ç±»å‹çš„åˆ¤æ–­è¯´æ˜¯ï¼Œå°±æ˜¯è¯¥ç±»å‹ï¼Œä½†æ˜¯ç°å®å¾€å¾€ä¸å°½äººæ„ã€‚</p><p>å‡å¦‚æœ‰ä¸‹å›¾æ‰€ç¤ºçš„ä¸€ä¸ªæ ·æœ¬é›†ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/5%7ES9ZBTNV%25J%40NFF1X9ER%29%24Y.png" alt=""></p><p>å¯ä»¥çœ‹åˆ°ä¸€å…±æœ‰å››ç±»ï¼Œåˆ†åˆ«å¯¹æ¯ä¸ªç±»å‹è¿›è¡Œæ˜¯éåˆ¤æ–­ï¼Œå¯ä»¥å¾—åˆ°ä¸‹é¢çš„4æ¡çº¿æ¥åˆ†ç±»ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/K%240%7E%29%7E_4A%25I%60_5B%29B4KOG1X.png" alt=""></p><p>ä»å·¦è‡³å³åˆ†åˆ«æ˜¯æ­£æ–¹ä½“ï¼Œè±å½¢ï¼Œä¸‰è§’å½¢ï¼Œäº”è§’æ˜Ÿå¾—åˆ°çš„çº¿ã€‚</p><p>èåˆåˆ°åŸæ¥çš„å›¾å½¢ä¸Šï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/56XK%28P%242S%29VUE%7B47%24WLLDH3.png" alt=""></p><p>å¯¹äºä¸Šå›¾ä¸­ï¼Œæœ‰å‡ ä¸ªéƒ¨åˆ†åŒºåŸŸçš„æ ·æœ¬å¯ä»¥å¾ˆæ˜ç¡®çš„åˆ¤æ–­å‡ºæ¥æ˜¯ä»€ä¹ˆç±»å‹ï¼Œä½†æ˜¯å…¶ä»–éƒ¨åˆ†åŒºåŸŸï¼Œè¦ä¹ˆæ˜¯å¤šä¸ªç±»å‹éƒ½è¯´æ˜¯ï¼Œè¦ä¹ˆæ²¡æœ‰ä¸€ä¸ªç±»å‹è¯´æ˜¯ï¼Œè¿™å°±è®©æˆ‘ä»¬æ— æ³•è¿›è¡Œåˆ¤æ–­ã€‚</p><p>æˆ‘ä»¬å¾ˆå®¹æ˜“æƒ³åˆ°æ—¢ç„¶ç”¨æ˜ç¡®çš„æ˜¯éæ— æ³•è¿›è¡Œåˆ¤æ–­ï¼Œå¦‚æœä½¿ç”¨æ¦‚ç‡ä¼šä¸ä¼šå¥½ä¸€ç‚¹ã€‚å› æ­¤è¦ä½¿ç”¨logistic regressionã€‚</p><p>ä½¿ç”¨logistic regressionè¿›è¡Œçš„è¿˜æ˜¯ä¸Šé¢çš„æ­¥éª¤ï¼Œå¾—åˆ°çš„æ˜¯æ˜¯å„ä¸ªç±»å‹çš„æ¦‚ç‡ã€‚é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ï¼Œå°±å¯è¿›è¡Œåˆ†ç±»ã€‚</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/S%402ZQSW29UEE%7D3X%7B9I0P%60%7ET.png" alt=""></p><p>ä¸‹é¢çš„åˆ†ç±»å›¾æ˜¯åˆ©ç”¨ä¸Šè¿°æ–¹æ³•è¿›è¡Œåˆ†ç±»å¾—åˆ°çš„ç»“æœï¼Œå¯ä»¥çœ‹åˆ°å¯¹æ¯ä¸ªåŒºåŸŸï¼Œéƒ½èƒ½ä¸ºå®ƒåˆ¶å®šä¸€ä¸ªç±»åˆ«ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯logistic functionæ˜¯å•è°ƒå¢å‡½æ•°ï¼Œå› æ­¤æ¯”è¾ƒæ¦‚ç‡çš„æ—¶å€™æˆ‘ä»¬å¹¶ä¸ç”¨çœŸçš„æ±‚å‡ºæ¥å¤§å°ï¼Œè€Œåªç”¨æ¯”è¾ƒ$s(X)$çš„å¤§å°å³å¯ã€‚</p><p>å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š</p><p>(1) for $k \in Y$, obtain W_{[k]} by running logistic regression on $D_{[k]} = \left \{ (X_n,yâ€™_n = 2[y_n = k] - 1) \right\}_{n=1}^N$.<br>(2) return $g(X) = argmax_{k \in y}(W_{[k]}^TX) $</p><p><em>*</em>æ³¨ï¼šargmaxæ˜¯ä¸€ç§å‡½æ•°ï¼Œå‡½æ•°y=f(x)ï¼Œx0= argmax(f(x)) çš„æ„æ€å°±æ˜¯å‚æ•°x0æ»¡è¶³f(x0)ä¸ºf(x)çš„æœ€å¤§å€¼ï¼›æ¢å¥è¯è¯´å°±æ˜¯ argmax(f(x))æ˜¯ä½¿å¾— f(x)å–å¾—æœ€å¤§å€¼æ‰€å¯¹åº”çš„å˜é‡xã€‚argå³argumentï¼Œæ­¤å¤„æ„ä¸ºâ€œè‡ªå˜é‡â€ã€‚åœ¨ä¸Šå¼ä¸­ä¸ºç»“æœä¸ºæŸä¸ªk$(k \in y)$ã€‚</p><hr><p>ä¸Šé¢çš„æ–¹æ³•å¾ˆç®€å•çš„å°±å¯ä»¥å®ç°äº†å¤šå…ƒåˆ†ç±»ã€‚ä½†æ˜¯ä¸Šé¢çš„ç®—æ³•æœ‰ä¸ªç¼ºç‚¹ï¼Œä¸€å¯¹å¤šï¼Œå½“ç§ç±»kç‰¹åˆ«å¤šæ—¶å€™ï¼Œå¾ˆå®¹æ˜“é€ æˆä¸å¹³è¡¡çš„æƒ…å†µï¼Œä¸€ä¸ªä¸å¥½çš„ç®—æ³•ä½†æ˜¯å´å¾—åˆ°äº†å¾ˆå¥½çš„$E_{in}$ï¼Œå½±å“æœ€åçš„åˆ†ç±»ç»“æœã€‚å› æ­¤å¸Œæœ›å¯ä»¥æ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p><h2 id="OVO-via-Binary-Classification"><a href="#OVO-via-Binary-Classification" class="headerlink" title="OVO(via Binary Classification)"></a>OVO(via Binary Classification)</h2><p>OVOæ˜¯ä¸€å¯¹ä¸€çš„ç®—æ³•ï¼Œå¯ä»¥å¾ˆå¥½çš„è§£å†³ä¸Šé¢æœ€åç•™ä¸‹æ¥çš„é—®é¢˜ã€‚å®ƒçš„æ€æƒ³æ˜¯è¿™æ ·çš„ï¼Œä»kä¸ªç±»åˆ«ä¸­æŒ‘å‡ºä¸¤ç§ç±»åˆ«æ¥è¿›è¡Œå­¦ä¹ ï¼Œæ¯æ¬¡å­¦ä¹ éƒ½å¯ä»¥å¾—åˆ°ä¸€ä¸ªç”¨æ¥åŒºåˆ†çš„$W$ï¼Œä¸€å…±å¯ä»¥å¾—åˆ°$C_k^2$ç§ã€‚</p><p>è¿™é‡Œå¾—åˆ°çš„$W_i$ä¸ä¸Šé¢çš„åŠæ³•ç”¨é€”æ˜¯ä¸ä¸€æ ·çš„ï¼Œå®ƒç›´æ¥ç”¨æ¥åšäºŒå…ƒåˆ†ç±»ï¼ˆ+1 or -1ï¼‰ï¼Œè€Œä¸æ˜¯å¾—åˆ°ä¸€ä¸ªæ¦‚ç‡ã€‚é€šè¿‡ä¸¤ä¸¤ç»„åˆè¿›è¡ŒäºŒå…ƒåˆ†ç±»çš„å­¦ä¹ ï¼Œæˆ‘ä»¬å¾—åˆ°äº†$C_k^2$ä¸ªåˆ†ç±»å™¨ï¼Œæ¯ä¸ªåˆ†ç±»å™¨éƒ½ä¼šå¯¹æ”¾å…¥çš„æ ·æœ¬è¿›è¡Œä¸€ä¸ªæ˜ç¡®çš„åˆ†ç±»ã€‚ä¸‹å›¾æ˜¯ä¸Šé¢çš„æ ·æœ¬é›†å¾—åˆ°çš„å‡ ä¸ªåˆ†ç±»å™¨ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/A%28PYVE%60M%404EFA73%25S531CMM.png" alt=""></p><p>ä»å·¦åˆ°å³ï¼Œåˆ†åˆ«æ˜¯å¯¹[è±å½¢ï¼Œæ–¹å—]ï¼Œ[ä¸‰è§’å½¢ï¼Œæ–¹å—],[äº”è§’æ˜Ÿï¼Œæ–¹å—]ï¼Œ[è±å½¢ï¼Œä¸‰è§’å½¢]ï¼Œ[è±å½¢ï¼Œäº”è§’æ˜Ÿ]ï¼Œ[ä¸‰è§’å½¢ï¼Œäº”è§’æ˜Ÿ]è¿›è¡ŒäºŒå…ƒåˆ†ç±»ã€‚</p><p>è¿›è¡Œé¢„æµ‹æ—¶å€™ï¼Œå–ä¸€ä¸ªæ ·æœ¬ï¼Œç»è¿‡6ä¸ªåˆ†ç±»å™¨æ¥é¢„æµ‹ï¼Œ6ä¸ªåˆ†ç±»å™¨å¾—åˆ°ä¸åŒçš„ç»“æœï¼Œä½†æ˜¯æ¯ä¸ªéƒ½ä¼šå¯¹è¯¥æ ·æœ¬çš„ç±»åˆ«è¿›è¡Œæ˜ç¡®çš„é¢„æµ‹ï¼Œç±»ä¼¼äºæŠ•ç¥¨ï¼Œæœ€åæˆ‘ä»¬é€‰æ‹©å¾—ç¥¨æœ€å¤šçš„ç±»åˆ«ã€‚</p><p>ä¸‹å›¾ä¸ºç”¨OVOåˆ†ç±»å¾—åˆ°çš„ç»“æœï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/7%28CYH%24KM%5BVTG%7EM%25UL3R%7D28F.png" alt=""></p><p>OVOç®—æ³•çš„ä¸»è¦è¿‡ç¨‹å¦‚ä¸‹ï¼š</p><p>(1) for ($k,\zeta$) $\in Y \times Y$, obtain $W_{[k,\zeta]}$ by running linear binary classification on $D_{[k,\zeta]} = \left \{ (X_n,yâ€™_n = 2[y_n = k] - 1):y_n = k or y_n = \zeta \right \}$ </p><p>(2) return $g(X) =$ $tournament$ $champion$ $\left \{W^T_{[k,\zeta]}\right \}$</p><h2 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h2><p>ä¸Šé¢å°±æ˜¯ä¸¤ç§ç”¨åœ¨å¤šå…ƒåˆ†ç±»ä¸Šçš„ç®—æ³•ï¼Œä»–ä»¬éƒ½æ˜¯å¾ˆç®€å•å¹¶ä¸”éå¸¸å¸¸è§çš„ç®—æ³•ã€‚ä¸¤ä¸ªç®—æ³•è¿è¡Œé€Ÿåº¦éƒ½å¾ˆå¿«ï¼ˆOVOè™½ç„¶å¢åŠ äº†åˆ†ç±»å™¨çš„ä¸ªæ•°ï¼Œä½†æ˜¯ç”¨æ¥å­¦ä¹ çš„æ ·æœ¬é‡ä¼šå‡å°‘å¾ˆå¤šï¼‰ã€‚OVOçš„ç¼ºç‚¹æ˜¯å¦‚æœç±»åˆ«çœŸçš„éå¸¸å¤§ï¼Œé‚£ä¹ˆåˆ†ç±»å™¨ä¸ªæ•°å¯èƒ½è¿‡å¤šï¼Œä¼šå ç”¨è¾ƒå¤§çš„ç©ºé—´ï¼Œä¸€å®šç¨‹åº¦ä¸Šä¹Ÿä¼šå½±å“é€Ÿåº¦ï¼Œä½†æ˜¯å®ƒæœ‰è¾ƒé«˜çš„ç¨³å®šæ€§ï¼Œå‡å°‘å‡ºç°ä¸å¹³è¡¡çš„æƒ…å†µã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> classification </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”linear model for classification</title>
      <link href="/2018/09/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94linear-model-for-classification/"/>
      <url>/2018/09/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94linear-model-for-classification/</url>
      
        <content type="html"><![CDATA[<p>åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå·²ç»å­¦ä¹ äº†3ä¸ªçº¿æ€§æ¨¡å‹äº†ï¼Œä»–ä»¬éƒ½ä½¿ç”¨åˆ°äº†$score = W^TX$(åæ–‡ä¸­ç®€å†™ä¸º$s$)ï¼Œä½¿ç”¨ç‰¹å¾é—´çº¿æ€§ç»„åˆæ¥æ‰“åˆ†ï¼Œé€šè¿‡åˆ†æ•°æ¥åšåç»­çš„å¤„ç†ã€‚<a id="more"></a></p><p>linear regressionç”¨äºåˆ†ç±»å‰é¢æœ‰ä¸€ç¯‡åšå®¢å·²ç»è¯´æ˜ï¼Œç°åœ¨æˆ‘ä»¬æƒ³è¦çŸ¥é“ï¼Œlogistic regression æ˜¯å¦ä¹Ÿå¯ä»¥ç”¨äºåˆ†ç±»ï¼Ÿæ¯•ç«Ÿçº¿æ€§å›å½’çš„é”™è¯¯å¯¹äºäºŒå…ƒåˆ†ç±»æ¥è¯´æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„ä¸Šç•Œï¼Œè¿™æ„å‘³ç€å®ƒçš„æ•ˆæœè™½ç„¶ä¸å·®ï¼Œä½†å¯èƒ½é”™å¤±æ›´å¥½çš„ã€‚è€ŒPLAæ‰¾åˆ°ä¸€ä¸ªæœ€å°çš„$E_{in}$æ˜¯NP-hardé—®é¢˜ï¼Œåªèƒ½ä½¿ç”¨æ”¹è¿›çš„POCKETç®—æ³•ã€‚æˆ‘ä»¬å¸Œæœ›çœ‹åˆ°logistic regressionç”¨äºäºŒå…ƒåˆ†ç±»å¯ä»¥æœ‰æ›´å¥½çš„è¡¨ç°ã€‚</p><p>ä¸ä¹‹å‰çš„æ­¥éª¤ä¸€æ ·ï¼Œé€»è¾‘å›å½’ä¸­ï¼Œ$E_{in} = \sum_{n = 1}^{N} \ln{1 + e^{-y_nW^TX_n}}$ï¼Œæˆ‘ä»¬å¯¹æ¯”çš„æ˜¯å•ä¸ªæ ·æœ¬çš„é”™è¯¯ï¼Œå°±å†™ä½œ$err_{name}$å¥½äº†ã€‚</p><p>ä¸ºäº†è®©è¿™3ç§æ¨¡å‹éƒ½æœ‰è¾ƒä¸ºæ¸…æ™°çš„å¯¹æ¯”ï¼Œæˆ‘ä»¬å¯¹PLAä»¥åŠçº¿æ€§å›å½’çš„é”™è¯¯è¡¡é‡ä¹Ÿåšå¤„ç†ï¼Œå¦‚ä¸‹ï¼š</p><div class="table-container"><table><thead><tr><th style="text-align:center">method</th><th style="text-align:center">linear classification</th><th style="text-align:center">linear regression</th><th style="text-align:center">logistic regression</th></tr></thead><tbody><tr><td style="text-align:center"> err</td><td style="text-align:center">$[sign(ys \neq 1)]$</td><td style="text-align:center">$(sy-1)^2$</td><td style="text-align:center">$\ln{1+e^{-ys}}$</td></tr></tbody></table></div><p> å°†å®ƒä»¬çš„æ›²çº¿ç»˜åˆ¶åˆ°ä¸€å¼ å›¾ä¸Šï¼Œå¯ä»¥å¾—åˆ°ä¸‹é¢çš„ç»“æœï¼š<br> <img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/87T%7EV%5DV75N992E1I%5B3M9%5DZK.png" alt=""><br> å…¶ä¸­è“è‰²æ˜¯linear classificationçš„é”™è¯¯ï¼Œçº¢è‰²æ˜¯linear regressionçš„é”™è¯¯ï¼Œç»¿è‰²æ˜¯logistic regressionã€‚åäº†ï¼Œç»¿è‰²çš„çº¿å¹¶ä¸æ€»æ˜¯å¤§äºè“è‰²çš„çº¿ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æ— æ³•åƒä¹‹å‰ä¸€æ ·ï¼Œç®€å•åœ°å°†$E_{in}(linear classification)$æ¢æˆ$E_{in}(logistic regression)$ã€‚</p><p> å®é™…ä¸Šï¼Œä¹‹å‰æˆ‘ä»¬é€‰æ‹©ç”¨$ln$å‡½æ•°æ˜¯å› ä¸ºè¿™æ˜¯æœ€å¸¸è§çš„ï¼Œåªæ˜¯å°†ä¹˜æ³•æ¢æˆåŠ æ³•ï¼Œç†è®ºä¸Šæˆ‘ä»¬å¯ä»¥å–ä»»ä½•å¯¹æ•°ï¼Œå¦‚ï¼Œå°†å¯¹æ•°å‡½æ•°æ¢ä¸º$log_2^x$,å°±å¯ä»¥å¾—åˆ°å¦å¤–ä¸€å‰¯æ›²çº¿å›¾ï¼š<br> <img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/CCO%29%25RV4OG9Z8%7D60%40%404W6G0.png" alt=""></p><p> è¿™æ ·å°±å¯ä»¥æ»¡è¶³æˆ‘ä»¬çš„éœ€è¦ï¼Œä¹Ÿæ–¹ä¾¿ä¸‹é¢çš„è¯æ˜ã€‚<br>æˆ‘ä»¬ç§°ä½¿ç”¨$ln(x)$å‡½æ•°çš„é”™è¯¯ä¸º$err_{ce}$ï¼Œä½¿ç”¨$log_2(x)$çš„ä¸º$err_{sce}$ï¼Œåˆ™ç”±ä¸Šå›¾å¯ä»¥çŸ¥é“ï¼š</p><p>$err_{0/1}(s,y) \leq err_{sce}(s,y) = \frac 1 {\ln2} err_{ce}(s,y)$ï¼Œ<br>ï¼ˆç”±æ¢åº•å…¬å¼ï¼š$\frac {\ln x}{\ln2} = log_2x ï¼‰<br>ä¹Ÿå°±å¯ä»¥çŸ¥é“ï¼š</p><script type="math/tex; mode=display">E_{in}^{0/1} \leq E_{in}^{sce} = \frac 1 {\ln2} E_{in}^{ce}</script><p>åŒæ ·çš„é“ç†ï¼š</p><script type="math/tex; mode=display">E_{out}^{0/1} \leq E_{out}^{sce} = \frac 1 {\ln2} E_{out}^{ce}</script><p>å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åƒä¹‹å‰ä¸€æ ·æ¨å¯¼ï¼š</p><script type="math/tex; mode=display">E_{out}^{0/1} \leq E_{in}^{0/1}+ \Omega ^{0/1} \leq \frac 1 {\ln2} E_{in}^{ce}+\Omega ^{0/1}</script><p>åŒæ ·ï¼Œä»å¦ä¸€ä¸ªæ–¹å‘ä¹Ÿå¯ä»¥æ¨å¯¼ï¼š</p><script type="math/tex; mode=display">E_{out}^{0/1} \leq \frac 1 {\ln{2}} E_{out}^{ce} \leq \frac 1 {\ln2} E_{in}^{ce}+\Omega ^{ce}</script><p>æ— è®ºå“ªä¸ªï¼Œéƒ½å¯ä»¥è¯æ˜ï¼Œlogistic regressionæ˜¯å¯ä»¥ç”¨äºäºŒå…ƒåˆ†ç±»çš„ã€‚è€Œä¸Šé¢çš„å›¾åƒä¹Ÿè¯´æ˜äº†ï¼Œä»–çš„æ•ˆæœæ¯”çº¿æ€§å›å½’æ›´å¥½ï¼Œboundæ›´ç´§ä¸€ç‚¹ã€‚</p><p>åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨linear regressionæ¥åˆå§‹åŒ–$W$ï¼Œç„¶åé€šè¿‡POCKETæˆ–è€…logistic regressionæ¥è¿›è¡Œåé¢çš„æ­¥éª¤ï¼Œè€Œä¸”logistic regressionæ›´ä¸ºå¸¸ç”¨ã€‚</p><p><strong><em>æ³¨ï¼šä¸Šé¢æ¨å¯¼ä¸­ï¼Œåˆ¤æ–­éƒ½æ˜¯ä»¥s=0ä¸ºç•Œå®šï¼Œå¯¹åº”åˆ°logistic regressionä¹Ÿå°±æ˜¯æ¦‚ç‡ä¸º0.5ä¸ºç•Œå®šã€‚</em></strong></p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> classification </tag>
            
            <tag> visualization </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”Gradient Decent</title>
      <link href="/2018/08/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Gradient-Decent/"/>
      <url>/2018/08/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Gradient-Decent/</url>
      
        <content type="html"><![CDATA[<p>Gradient Decent,å³æ¢¯åº¦ä¸‹é™ã€‚æ¢¯åº¦ä¸‹é™æ˜¯æœºå™¨å­¦ä¹ ä¸­éå¸¸é‡è¦çš„æ¥è¿‘æœ€ä¼˜è§£çš„å·¥å…·ã€‚<a id="more"></a>ç†è®ºä¸Šåªè¦å¾—åˆ°æ¢¯åº¦ï¼Œå°±å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™ã€‚å› æ­¤å®ƒåŒæ ·å¯ä»¥ç”¨äºlinear regressionï¼Œåœ¨æ•°æ®é‡å¾ˆå¤§ç‰¹å¾å¾ˆå¤šçš„æƒ…å†µä¸‹ï¼Œå› ä¸ºçŸ©é˜µæ±‚é€†çš„æ—¶é—´å¤æ‚åº¦å¾ˆå¤§ï¼Œå®ƒå¾€å¾€æ¯”ä¹‹å‰ä»‹ç»çš„linear regressionâ€œä¸€æ­¥ç™»å¤©â€çš„åšæ³•æ€§èƒ½æ›´å¥½ã€‚</p><p>é¦–å…ˆï¼Œå‡è®¾æˆ‘ä»¬å·²ç»çŸ¥é“äº†$E_{in}$çš„æ¢¯åº¦ä¸º$\nabla E_{in}$ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¦çŸ¥é“ä¸€ä»¶äº‹ï¼Œæ¢¯åº¦æ˜¯æ‰€æœ‰æ–¹å‘ä¸Šï¼Œâ€œå¡åº¦â€æœ€é™¡çš„æ–¹å‘ã€‚æ¢¯åº¦ä¸‹é™ï¼Œä½¿ç”¨äº†è´ªå¿ƒçš„æ€æƒ³ï¼šæ¯æ¬¡éƒ½æœç€å¡åº¦æœ€é™¡çš„æ–¹å‘å¾€ä¸‹èµ°ã€‚ä¹Ÿè®¸æœ€åå¾—åˆ°çš„ä¸ä¸€å®šæ˜¯å…¨å±€æœ€ä¼˜è§£ï¼Œä½†æ˜¯ä¸€å®šæ˜¯ä¸€ä¸ªæå°å€¼ç‚¹ï¼Œé€šå¸¸ä¹Ÿèƒ½å–å¾—ä¸é”™çš„æ•ˆæœã€‚</p><p>æ‰€ä»¥ï¼Œæœ‰ä¸€ä¸ªèµ·å§‹ç‚¹ä¸º$W_0$ï¼Œæ¯æ¬¡å‘ä¸‹èµ°ä¸€ä¸ªå•ä½çš„é•¿åº¦ä¹˜ä¸Šä¸€ä¸ªæœªçŸ¥çš„$\eta$ï¼Œç”¨æ¥æ§åˆ¶æ­¥é•¿ã€‚åœ¨æ¢¯åº¦æ–¹å‘ä¸Šçš„å•ä½å‘é‡ç­‰äº$\frac {\nabla E_{in}}{|\nabla E_{in}|}$ï¼Œå½“ç„¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥å¾ˆå®¹æ˜“çŸ¥é“ï¼Œæˆ‘ä»¬è¦èµ°çš„æ–¹å‘åº”è¯¥æ˜¯æ¢¯åº¦çš„åæ–¹å‘ã€‚å› ä¸ºæ±‚å‡ºæ¥çš„æ¢¯åº¦å¾€å¾€æŒ‡å‘çš„æ˜¯å‡½æ•°å€¼å¢åŠ æœ€å¿«çš„æ–¹å‘ï¼Œè€Œæˆ‘ä»¬è¦çš„æ˜¯å°½å¯èƒ½å‡å°‘$E_{in}$ã€‚æ‰€ä»¥å°±å¯ä»¥å¾—åˆ°ä¸‹é¢çš„å¼å­ï¼š</p><script type="math/tex; mode=display">W_1 = W_0 - \eta \frac {\nabla E_{in}}{|\nabla E_{in}|}</script><p>æ¯æ¬¡æœç€æ¢¯åº¦æ–¹å‘èµ°ä¸€æ­¥ï¼Œç†æƒ³ä¸­è¿™ä¸ªå‡½æ•°å°±ä¼šä¸‹é™ä¸€ç‚¹ï¼Œå› ä¸ºå½“èµ°åŠ¨çš„è·ç¦»å¾ˆå°æ—¶å€™ï¼Œæœ‰ä»¥ä¸‹è¿‘ä¼¼å¼ï¼š</p><script type="math/tex; mode=display">E_{in}(W_0+\Delta) \approx E_{in} + \Delta \nabla E_{in} (W_0)</script><p>ä¸Šå¼å…¶å®æ˜¯å¤šç»´å‡½æ•°æ³°å‹’å±•å¼€å¼ï¼ˆä¸€é˜¶ï¼‰ã€‚</p><p>æ¥ä¸‹æ¥ï¼Œå°±æ˜¯å¯¹$\eta$çš„é€‰æ‹©äº†ã€‚é€‰æ‹©$\eta$æ—¶å€™ï¼Œæœ‰ä¸‹é¢å‡ ç§æƒ…å†µï¼š<br>1.$\eta$ å¤ªå°ã€‚ å¦‚æœ$\eta$å¾ˆå°ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥ä¿è¯å®ƒæœ€åä¸€å®šå¯ä»¥èµ°åˆ°ä¸€ä¸ªå¾ˆä½çš„åœ°æ–¹ï¼Œä¸è¿‡é€Ÿåº¦å¤ªæ…¢äº†ã€‚å› ä¸ºæ¯ä¸€æ­¥æ­¥é•¿å¤ªå°ã€‚</p><p>2.$\eta$ å¤ªå¤§ã€‚å¦‚æœ$\eta$å¤ªå¤§ï¼Œé‚£ä¹ˆä¹‹å‰çš„æ³°å‹’å±•å¼€å¼å°±ä¸é€‚ç”¨ï¼Œå……æ»¡äº†ä¸ç¡®å®šæ€§ã€‚æœ‰å¯èƒ½ä¸€æ­¥å¤ªé•¿èµ°åˆ°å¯¹é¢ï¼Œè¿æ°”å¥½çš„è¯å‡½æ•°å€¼è¿˜åœ¨å˜å°ï¼Œè¿æ°”ä¸å¥½å‡½æ•°å€¼ä¼šå¢åŠ ï¼Œå› æ­¤è¿™æ ·ä¹Ÿæ˜¯ä¸å¯é€‰çš„ã€‚</p><p>3.å¦‚ä¸‹å›¾ï¼Œè®©$\eta$åœ¨å˜åŒ–ã€‚å½“æ¢¯åº¦å¾ˆå¤§æ—¶å€™ï¼Œæ­¥å­è¿ˆçš„å¤§ä¸€ç‚¹ï¼Œå½“æ¢¯åº¦å°äº†ï¼Œæ„å‘³ç€è·ç¦»æœ€ä½ç‚¹å¾ˆè¿‘äº†ï¼Œæ­¥å­å°ä¸€ç‚¹ï¼Œè°¨æ…ä¸€ç‚¹èµ°ã€‚ä¸Šé¢ä¸‰ç§æƒ…å†µå¦‚ä¸‹å›¾ï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/eta_choice.png" alt=""></p><p>å¾ˆæ˜æ˜¾ï¼Œæˆ‘ä»¬åº”å½“é€‰æ‹©çš„æ˜¯ç¬¬ä¸‰ç§ç­–ç•¥ã€‚å¦‚ä½•æ§åˆ¶$\eta$å¯å˜ï¼Ÿç®€å•çš„åšæ³•å°±æ˜¯è®©$\eta$ä¸$|\nabla E_{in}|$æˆæ­£æ¯”ï¼Œå¦‚æœ$\eta = \alpha|\nabla E_{in}| $å°±ä¼šæŠµæ¶ˆäº†åŸæ¥å¼å­ä¸­çš„åˆ†æ¯éƒ¨åˆ†ï¼Œæœ€åå¾—åˆ°ä¸‹å¼ï¼š</p><script type="math/tex; mode=display">W_{i+1} = W_i - \eta  {\nabla E_{in}}</script><p>å¯ä»¥çœ‹åˆ°å¼å­å˜å¾—æ›´åŠ ç®€æ´äº†ã€‚</p><p>ä¸Šå¼ä¸­çš„$\eta$æˆä¸ºfixed learning rateï¼Œå°½ç®¡å­¦ä¹ ç‡æ˜¯å›ºå®šçš„ï¼Œä½†æ˜¯æ¯ä¸€æ­¥æ­¥é•¿å´åœ¨æ”¹å˜ã€‚å½“ç„¶ï¼Œå¯¹äºä¸Šå¼ä¸­çš„$\eta$æˆ‘ä»¬ä¹Ÿåº”è¯¥é€‰æ‹©åˆé€‚çš„å€¼ï¼Œå¦åˆ™è¿˜æ˜¯ä¼šå‡ºç°ä¸Šè¿°çš„ä¸¤ç§æƒ…å†µï¼Œä½†æ˜¯å®ƒæ”¹è¿›äº†å¾ˆå¤šï¼Œä½¿å¾—ç®—æ³•æ•ˆç‡å¾—åˆ°äº†æå‡ã€‚</p><p>æœ€åï¼Œæ¢¯åº¦ä¸‹é™ä»€ä¹ˆæ—¶å€™åœæ­¢ï¼Ÿå½“$\nabla E_{in} \approx 0 $æˆ–è€…è¿›è¡Œäº†è¶³å¤Ÿå¤šæ¬¡æ•°çš„è¿­ä»£ä¹‹åï¼Œæˆ‘ä»¬å°±åº”è¯¥åœæ­¢äº†ã€‚å› ä¸ºåœ¨è®¡ç®—æœºä¸­æ•°å€¼ä¸Šå¾—åˆ°ä¸€ä¸ªå®Œå…¨ä¸º0çš„å€¼æ˜¯å¾ˆå›°éš¾çš„ï¼Œè€Œçº¦ç­‰äº0æ—¶å€™ï¼Œæˆ‘ä»¬å°±å¯ä»¥å–å¾—æ»¡æ„çš„ç»“æœã€‚æˆ–è€…è¿›è¡Œäº†è¶³å¤Ÿå¤šçš„æ¬¡æ•°ï¼Œä¾ç„¶å¾—ä¸åˆ°æ»¡æ„çš„ç»“æœï¼Œæˆ‘ä»¬éœ€è¦è€ƒè™‘çš„æ˜¯æ”¹è¿›ç®—æ³•ï¼Œæ˜¯ä¸æ˜¯å­¦ä¹ ç‡å€¼å–å¾—ä¸å¤Ÿå¥½ç­‰ç­‰ã€‚</p><p>ä»¥ä¸Šå°±æ˜¯æ¢¯åº¦ä¸‹é™ã€‚</p><p>åœ¨è¿™é‡Œï¼Œå¦å¤–æåˆ°ä¸€ä¸ªå«éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic Gradient Desentï¼‰çš„ç®—æ³•ã€‚ä¸Šé¢ä»‹ç»çš„æ¢¯åº¦ä¸‹é™ç®—æ³•è™½ç„¶å¯ä»¥å¾ˆå¥½çš„æ‰¾åˆ°æˆ‘ä»¬æƒ³è¦çš„ç»“æœï¼Œä½†æ˜¯åœ¨nå¾ˆå¤§çš„æ—¶å€™ï¼Œæ¯æ¬¡æ›´æ–°éƒ½éœ€è¦è¿›è¡Œæ±‚å’Œå¹³å‡ï¼Œè¿™å°±å¯¼è‡´äº†ç®—æ³•é€Ÿåº¦å¯èƒ½å—åˆ°å½±å“ã€‚å®é™…ä¸Šï¼Œå®ƒçš„æ—¶é—´å¤æ‚åº¦ä¸POCKETç®—æ³•æ˜¯ä¸€æ ·çš„ã€‚æˆ‘ä»¬å¸Œæœ›å¯ä»¥å°†å¤æ‚åº¦æå‡åˆ°PLAçš„çº§åˆ«ã€‚</p><p>$E_{in}$çš„å®šä¹‰æ˜¯å¯¹æ‰€æœ‰ç‚¹çš„$err$åŠ èµ·æ¥æ±‚å¹³å‡ï¼Œåœ¨nå¾ˆå¤§æ—¶å€™ï¼Œå¹³å‡å€¼çš„æœŸæœ›å€¼ä¸éšæœºæŠ½å–ä¸€ä¸ªæ ·æœ¬çš„å€¼çš„æœŸæœ›å€¼æ˜¯æ¥è¿‘çš„ï¼Œå› æ­¤éšæœºæ¢¯åº¦ä¸‹é™å®é™…ä¸Šå°±æ˜¯æ¯æ¬¡éšæœºé€‰å–ä¸€ä¸ªç‚¹ï¼ˆæˆ–è€…å°‘é‡ç‚¹æ±‚å¹³å‡ï¼‰ï¼Œç„¶åç”¨å®ƒçš„$err$æ¥è®¡ç®—æ¢¯åº¦ï¼Œå¹¶è¿›è¡Œæ›´æ­£ã€‚å®ƒç±»ä¼¼äºPLAç®—æ³•çš„æ­¥éª¤:</p><script type="math/tex; mode=display">SGD logistic regression:  W_{t+1} = W_t + \eta  \theta (-y_nW_t^TXn)(y_nX_n)</script><script type="math/tex; mode=display">PLA:W_{t+1} =  W_{t}+[y \neq sign(W_t^TX_n)] (y_nX_n)</script><p>å½“SGD logistic regressionçš„é”™è¯¯éå¸¸ç¦»è°±ï¼Œå®ƒçš„æ›´æ–°æ•ˆæœå®é™…ä¸Šæ¥è¿‘äºPLAç®—æ³•ã€‚</p><p>è¿™ä¸ªç®—æ³•è¿è¡Œä»€ä¹ˆæ—¶å€™ç»ˆç»“ï¼Ÿä¸€èˆ¬æ¥è¯´è¿è¡Œè¶³å¤Ÿé•¿çš„æ—¶é—´ä¹‹åæˆ–è€…æ­¤æ—¶çš„$E_{in}$å·²ç»è¶³å¤Ÿå°ï¼Œæˆ‘ä»¬å°±è®¤ä¸ºå·²ç»å–å¾—äº†ä¸é”™çš„æ•ˆæœã€‚å®ƒçš„ä¼˜ç‚¹æ˜¯é€Ÿåº¦å¿«ï¼Œç¼ºç‚¹æ˜¯æœ€ç»ˆçš„ç»“æœç›¸æ¯”äºæ¢¯åº¦ä¸‹é™è¿˜æ˜¯å·®äº†ä¸€ç‚¹ã€‚å› æ­¤å®ƒæ˜¯æ¢¯åº¦ä¸‹é™ç®—æ³•çš„ä¸€ä¸ªæ”¹è¿›ã€‚</p><p>é€‚ç”¨åœºæ™¯ï¼š1.nç‰¹åˆ«å¤§ 2.å¦‚æœæœ¬èº«æ ·æœ¬æ˜¯ä¸€ä¸ªä¸€ä¸ªæ¥çš„ï¼Œè€Œä¸æ˜¯ä¸€æ‰¹ä¸€æ‰¹æ¥çš„ï¼Œä¹Ÿå¯ä»¥é€‚ç”¨è¿™ä¸ªæ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯å¯ä»¥é€‚ç”¨äºåœ¨çº¿å­¦ä¹ ï¼ˆonline learningï¼‰ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> mathematics </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”logistic Regression</title>
      <link href="/2018/08/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94logistic-Regression/"/>
      <url>/2018/08/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94logistic-Regression/</url>
      
        <content type="html"><![CDATA[<p>é€šè¿‡çº¿æ€§åˆ†ç±»ï¼Œæˆ‘ä»¬é¢„æµ‹çš„æ˜¯ä¸€ä¸ªæ ·æœ¬æ˜¯positiveï¼Œè¿˜æ˜¯negativeï¼Œä¸è¿‡æœ‰æ—¶å€™æˆ‘ä»¬å¹¶ä¸æƒ³è¦é‚£æ ·æ˜ç¡®çš„ç»“æœã€‚<a id="more"></a>æœ‰æ—¶å€™ï¼Œè¿™ç§æƒ…å†µåœ¨åŒ»å­¦ä¸­æ›´ä¸ºå¸¸è§ï¼Œæˆ‘ä»¬æƒ³çŸ¥é“ä¸€ä¸ªæ ·æœ¬æ˜¯æ­£çš„æ¦‚ç‡ï¼Œæ¯”å¦‚åŒ»é™¢ä¸­å¯¹è‚¿ç˜¤è‰¯æ€§ä¸æ¶æ€§çš„é¢„æµ‹ï¼Œæ¥å†³å®šåç»­çš„æ²»ç–—æ–¹å¼ã€‚æ¦‚ç‡æ˜¯ä»0åˆ°1çš„å®æ•°ï¼Œå› æ­¤å¯¹æ¦‚ç‡çš„é¢„æµ‹ä¾ç„¶å±äºå›å½’è€Œéåˆ†ç±»ã€‚</p><p>ç†æƒ³ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æ ·æœ¬çš„æ ·å­æ˜¯è¿™æ ·çš„ï¼š$\{X_1,0.9\},\{x_2,0.2\},\{x_3,0.65\}â€¦$ï¼Œå³å¯¹æ¯ä¸ªæ ·æœ¬ï¼Œéƒ½å·²çŸ¥å®ƒæ˜¯positiveçš„æ¦‚ç‡ã€‚ä½†æ˜¯å®é™…ä¸Šæˆ‘ä»¬å¾€å¾€æ— æ³•å¾—åˆ°è¿™æ ·çš„ç»“æœï¼Œæˆ‘ä»¬æ— æ³•ç¡®åˆ‡çŸ¥é“æŸä¸ªæ ·æœ¬æ˜¯æ­£çš„æ¦‚ç‡ã€‚æˆ‘ä»¬å¾—åˆ°çš„æ ·æœ¬ï¼Œå¾€å¾€ä¸åˆ†ç±»é—®é¢˜çš„æ ·æœ¬ä¸€æ ·ï¼Œå¯¹æ¯ä¸ªæ ·æœ¬ï¼ŒçŸ¥é“å®ƒæ˜¯negativeï¼Œè¿˜æ˜¯positiveã€‚ä¸è¿‡æˆ‘ä»¬å¯ä»¥å‡æƒ³ï¼Œå¾—åˆ°çš„è®­ç»ƒé›†æ˜¯ç†æƒ³æƒ…å†µ+å™ªå£°é€ æˆçš„ï¼šå¦‚æœæ˜¯negativeï¼Œæˆ‘ä»¬å¯ä»¥è¯´å®ƒæ˜¯positiveçš„æ¦‚ç‡ä¸º0ï¼Œå¦‚æœæ˜¯positiveï¼Œæˆ‘ä»¬ç§°è¯¥æ¦‚ç‡ä¸º1ã€‚æˆ‘ä»¬å¸Œæœ›å¯ä»¥é¢„æµ‹å‡ºæ¦‚ç‡ã€‚</p><p>é¦–å…ˆï¼Œæœ‰ä¹‹å‰çš„plaä¸linear regressionçš„åŸºç¡€ï¼Œå¾ˆå®¹æ˜“æƒ³åˆ°ï¼Œä½¿ç”¨$WX$å»å¾—åˆ°é¢„æµ‹å€¼ã€‚ä½†æ˜¯é¢„æµ‹å€¼è™½ç„¶æ˜¯å®æ•°ï¼Œä½†æ˜¯å› ä¸ºæ˜¯æ¦‚ç‡ï¼Œæ‰€ä»¥å®ƒçš„åˆ†å¸ƒä»…åœ¨äº$[0,1]$ï¼Œå› æ­¤ä»…ä»…ä½¿ç”¨$WX$æ˜¯ä¸ç¬¦åˆé¢„æœŸçš„ã€‚</p><p>è¿™é‡Œï¼Œå°†ä»‹ç»ä¸€ç§å‡½æ•°å«logisticå‡½æ•°ï¼š$f(x) = \frac {1} {1+e^{-x}}$ï¼ˆåœ¨æ•°å­¦ä¸Šè¿™ä¸ªå®šä¹‰æ›´åŠ ä¸¥æ ¼ä¸€ç‚¹ï¼Œè€Œæ­¤å¤„æ˜¯logisticå‡½æ•°çš„ä¸€ç§ï¼‰.è¿™ä¸ªå‡½æ•°çš„å›¾åƒå¦‚ä¸‹ï¼š<br><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/480px-Logistic-curve.svg.png" alt=""></p><p>å®ƒæ»¡è¶³ä¸‹é¢çš„æ¡ä»¶ï¼š$\lim_{x \to +\infty} f(x) = 1$ &amp; $\lim_{x \to -\infty} f(x) = 0 $ï¼Œå®ƒçš„å€¼åŸŸæ˜¯ï¼ˆ0ï¼Œ1ï¼‰ï¼Œè¿™ä¸ªæ€§è´¨ç›®å‰å¾ˆç¬¦åˆæˆ‘ä»¬çš„æœŸæœ›ã€‚</p><p>æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰çš„å°±æ˜¯$E_{in}$ï¼Œå› ä¸ºæœ‰$E_{in}$æˆ‘ä»¬æ‰èƒ½ä»Hç§æ‰¾åˆ°ä¸€ä¸ªgï¼Œå®ƒçš„$E_{in}$æœ€å°ã€‚ä½†æ˜¯$E_{in}$æ˜¯ä¸èƒ½éšä¾¿å®šä¹‰çš„ã€‚ä¹‹å‰çš„$E_{in}$ï¼Œéƒ½æ˜¯åœ¨æ‰¾ä¸çœŸæ­£å¯¹åº”çš„$y$ä¹‹é—´çš„è·ç¦»ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸çŸ¥é“çœŸæ­£çš„$y$ï¼ˆä¹Ÿå°±æ˜¯æ¦‚ç‡å€¼ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬è¦æ¢ç§è€ƒè™‘æ–¹æ³•ã€‚</p><p>å‡å¦‚æœ‰ä¸€ä¸ªç†æƒ³çš„å‡½æ•°$f$ï¼Œèƒ½çŸ¥é“æ ·æœ¬ä¸ºpositiveçš„çœŸå®æ¦‚ç‡ï¼Œè¡¨ç¤ºä¸ºï¼š$P(y_i=+1|X_i) = f(X_i)$ï¼Œé‚£ä¹ˆè¯¥æ ·æœ¬æ˜¯ç°åœ¨è¿™ä¸ªæ ·å­çš„æ¦‚ç‡ä¸ºï¼š$P(X_i \bigcap y_i = +1) = P(X_i) \times P(y_i=+1|X_i)$ï¼Œè€Œå½“$y_i = -1$æ—¶ï¼Œ$P(X_i \bigcap y_i = -1) = P(X_i) \times (1-P(y_i=+1|X_i))$ï¼Œåˆå¹¶ä¸¤ç§æƒ…å†µï¼Œå¾—åˆ°ï¼š$P(X_i \bigcap y_i) =\frac {(y_i+1)} 2 P(X_i) \times P(y_i = +1|X_i) +\frac {(1 - y_i)} 2 P(X_i) \times (1 - P(y_i = +1|X_i)) $.å„ä¸ªæ ·æœ¬ä¹‹é—´æ˜¯ç‹¬ç«‹çš„è¯ï¼Œé‚£ä¹ˆå‡ºç°è¿™ä¸ªæ ·æœ¬é›†çš„æ¦‚ç‡ç­‰äºï¼š$\prod_{i=1}^{n}P(X_i \bigcap y_i)$ã€‚</p><p>ç›®å‰ï¼Œæˆ‘ä»¬ä¸çŸ¥é“$f$ï¼Œä½†æ˜¯å‡å¦‚æˆ‘ä»¬ç”¨$H$ä¸­çš„æŸä¸ª$h$æ¥ä»£æ›¿ï¼Œè¿™å°±æ„å‘³ç€åœ¨å‡½æ•°hçš„æƒ…å†µä¸‹ï¼Œå‡ºç°è¿™ä¸ªæ ·æœ¬é›†çš„æ¦‚ç‡ï¼Œå½“ç„¶æˆ‘ä»¬æƒ³è¦åšçš„æ˜¯ä»¤è¿™ä¸ªæ¦‚ç‡è¶Šå¤§è¶Šå¥½ã€‚å¹³æ—¶å¤„ç†çš„$E_{in}$éƒ½æ˜¯å’Œï¼Œè€Œè¿™æ¬¡æˆ‘ä»¬ä¾ç„¶å¸Œæœ›ç”¨å’Œæ¥å¤„ç†ï¼Œè€Œä¸æ˜¯è¿ä¹˜ï¼Œå› æ­¤æˆ‘ä»¬ä¸ºä¸Šå¼åŠ ä¸Šä¸€ä¸ª$\ln$ï¼Œè€Œå› ä¸ºP(X_i)çš„æ¦‚ç‡éƒ½æ˜¯ä¸€å®šçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬æ— éœ€å…³æ³¨ã€‚çœæ‰$P(X_i)$,å†è¿›è¡Œä¸Šé¢çš„å¤„ç†ä¹‹åå˜ä¸ºï¼š<br>$\frac{1}{N} \sum _{i = 1}^{n} \ln {(\frac {(y_i+1)} 2 h(X_i) +\frac {(1 - y_i)} 2 (1 - h(X_i)))}$.</p><p>ä½†æ˜¯è¿™ä¸ç¬¦åˆ$E_{in}$ï¼Œå› ä¸ºæˆ‘ä»¬æƒ³è¦çš„æ˜¯$E_{in}$è¶Šå°è¶Šå¥½ï¼Œè€Œä¸Šé¢çš„å¼å­æ˜¯è¶Šå¤§è¶Šå¥½ï¼Œå› æ­¤éœ€è¦åŠ ä¸Šä¸€ä¸ªè´Ÿå·ï¼ŒåŒæ—¶ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ä¹‹å‰çš„logistic functionä½œä¸ºhï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°å®ƒæœ‰ä¸€ä¸ªæ€§è´¨ï¼š1 - h(x) = h(-x)ï¼Œå› æ­¤,å¯ä»¥å¾—åˆ°ä¸‹é¢çš„å¼å­ï¼š</p><script type="math/tex; mode=display">E_{in} = -\frac{1}{n}  \sum _{i = 1}^{n} \ln {(\frac {(y_i+1)} 2 h(X_i) +\frac {(1 - y_i)} 2 (1 - h(X_i)))} = -\sum _{i = 1}^{n} \ln h(y_iX_i).</script><p>å°†logistic function å¸¦å…¥ä¸Šå¼ï¼š</p><script type="math/tex; mode=display">E_{in} =\frac{1}{n}  \sum _{i = 1}^{n} \ln(1+e^{-y_iW^TX_i})</script><p>ä»ä¸Šå¼å¯ä»¥çœ‹å‡ºæ¥ï¼Œå¦‚æœ$y_i = 1$ï¼Œè€Œé¢„æµ‹å®ƒä¸º1çš„æ¦‚ç‡å°äº0.5ï¼ˆ$W^TX_i&lt;0$ï¼‰ï¼Œé‚£ä¹ˆ$E_{in}$çš„å€¼ä¼šå¤§äº$ln2$ï¼Œè€Œ$E_{in}$æ˜¯æ²¡æœ‰ä¸Šç•Œçš„ã€‚é”™çš„è¶Šç¦»è°±ï¼Œæƒ©ç½šå°±è¶Šå¤§ã€‚</p><p>æ¥ä¸‹æ¥çš„é—®é¢˜ï¼Œå°±æ˜¯å¦‚ä½•è®©$E_{in}$å–åˆ°æœ€å°äº†ã€‚</p><p>é¦–å…ˆï¼Œå¯ä»¥è¯æ˜çš„æ˜¯ï¼Œè¿™ä¸ª$E_{in}$ä¹Ÿæ˜¯ä¸€ä¸ªå‡¸å‡½æ•°ï¼ˆè¯æ˜åŠæ³•éœ€è¦ç”¨åˆ°æ›´æ·±å…¥çš„çº¿æ€§ä»£æ•°çŸ¥è¯†ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªæœ€å°å€¼ã€‚å’Œçº¿æ€§å›å½’æ—¶å€™é‡åˆ°çš„æƒ…å†µä¸€æ ·ï¼Œè¦å¾—åˆ°æå€¼ç‚¹ï¼Œå°±è¦æ‰¾åˆ°æ¢¯åº¦ä¸º0çš„ç‚¹ã€‚å› æ­¤é¦–å…ˆè¦å¾—åˆ°çš„æ˜¯$E_{in}$çš„æ¢¯åº¦ã€‚ä»å¾®ç§¯åˆ†é‡Œï¼Œæˆ‘ä»¬çŸ¥é“æ¢¯åº¦çš„æ±‚æ³•ï¼Œä¹Ÿå°±æ˜¯å¯¹æ¯ä¸ªæ–¹å‘æ±‚åå¯¼ï¼Œç”±å®ƒä»¬ç»„æˆçš„å‘é‡ã€‚ä¸ºäº†å¾—åˆ°æ¢¯åº¦ï¼Œæˆ‘ä»¬é¦–å…ˆåº”è¯¥æ±‚å‡º$E_{in}$å¯¹æ¯ä¸ª$w_i$çš„åå¯¼æ•°ï¼Œå¯¹äºåå¯¼æ•°çš„æ±‚æ³•åœ¨å¾®ç§¯åˆ†ä¸­è¯¾ç¨‹ä¸­æˆ‘ä»¬ä¹Ÿå­¦ä¹ è¿‡ï¼Œå¯ä»¥å¾—åˆ°ä¸‹é¢çš„ç»“æœï¼š</p><script type="math/tex; mode=display">\frac {dE_{in}} {d{w_i}} =\frac{1}{N}  \sum _{n = 1}^N \frac {e^{-y_nW^TX_n} \times ( -y_nx_{n,i})} {1+e^{-y_nW^TX_n}}</script><p>æ³¨æ„ï¼šä¸Šå¼ä¸­ä¸ºäº†æ–¹ä¾¿ï¼Œæˆ‘ä»¬ç”¨$N$æ›¿ä»£äº†ä¹‹å‰çš„$n$ï¼Œç”¨$n$æ›¿ä»£äº†$i$ï¼Œå¹¶ä¸”ç”¨$i$ä»£è¡¨äº†$W$å‘é‡ä¸­ç»´åº¦åºåˆ—ã€‚</p><p>å¦‚æœå¯¹ä¸Šå¼å‘é‡åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ° $\nabla E_{in}(\mathbf{w}) = \frac{1}{N} \sum\limits_{n=1}^N {\theta\left({-y_n \mathbf{w}^T \mathbf{x}_n}\right)} {\bigl(-y_n \mathbf{x}_n\bigr)}$. </p><p>ä¸Šå¼ä¸­ï¼Œ$\theta(x) = \frac {1} {1+e^{-x}}$.</p><p>ç”±æ­¤ï¼Œæˆ‘ä»¬æ±‚åˆ°äº†$E_{in}$çš„æ¢¯åº¦ã€‚ä¸ºäº†è®©è¿™ä¸ªæ¢¯åº¦ä¸º0ï¼Œé¦–å…ˆæˆ‘ä»¬æƒ³åˆ°çš„æ˜¯ï¼Œæ‰€æœ‰çš„$\theta\left({-y_n \mathbf{w}^T \mathbf{x}_n}\right)$ç­‰äº0ï¼Œè¿™è¦æ±‚æ‰€æœ‰çš„$y_nW^TX_n$éƒ½æ˜¯è¿œå¤§äº0ï¼Œè¿™æ„å‘³ç€åŸæ¥çš„æ•°æ®æ˜¯çº¿æ€§å¯åˆ†çš„ã€‚</p><p>å¦‚æœä¸æ˜¯è¿™ç§æƒ…å†µï¼ŒåŸæ¥çš„æ•°æ®åˆ™ä¸æ˜¯çº¿æ€§å¯åˆ†ï¼ˆå¤§å¤šæ•°æƒ…å†µä¸‹æˆ‘ä»¬çš„æ•°æ®éƒ½ä¸æ˜¯è¿™ä¹ˆç†æƒ³ï¼‰ï¼Œæƒ³ä½¿å¾—æœ€ç»ˆç»“æœä¸º0ï¼Œé‚£ä¹ˆå„ä¸ªæ•°æ®æ˜¯åŠ æƒåŠ èµ·æ¥æœ€åå¾—0.é‚£ä¹ˆæˆ‘ä»¬å¸Œæœ›å¯ä»¥åƒçº¿æ€§å›å½’ä¸€æ ·é€šè¿‡æŸç§è¡¨è¾¾å¼ç›´æ¥æ±‚å¾—ä¸€ä¸ªè§£ï¼Œä¸è¿‡é—æ†¾çš„æ˜¯è¿™ä¸ªå‡½æ•°ä¸æ˜¯çº¿æ€§å‡½æ•°ï¼Œæˆ‘ä»¬æ²¡æœ‰åŠæ³•ä¸€ä¸‹æ±‚å¾—è¿™ä¸ªè§£ã€‚æˆ‘ä»¬èƒ½åšçš„åªèƒ½æ˜¯æ­¥æ­¥é€¼è¿‘ï¼Œç±»ä¼¼äºä¹‹å‰çš„PLAç®—æ³•ã€‚</p><p>è¿™å°±è¦ä»‹ç»ä¸€ä¸ªæ–¹æ³•ï¼Œå«<strong>æ¢¯åº¦ä¸‹é™</strong>ï¼ˆgradient decentï¼‰ã€‚</p><p>æ¢¯åº¦ä¸‹é™å…·ä½“å†…å®¹æŸ¥çœ‹ï¼š<a href="https://wlsdzyzl.top/2018/08/31/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Gradient-Decent/" target="_blank" rel="noopener">Gradient Decent</a>ã€‚</p><p>é€šè¿‡æ¢¯åº¦ä¸‹é™ï¼Œæˆ‘ä»¬å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„$W$ï¼Œä»è€Œå¾—åˆ°è¾ƒå¥½çš„é€»è¾‘å›å½’æ•ˆæœã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> regression </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æ–°ç”Ÿæ´»â€”â€”æœ‰ç‚¹æƒ³å›å»</title>
      <link href="/2018/08/29/%E6%96%B0%E7%94%9F%E6%B4%BB%E2%80%94%E2%80%94%E6%9C%89%E7%82%B9%E6%83%B3%E5%9B%9E%E5%8E%BB/"/>
      <url>/2018/08/29/%E6%96%B0%E7%94%9F%E6%B4%BB%E2%80%94%E2%80%94%E6%9C%89%E7%82%B9%E6%83%B3%E5%9B%9E%E5%8E%BB/</url>
      
        <content type="html"><![CDATA[<p>27å·ï¼Œæˆ‘åä¸Šè¿œç¦»å®¶ä¹¡çš„é£æœºï¼Œä¸è¿‡ä¸¤ä¸ªåŠå°æ—¶ï¼Œæˆ‘å°±æ¥åˆ°äº†æ·±åœ³â€”â€”â€”â€”è¿™é‡Œï¼Œè·ç¦»æˆ‘çš„å®¶ï¼Œæœ‰1700å¤šå…¬é‡Œï¼Œè·ç¦»å¤ªåŸï¼Œæœ‰2000å…¬é‡Œã€‚<a id="more"></a></p><p>æˆ‘æ„Ÿæ…¨é£æœºçš„é€Ÿåº¦çœŸæ˜¯å¿«ï¼Œå®é™…ä¸Šæˆ‘è¿˜å¸Œæœ›å®ƒèƒ½æ…¢ä¸€ç‚¹ã€‚æˆ‘ä¸æƒ³è¿™ä¹ˆå¿«å°±æ¥ã€‚å¯æ˜¯å³ä½¿é€‰æ‹©ç«è½¦ï¼Œè¯¥æ¥è¿˜æ˜¯è¦æ¥ï¼Œåªä¸è¿‡è·¯ä¸Šæ›´åŠ æŠ˜è…¾äº†ã€‚</p><p>ç°åœ¨è·ç¦»æˆ‘åˆ°æ·±åœ³å·²ç»è¿‡äº†ä¸¤å¤©äº†ï¼Œæˆ‘è¿˜ä¸æ˜¯å¾ˆé€‚åº”è¿™é‡Œçš„ç”Ÿæ´»ã€‚æˆ‘è¯´ä¸æ¸…æ¥šå¿ƒé‡Œä»€ä¹ˆæ»‹å‘³ï¼Œä¹Ÿè¯´ä¸æ¸…æ¥šåˆ°åº•ä¸ºä»€ä¹ˆä¼šæœ‰è¿™ç§æ»‹å‘³ã€‚å¥½åƒæ˜¯æœ‰ç‚¹éº»æœ¨ï¼Œå¯¹æ–°çš„ç”Ÿæ´»ä¸æ˜¯å¾ˆæœŸå¾…ã€‚</p><p>ä¸€ç›´ä»¥æ¥ï¼Œæˆ‘éƒ½ä¸æ˜¯ä¸ªæ“…é•¿é€‚åº”æ–°ç¯å¢ƒçš„äººï¼Œæ¯æ¬¡å»ç»“äº¤æ–°çš„æœ‹å‹ï¼Œæˆ‘çš„å¿ƒé‡Œéƒ½æ˜¯åœ¨é€¼è‡ªå·±ã€‚åˆä¸­å’Œå°å­¦ï¼Œæˆ‘å·²ç»è®°ä¸å¾—äº†ï¼Œåˆ°äº†é«˜ä¸­ï¼Œæˆ‘è®°å¾—æˆ‘ç¬¬ä¸€æ¬¡ååº§ä½å’Œç”·ç¥èƒ¡åšåŒæ¡Œï¼Œè€Œä¸”è¿˜æ˜¯ä¸Šä¸‹é“ºã€‚æˆ‘è¯´æˆ‘å–œæ¬¢æ‰“ç¯®çƒï¼Œèƒ¡å­è¯´å»æ–—ç‰›ï¼Œå®é™…ä¸Šæˆ‘æ‰“çš„å¾ˆçƒ‚ã€‚å¤§å­¦ï¼Œæˆ‘ç»™æ¨é¾™ç‚«è€€æˆ‘çš„æµ·è´¼ç‹æƒ…ä¾£è£…ï¼Œé¾™å“¥æ‹¿å‡ºæ¥æµ·ç»µå®å®æƒ…ä¾£è£…ï¼ŒåŒæ—¶æ‰”ç»™æˆ‘ä¸€å †æƒ…è¶£ç”¨å“ã€‚æˆ‘ä¸æ“…é•¿äº¤æœ‹å‹ï¼Œä½†æ˜¯æˆ‘çš„å¿ƒé‡Œæœ‰æ†ç§¤ã€‚å¯¹ä»»ä½•æˆ‘è§‰å¾—æœ‰å…´è¶£çš„äººï¼Œæˆ‘çš„å¿ƒé‡Œå°±åƒä¸€ä¸ªç¨‹åºï¼Œä¸æ–­æ‰§è¡Œï¼šæˆ‘å…ˆæ‰¾ä»–ï¼ˆå¥¹ï¼‰dæ¬¡ï¼Œdæ¬¡æ‰¾å®Œï¼Œä»–ï¼ˆå¥¹ï¼‰ä»ç„¶æ²¡æœ‰æ‰¾è¿‡æˆ‘ï¼Œæˆ‘è‡ªå·±å°±ä¸å†å»æ‰¾ï¼›å¦‚æœå¯¹æ–¹ä¸»åŠ¨æ‰¾æˆ‘äº†ï¼Œæ¬¡æ•°é‡æ–°åˆ·æ–°ä¸ºdã€‚å¯¹äº60%çš„åŒå­¦ï¼Œd = 0ï¼Œå¯¹äº39%çš„åŒå­¦ï¼Œd = 1,å¯¹äº1%çš„åŒå­¦ï¼Œdè¿˜ä¼šæ›´å¤§ä¸€ç‚¹ã€‚æˆ‘çš„å¿ƒé‡Œæ²¡æœ‰åˆ»æ„ç®—è®¡è¿‡è¿™äº›ä¸œè¥¿ï¼Œæˆ‘åªæ˜¯ç”¨è¿™ä¸ªç¨‹åºå»æ¨¡æ‹Ÿäº†æˆ‘çš„å†…å¿ƒã€‚</p><p>è¿™æ„å‘³ç€æˆ‘çš„ç¤¾äº¤åœˆæ˜¯éå¸¸å°çš„ï¼Œæ¯•ç«Ÿæˆ‘ä¸æ˜¯ä»€ä¹ˆå‡ºä¼—çš„äººï¼Œè°ä¼šé—²ç€æ²¡äº‹æ‰¾æˆ‘ã€‚å®é™…ä¹Ÿç¡®å®æ˜¯è¿™æ ·ï¼Œç°åœ¨é«˜ä¸­è¿˜è”ç³»çš„æœ‹å‹ä¹Ÿå°±5ï¼Œ6ä¸ªï¼Œå¤§å­¦è¯´è¯çš„ä¹Ÿåªæœ‰å‡ ä¸ªå®¤å‹å’Œç¤¾å›¢çš„æœ‹å‹äº†ã€‚ç¤¾å›¢çš„æœ‹å‹æ˜¯éå¸¸çƒ­å¿ƒçš„ï¼Œå†åŠ ä¸Šå¿—åŒé“åˆï¼Œæˆ‘ä»¬å…³ç³»éå¸¸å¥½ã€‚</p><p>ç°åœ¨ä¸Šç€ç ”ç©¶ç”Ÿï¼Œæˆ‘è¿˜æ²¡äº¤ä¸‹ä»€ä¹ˆæœ‹å‹ã€‚</p><p>ä¸æ˜¯ç ”ç©¶ç”Ÿçš„åŒå­¦ä¸å‹å¥½ï¼Œä»Šå¤©ä¸‹å¤§é›¨æˆ‘æ²¡å¸¦ä¼ï¼Œæœ‰çƒ­å¿ƒè‚ çš„åŒå­¦ä¸æˆ‘åˆ†äº«ä»–çš„é›¨ä¼ï¼Œæœ€åæˆ‘ä»¬éƒ½è¢«æ·‹æ¹¿ã€‚æˆ‘ä¹ŸåŠ äº†å¾ˆå¤šå¥½å‹ï¼Œæœ‰ä¸€ä¸ªåŒå­¦ä»‹ç»çš„æ—¶å€™è¯´æ›¾ç»å‚å†›å½“è¿‡æ’é•¿ï¼Œæˆ‘å¾ˆç¾¡æ…•è¿™ç§ç»å†ï¼Œä¹Ÿæƒ³å’Œä»–äº¤æµã€‚æœ‰å‡ ä¸ªå›½å¤–æ¥çš„ç•™å­¦ç”Ÿï¼Œæˆ–è€…æ˜¯åäººæˆ–è€…æ˜¯æ„å¤§åˆ©æ–°è¥¿å…°ï¼Œæˆ‘ä¹Ÿè›®æƒ³äº†è§£é‚£äº›å›½å¤–çš„ç”Ÿæ´»ï¼Œè¿˜æœ‰ä¸€äº›å¥èº«æˆ¿å¤§ä½¬ï¼Œæˆ‘ä¹Ÿæƒ³å»å¥èº«æˆ¿ç»ƒç»ƒç¡¬ä»¶ï¼Œå¹¶å‘ä»–ä»¬ä»‹ç»è·‘é…·ä¸è¡—å¥ã€‚ä»Šå¤©è‡ªæˆ‘ä»‹ç»çš„æ—¶å€™ï¼Œæˆ‘è¿˜æ‹…å¿ƒæˆ‘è¯´æˆ‘å–œæ¬¢è·‘é…·ï¼Œå¤§å®¶ä¼šå“‡ï¼Œè®©æˆ‘å»éœ²ä¸€æ‰‹ï¼Œéœ²ä¸€æ‰‹æ˜¯ä¸å¯èƒ½å»éœ²ä¸€æ‰‹çš„ï¼Œä¸è¿‡æˆ‘æƒ³å¥½äº†æ€ä¹ˆæ¨è¾ï¼šè·‘é…·æ˜¯å®¤å¤–è¿åŠ¨ï¼Œè€Œä¸”æˆ‘2ä¸ªæœˆæ²¡ç»ƒäº†ï¼Œåœ¨è¿™ä¹ˆå¤šäººé¢å‰æˆ‘ç´§å¼ ï¼Œæ€•å‡ºç°å±é™©â€¦â€¦ä¸è¿‡æˆ‘ä»‹ç»å®Œä¹‹åï¼Œæ²¡ä»€ä¹ˆäººå¯¹æˆ‘ï¼Œä»¥åŠæˆ‘çš„è·‘é…·å±•ç°å‡ºå¤ªå¤§å…´è¶£ã€‚ä¹‹å‰çš„æ‹…å¿ƒï¼Œå˜å¾—ç¨å¾®æœ‰äº›ä¼¤å¿ƒã€‚</p><p>å°±åƒæˆ‘çš„è‡ªæˆ‘ä»‹ç»å¹³æ·¡æ— å¥‡ï¼Œæˆ‘åŠ äº†é‚£äº›æˆ‘æœ‰å…´è¶£äº¤æœ‹å‹çš„åŒå­¦ä½œä¸ºå¥½å‹åï¼Œä¹Ÿå°±æ²¡å†è”ç³»ã€‚</p><p>ä¼¼ä¹æˆå¹´äººï¼Œå¤§å®¶éƒ½æŠŠè‡ªå·±å°é—­äº†èµ·æ¥ã€‚å†µä¸”æˆ‘å®¿èˆè¿˜æ²¡æœ‰åŒå­¦åœ¨ã€‚æˆ‘ä¸€ä¸ªäººä¸å­¦é•¿ä½ï¼Œå¾—åˆ°è¿™ä¸ªæ¶ˆæ¯ï¼Œæˆ‘è¿˜æœ‰ç‚¹å…´å¥‹ã€‚æˆ‘äº«å—ç‹¬å¤„çš„æ—¶å…‰ï¼Œæ¯”å¦‚ç°åœ¨æˆ‘ç è¿™ç¯‡åšå®¢ï¼Œæ²¡æœ‰äººä¼šåœ¨æˆ‘èº«åçœ‹ï¼Œæˆ‘å°±ä¸ä¼šä¸è‡ªåœ¨ã€‚</p><p>æ™šä¸Šæœ‰ä¸ªå¿ƒç†è®²åº§ï¼Œæˆ‘è§‰å¾—æˆ‘çš„å¿ƒç†å¯èƒ½æœ‰ç‚¹é—®é¢˜ï¼Œæ‰€ä»¥æ²¡å»ã€‚</p><p>æˆ‘æ¸´æœ›æœ‰ä¸ªå¥½æœ‹å‹ï¼Œæˆ‘å¿ƒé‡Œæ²¡æ‹’ç»ä»»ä½•äººåšæˆ‘çš„å¥½æœ‹å‹ï¼Œä½†æ˜¯æˆ‘å°±æ˜¯ä¸»åŠ¨ä¸èµ·æ¥ã€‚å’Œä¸€ä¸ªäººä»é™Œç”Ÿäººï¼Œå˜åˆ°çŸ¥æ ¹çŸ¥åº•ï¼Œéœ€è¦å¾ˆä¹…ã€‚æˆ‘ä¸æƒ³ç»å†è¿™ä¸ªè¿‡ç¨‹ã€‚ä½†æ˜¯å‹è°Šå°±æ˜¯ä»è¿™äº›è¿‡ç¨‹ä¸­ç§¯ç´¯èµ·æ¥çš„ï¼ŒçœŸæ˜¯çŸ›ç›¾ã€‚æˆ‘éƒ½æƒ³ä¸èµ·æ¥ä¹‹å‰çš„æœ‹å‹æ€ä¹ˆäº¤çš„äº†ï¼ŒåŒå¤„ä¸€å®¤ä¸å¾—ä¸äº¤æµå§ï¼Œè€Œä¸”ä¹Ÿå¾—çœ‹å¯¹çœ¼ã€‚</p><p>æ·±åœ³è¿™ä¸ªé›¨ï¼Œä¸‹å¾—åœä¸ä¸‹æ¥ï¼Œæˆ‘æƒ³ä¸‹å»è·‘è·‘æ­¥æ‹‰æ‹‰å•æ ï¼Œéƒ½æ²¡æœºä¼šã€‚</p><p>å…¶å®ä¸ºå•¥ä¼šè¿™æ ·ï¼Œæˆ‘çœŸè¯´ä¸æ¸…ã€‚è¿™æ·±åœ³å¤ªè¿œï¼Œæ°”å€™å¤ªæ¹¿ï¼›è¿™åŒå­¦ä»¬å¤ªä¼˜ç§€ï¼Œè‹±æ–‡æ— éšœç¢äº¤æµï¼Œé›…æ€æ‰˜ç¦è€ƒç€å½“ç»ƒæ‰‹ï¼Œè€Œæˆ‘å…­çº§444ï¼›å¤§å®¶å–œæ¬¢æ‰“çƒï¼Œç‹¼äººæ€ï¼Œlolï¼Œæˆ‘å–œæ¬¢parkourï¼Œä¸€ä¸ªäººé»˜é»˜æ‰“gtaï¼›ä¹‹å‰æœ‰å•¥æƒ…ç»ªè—å¿ƒé‡Œï¼Œç°åœ¨æœ‰äº†æ²¡å•¥äººçœ‹çš„åšå®¢è®©æˆ‘å¯ä»¥éšæ„æŠŠçŸ«æƒ…çš„æƒ…ç»ªå†™å‡ºæ¥ï¼›å¤§å®¶è‡ªæˆ‘ä»‹ç»ä¸´åœºå‘æŒ¥é€—ç¬‘å…¨ç­åŒå­¦ï¼Œè€Œåœ¨è·‘é…·ç¤¾è¢«ç§°ä¸ºâ€œæ®µå­æ‰‹â€çš„æˆ‘å°±åªè¯´å‡ºæ¥äº†å¿ƒé‡Œé»˜å¿µäº†å‡ éçš„å‡ å¥ï¼š</p><blockquote><p>æˆ‘å«å¼ å›½åº†ï¼Œæ¥è‡ªå±±è¥¿è¿åŸï¼Œæœ¬ç§‘è¯»è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯ã€‚æˆ‘æœ¬ç§‘åŠ å…¥äº†è·‘é…·ç¤¾ï¼Œæ‰€ä»¥è¿™æ˜¯æˆ‘çš„çˆ±å¥½ï¼Œä½†æ˜¯ç»ƒäº†4å¹´ï¼Œæˆ‘ä¾ç„¶ä¸æ˜¯å¾ˆå‰å®³ã€‚æˆ‘è¿˜å–œæ¬¢çœ‹ç”µå½±åŠ¨æ¼«ï¼Œç”»ç”»ã€‚è°¢è°¢å¤§å®¶ã€‚</p></blockquote><p>å…¶å®æˆ‘æƒ³è¯´çš„æ˜¯ï¼š</p><blockquote><p>æˆ‘å–œæ¬¢è·‘é…·ï¼Œæ›¾ç»è·å¾—å…¨å›½è·‘é…·å¤§èµ›å† å†›ã€‚å½“ç„¶æˆ‘è¿˜å–œæ¬¢å¹ç‰›ã€‚ä¸‹é¢æ˜¯æˆ‘å¹³æ—¶è·‘é…·çš„è§†é¢‘ï¼šï¼ˆ<a href="">è¿™é‡Œæ”¾ä¸Šæˆ‘çš„è·‘é…·è§†é¢‘</a>ï¼‰ã€‚ä½ ä»¬å¯ä»¥åŠ æˆ‘å¾®ä¿¡ï¼Œæˆ‘è·‘å®Œä¼šå‘è§†é¢‘ï¼Œå¯ä»¥å¯åŠ²å¤¸æˆ‘å¸…ã€‚æˆ‘è¿˜å–œæ¬¢ç”»ç”»ï¼Œä½ çœ‹æˆ‘ç”»çš„è¿™äº›ï¼Œï¼ˆ<a href="">è¿™é‡Œæ”¾ä¸Šæˆ‘ç”»çš„ç”»</a>ï¼‰è™½ç„¶æ¯”ä¸ä¸Šä¸“ä¸šçš„ï¼Œä½†æ˜¯ç³Šå¼„ç³Šå¼„æ™®é€šäººè¿˜æ˜¯å¤Ÿäº†å§ï¼æ˜¯ä¸æ˜¯å¾ˆæƒŠè‰³ï¼Œæˆ‘ç”šè‡³éƒ½æ²¡å­¦è¿‡ã€‚æˆ‘éå¸¸å–œæ¬¢æ¼«å¨ï¼ŒåŠ¨æ¼«å–œæ¬¢æµ·è´¼ç‹ï¼Œè¿›å‡»çš„å·¨äººï¼Œè¿˜æœ‰é¾™ç ã€‚</p></blockquote><p>ä¸è¿‡è¿™äº›è¯ï¼Œå¦‚æœä¸ç†Ÿæˆ‘æ˜¯è¯´ä¸å‡ºå£çš„ã€‚</p><p>æ‰€ä»¥æˆ‘æœ‰ç‚¹æƒ³å›å»ï¼Œæˆ‘å¤šå¹´çš„æœ‹å‹åœ¨é‚£è¾¹ï¼Œæˆ‘æœ€çˆ±çš„äººåœ¨é‚£è¾¹ï¼Œæˆ‘çš„å®¶åœ¨é‚£è¾¹ã€‚  </p>]]></content>
      
      
      <categories>
          
          <category> çŒæ°´ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> casual note </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”linear regressionä¸linear classification</title>
      <link href="/2018/08/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94linear-regression%E4%B8%8Elinear-classification/"/>
      <url>/2018/08/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94linear-regression%E4%B8%8Elinear-classification/</url>
      
        <content type="html"><![CDATA[<p>ä¸Šä¸€ç¯‡åšå®¢ä»‹ç»äº†çº¿æ€§å›å½’æ¨¡å‹ï¼Œä½†æ˜¯ä¸çŸ¥é“å¤§å®¶æ˜¯å¦æœ‰è¿™æ ·çš„æ„Ÿè§‰ï¼šå®ƒä¸ä¹‹å‰çš„PLAç®—æ³•ä¼¼ä¹æœ‰å¾ˆå¤šç±»ä¼¼ä¹‹å¤„ã€‚<br><a id="more"></a><br>è¿™ä¸¤ä¸ªç®—æ³•éƒ½åœ¨ä½¿ç”¨çº¿æ€§çš„æ–¹æ³•ï¼Œä¹Ÿå°±æ˜¯å®ƒä»¬éƒ½æ˜¯é€šè¿‡$y_n = W^TX_n$æ¥è®¡ç®—å‡º$y_n$ï¼Œåªä¸è¿‡PLAç®—æ³•é€šè¿‡$y_n$æ¥åˆ¤æ–­ç¬¬nä¸ªæ ·æœ¬çš„åˆ†ç±»ï¼Œè€Œçº¿æ€§å›å½’ç›´æ¥ä½¿ç”¨$y_n$æ¥ä½œä¸ºå®ƒçš„é¢„æµ‹å€¼ã€‚</p><p>çº¿æ€§å›å½’ä¸­ï¼Œ$y$çš„å€¼çš„èŒƒå›´æ˜¯æ•´ä¸ªå®æ•°ï¼Œä»å¦ä¸€æ–¹é¢æƒ³ï¼Œçº¿æ€§classificationçš„$y$å–å€¼åªæœ‰+1ï¼Œ-1ä¸¤ä¸ªå–å€¼ï¼Œè¿™ä¸¤ä¸ªå€¼å½“ç„¶ä¹Ÿæ˜¯å®æ•°ï¼Œæˆ‘ä»¬æ˜¯å¦èƒ½å°†çº¿æ€§å›å½’ç®—æ³•ç”¨ä½œäºçº¿æ€§åˆ†ç±»ä¸­å‘¢ï¼Ÿ</p><p>è¦æ€è€ƒè¿™ä¸ªé—®é¢˜ï¼Œé¦–å…ˆè¦å†³å®šå¦‚ä½•å°†çº¿æ€§å›å½’ç”¨äºçº¿æ€§åˆ†ç±»ï¼šå¯¹æ ·æœ¬é›†$\{X_n,Y_n\}$åˆ©ç”¨çº¿æ€§å›å½’æ¥è¿›è¡Œå­¦ä¹ ï¼Œå¾—åˆ°$W$ï¼Œç„¶åé€šè¿‡$sign(yâ€™_n)$ï¼ˆå…¶ä¸­$yâ€™_n = W^TX_n$ï¼‰æ¥å¯¹è¯¥æ ·æœ¬è¿›è¡Œåˆ†ç±»ã€‚ä¸PLAçš„åŒºåˆ«æ˜¯PLAä¸€ç›´åœ¨å°è¯•å„ç§ä¸åŒçš„$W$ï¼Œè€Œçº¿æ€§å›å½’ç›´æ¥å¾—åˆ°ä¸€ä¸ªè‡ªå·±è®¤ä¸ºæœ€å¥½çš„$W$ã€‚ä½†æ˜¯å®é™…ä¸Šå®ƒä»¬çš„Hypothesiséƒ½æ˜¯ä¸€æ ·çš„ã€‚</p><p>è¿™ä¸ªç®—æ³•æ˜¯å¦å¯è¡Œï¼Œéœ€è¦æ¥æ£€æŸ¥å®ƒçš„æ³›åŒ–èƒ½åŠ›ï¼Œä¹Ÿå°±æ˜¯å®ƒçš„$E_{out}$æ˜¯å¦åƒPLAç®—æ³•ä¸€æ ·æœ‰ä¸ªä¸Šç•Œã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è§‚å¯Ÿä¸¤ä¸ªç®—æ³•å¯¹äºé”™è¯¯çš„è¡¡é‡æœ‰ä»€ä¹ˆåŒºåˆ«ï¼ˆæ³¨ï¼šä»¥ä¸‹é”™è¯¯éƒ½æ˜¯å•ä¸ªæ ·æœ¬çš„é”™è¯¯ï¼‰ï¼š</p><div class="table-container"><table><thead><tr><th>category</th><th>$E_{in}$</th></tr></thead><tbody><tr><td> linear classification(PLA)</td><td>$y \neq yâ€™$</td></tr><tr><td> linear regression</td><td>$(y - yâ€™)^2$</td></tr><tr><td> linear regression to classification</td><td>$y \neq sign(yâ€™)$</td></tr></tbody></table></div><p>åœ¨ä¸Šé¢çš„è¡¨æ ¼ä¸­ï¼Œæˆ‘åŠ äº†ä¸€è¡Œï¼Œä¹Ÿå°±æ˜¯åˆ©ç”¨çº¿æ€§å›å½’ç®—æ³•æ¥è¿›è¡Œçº¿æ€§åˆ†ç±»æ—¶å€™çš„$E_{in}$ï¼Œè¿™æ„å‘³ç€å®ƒä»¬çš„$yâ€™$æ˜¯ä¸€æ ·çš„(å®é™…ä¸Š$yâ€™$ä»£è¡¨çš„åº”è¯¥æ˜¯æœ€ç»ˆçš„é¢„æµ‹å€¼ï¼Œè¿™é‡Œæ­£ç¡®çš„å†™æ³•åº”è¯¥æ˜¯$h(X)$,æ­¤å¤„åªä¸ºäº†æ–¹ä¾¿åŒºåˆ†PLA)ã€‚</p><p>å¦‚æœç”»å‡ºy = -1 ä¸ y = +1 æ—¶å€™çš„æ›²çº¿å›¾ï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ™°åœ°è§‚å¯Ÿåˆ°ï¼Œçº¿æ€§å›å½’å¾—åˆ°çš„é”™è¯¯æ°¸è¿œæ˜¯å¤§äºåˆ©ç”¨çº¿æ€§å›å½’è¿›è¡Œåˆ†ç±»çš„é”™è¯¯çš„ï¼š</p><p>y = +1æ—¶ï¼š<br> <img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/W%24%40%7B0SE%246CEJRR%6098Y255E0.png" alt=""></p><p>y = -1æ—¶ï¼š<br> <img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/V%5D%5D%24DG%28R6%5B6G%7E0%29%5D%2588KLDE.png" alt=""> </p><p>è€Œé€šè¿‡ä¹‹å‰vc boundé‚£ä¸€èŠ‚æˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼š</p><script type="math/tex; mode=display">E_{out} \leq E_{in}(classification)+\sqrt {...}</script><p>ä¹Ÿå°±å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">E_{out} \leq E_{in}(regression) +\sqrt {...}</script><p>è¿™æ„å‘³ç€ï¼Œåªè¦linear regressionçš„$E_{in}$åšçš„è¶³å¤Ÿå¥½ï¼Œé‚£ä¹ˆä½¿ç”¨çº¿æ€§å›å½’æ¥åšçº¿æ€§åˆ†ç±»ï¼Œå¾€å¾€ä¹Ÿèƒ½å–åˆ°æ¯”è¾ƒå¥½çš„æ•ˆæœã€‚</p><p>å®é™…ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨çº¿æ€§å›å½’ä¸PLAç®—æ³•ç»“åˆï¼Œå…ˆé€šè¿‡çº¿æ€§å›å½’å¾—åˆ°Wï¼Œç„¶åå› ä¸ºç»™äº†PLAæˆ–è€…POCKETç®—æ³•ä¸€ä¸ªå¥½çš„åˆå§‹ç‚¹ï¼Œå®ƒèƒ½æ›´å¿«å¾—åˆ°æœ€åå¥½çš„ç»“æœã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> classification </tag>
            
            <tag> regression </tag>
            
            <tag> visualization </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”linear regression</title>
      <link href="/2018/08/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94linear-regression/"/>
      <url>/2018/08/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94linear-regression/</url>
      
        <content type="html"><![CDATA[<p>ç»ˆäºå®Œæˆäº†æœºå™¨å­¦ä¹ åŸºçŸ³ç›¸å¯¹ç†è®ºçš„éƒ¨åˆ†ï¼Œå¯ä»¥å¼€å§‹ä¸€äº›å…·ä½“çš„ç®—æ³•çš„å­¦ä¹ äº†ã€‚é¦–å…ˆå­¦ä¹ çš„ç¬¬ä¸€ä¸ªç®—æ³•å°±æ˜¯çº¿æ€§å›å½’ï¼ˆlinear regressionï¼‰ã€‚<br><a id="more"></a><br>çº¿æ€§å›å½’æƒ³åšåˆ°çš„äº‹æƒ…æ˜¯ï¼Œç»™å‡ºä¸€å †ç‚¹ï¼Œä½¿ç”¨ä¸€æ¡ç›´çº¿ï¼ˆå¹³é¢æˆ–è€…å…¶ä»–ï¼‰æ¥æ‹Ÿåˆè¿™ä¸ªdatasetã€‚è€Œåœ¨ä¹‹å‰çš„noise and errorä¸­æåˆ°äº†ï¼Œå®ƒç”¨æ¥è¡¡é‡é”™è¯¯çš„åŠæ³•æ˜¯$(y - yâ€™)^2$ã€‚</p><p>å’Œä¹‹å‰çš„æœºå™¨å­¦ä¹ ç®—æ³•ä¹Ÿä¸€è‡´ï¼Œå› ä¸ºVC boundåœ¨çº¿æ€§å›å½’çš„ä¾‹å­ä¸­ä¹Ÿä¸€æ ·é€‚ç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬æƒ³åšçš„æ˜¯ä¿è¯$E_{in}$è¶Šå°è¶Šå¥½ã€‚</p><p>å¯¹äºçº¿æ€§å›å½’çš„Hé›†åˆæ˜¯ $yâ€™ = W^TX$ï¼Œå½“ç„¶$X$ï¼Œ$W$éƒ½æ˜¯åˆ—å‘é‡ï¼Œå‘é‡çš„ç»´åº¦æ˜¯d+1ï¼ˆdä¸ºç‰¹å¾é‡çš„ä¸ªæ•°ï¼‰.ä¸ºäº†minimize $E_{in}$ï¼Œé¦–å…ˆæˆ‘ä»¬åº”è¯¥å†™å‡ºæ¥$E_{in}$çš„è¡¨è¾¾å¼ï¼š</p><script type="math/tex; mode=display">E_{in} = \frac 1 n \sum _{i = 1} (h(X_n) - y)^2 =  \frac 1 n \sum _{i = 1} (W^TX_n - y_n)^2</script><p>è¿™ä¸ªè¡¨è¾¾å¼çœ‹ç€è¿˜æ¯”è¾ƒå¤æ‚ï¼Œæœ‰ä¸ªæ±‚å’Œç¬¦å·åœ¨é‡Œé¢ã€‚é¦–å…ˆï¼Œè¿™åé¢æœ‰Nä¸ªå¹³æ–¹å’Œï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆè½»æ¾çš„æƒ³åˆ°å‘é‡çš„èŒƒæ•°æ±‚æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å°†åŸå¼å†™æˆå¯¹å‘é‡æ±‚èŒƒæ•°çš„å½¢å¼ï¼š</p><script type="math/tex; mode=display">E_{in} = \frac 1 n || XW - Y||^2</script><p>ä¸Šå¼ä¸­</p><script type="math/tex; mode=display">X = \begin{bmatrix}...X_1^T... \\...X_2^T... \\..........\\...X_n ^T ...\end{bmatrix}</script><p>Xæ˜¯ä¸€ä¸ªn*ï¼ˆd+1ï¼‰çš„çŸ©é˜µï¼Œnæ˜¯æ ·æœ¬ä¸ªæ•°ï¼Œdæ˜¯ç‰¹å¾é‡ä¸ªæ•°ã€‚</p><p>å› æ­¤$E_{in}$å¯ä»¥è¯´æ˜¯ä¸€ä¸ªå…³äºWçš„å‡½æ•°ï¼Œè€Œä¸”å¯ä»¥è¯æ˜çš„æ˜¯ï¼Œè¿™ä¸ªå‡½æ•°æ˜¯è¿ç»­çš„ï¼ˆcontinuousï¼‰ï¼Œå¯å¯¼çš„ï¼ˆdifferentiableï¼‰,å¹¶ä¸”æ˜¯å‡¸å‡½æ•°ï¼ˆconvexï¼‰ã€‚å› æ­¤æˆ‘ä»¬ä¸€å®šå¯ä»¥æ±‚å¾—æœ€ä½ç‚¹ï¼Œä¹Ÿå°±æ˜¯$E_{in}$çš„æœ€å°å€¼ã€‚ä¸å®æ•°çš„å‡½æ•°ä¸€æ ·ï¼ˆå®é™…ä¸Šæ±‚çš„æ˜¯Wçš„å„ä¸ªæ–¹å‘çš„åå¯¼æ•°ï¼‰ï¼Œåœ¨å–æœ€å°å€¼çš„é‚£ä¸ªç‚¹çš„æ—¶å€™ï¼Œ$E_{in}$å…³äº$W$çš„å¯¼æ•°æ˜¯0ï¼Œè¡¨æ˜å–å¾—æ˜¯æå€¼ï¼Œä¹Ÿå°±æ˜¯æ¢¯åº¦æ˜¯0ã€‚</p><p>å°†ä¸Šå¼å±•å¼€å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">E_{in} = \frac 1 n (W^TX^TXW -2Y^TXW +Y^TY)ï¼Œ</script><p>å°†é™¤äº†Wä¹‹å¤–çš„çŸ©é˜µæˆ–è€…æ•°å­—çœ‹æˆå¸¸é‡ï¼Œåˆ™å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">E_{in} =  \frac 1 n (W^TAW-2BW+c)</script><p>ä¸ºäº†æ±‚å…³äº$W$çš„å¯¼æ•°ï¼Œæˆ‘ä»¬é¦–å…ˆæƒ³è±¡ä¸€ä¸‹å¦‚æœæ˜¯$W$ä¸€ç»´çš„è¯çš„æ ·å­ã€‚</p><script type="math/tex; mode=display">E_{in} = aW^2-2bW+c ; \frac {dE_{in}} {dW} = 2aW-2b</script><p>å¯¹åº”åˆ°çŸ©é˜µä¸Šæ¥è¯´:</p><script type="math/tex; mode=display">\frac {dE_{in}} {dW} = \frac 1 n 2AW-2B</script><p>å¯ä»¥çœ‹åˆ°çš„æ˜¯æ¢¯åº¦æ˜¯ä¸€ä¸ªå‘é‡ã€‚</p><p>ä¸ºäº†ä½¿å¾—$E_{in}$å–å¾—æœ€å°å€¼ï¼Œé‚£ä¹ˆ$X^TXW = X^TY$,å¦‚æœ$(X^TX)$çš„åçŸ©é˜µå­˜åœ¨ï¼Œé‚£ä¹ˆå¾ˆç®€å•:</p><script type="math/tex; mode=display">W = (X^TX)^{-1}X^TY</script><p>æˆ‘ä»¬ç§°$(X^TX)^{-1}X^T$ä¸ºpseudo-inverse $X^{+}$ã€‚è€Œä¸”å¤§å¤šæ•°æ—¶å€™æˆ‘ä»¬é‡åˆ°çš„éƒ½æ˜¯å¯é€†çš„ï¼Œå› ä¸º$n&gt;&gt;d+1$ï¼Œè¿™æ„å‘³ç€é¦–å…ˆå¯¹äºXçŸ©é˜µæ¥è¯´ï¼Œå®ƒçš„ç§©å¾ˆå¯èƒ½å°±æ˜¯ç­‰äºd+1çš„.è¿™æ ·å®ƒä»¬ç›¸ä¹˜çš„ç§©å¾ˆå¤§å¯èƒ½ä¹Ÿæ˜¯d+1ï¼Œä¹Ÿå°±æ˜¯å¯é€†ã€‚</p><p>ä½†æ˜¯å¦‚æœé‡åˆ°å¦å¤–çš„æƒ…å†µï¼Œå¦‚ä¸å¯é€†ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å…¶ä»–çš„æ–¹å¼æ¥å®šä¹‰$X^{+}$ï¼Œå…·ä½“çš„æ•°å­¦åŸç†éœ€è¦æ›´è¯¦ç»†çš„çº¿æ€§ä»£æ•°çŸ¥è¯†ï¼Œä½†æ˜¯æˆ‘ä»¬çŸ¥é“å¾ˆå¤šç¨‹åºåŒ…é‡Œéƒ½å®ç°äº†è¿™äº›ä¸œè¥¿ï¼Œç”¨å®ƒä¸€æ ·å¯ä»¥å¾—åˆ°æ¯”è¾ƒå¥½çš„ç»“æœã€‚</p><p>ç»¼ä¸Šï¼Œ$W = X^{+}Y$,$Yâ€™ = XW = XX^{+}Y$.</p><p>æœ€åï¼Œè¿™ä¸ªç®—æ³•ä¹‹æ‰€ä»¥å¯ä»¥å­¦ä¹ ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨VC boundæ¥è¯æ˜ã€‚ä½†æ˜¯è¿˜æœ‰å¦ä¸€ç§æ–¹æ³•ï¼Œä¹Ÿå¯ä»¥è¯æ˜å®ƒæ³›åŒ–èƒ½åŠ›ä¸é”™ï¼Œå¯ä»¥å–å¾—è‰¯å¥½çš„å­¦ä¹ æ•ˆæœï¼Œå½“ç„¶ï¼Œä¸VC boundä¸€æ ·ï¼Œä¸¥æ ¼çš„è¯æ˜éœ€è¦æ›´ä¸¥å¯†çš„æ•°å­¦æ¨å¯¼ï¼Œä¸‹é¢åªæ˜¯ç®€è¦ä»‹ç»ã€‚</p><p>é¦–å…ˆï¼Œæˆ‘ä»¬æ¨å¯¼$\overline {E_{in}}$æ˜¯å¾ˆæ¥è¿‘å™ªå£°çš„ï¼ˆå™ªå£°æ„å‘³ç€æˆ‘ä»¬æ— æ³•é€šè¿‡å­¦ä¹ è¿›è¡Œæ”¹å–„çš„éƒ¨åˆ†ï¼‰ï¼Œè€ŒåŒæ ·çš„æ­¥éª¤å¯ä»¥ç”¨åœ¨å¯¹$\overline {E_{out}}$çš„åˆ†æä¸Šï¼Œè¿™æ ·å°±è¯´æ˜äº†ï¼Œå¹³å‡æ¥è¯´æˆ‘ä»¬çš„ç®—æ³•æ˜¯å¯ä»¥å–å¾—å¾ˆå¥½çš„å­¦ä¹ æ•ˆæœçš„ã€‚</p><p>é¦–å…ˆï¼Œæˆ‘ä»¬åº”è¯¥å®šä¹‰ä¸€ä¸‹$\overline {E_{in}}$ï¼š</p><script type="math/tex; mode=display">\overline {E_{in}} = \epsilon _{D~P^N}\left\{\ E_{in}(W_{LIN} w.r.t. D)  \right\} = noise  level \cdot (1-\frac {d+1}{N})</script><p>å…·ä½“çš„å«ä¹‰å°±æ˜¯å¤šä¸ªæ ·æœ¬çš„$E_{in}$å¹³å‡å€¼ï¼Œå¤§æ¦‚çœ‹èµ·æ¥ä¼šæ¥è¿‘noise levelï¼Œè€Œå½“æ ·æœ¬é‡è¶Šå¤§ï¼Œä¸noise levelè¶Šæ¥è¿‘ã€‚</p><p>é¦–å…ˆï¼Œæˆ‘ä»¬åº”è¯¥å°†çº¿æ€§å›å½’å¾—åˆ°çš„ç»“æœå¸¦å…¥$E_{in}$çš„è¡¨è¾¾å¼ä¸­ï¼š</p><script type="math/tex; mode=display">E_{in} = \frac 1 N ||Y - \hat Y||^2 = \frac 1 N ||Y - XX^{+}Y ||^2 = \frac 1 N ||(I - X X^{+})Y||^2</script><p>æˆ‘ä»¬ç§°$XX^{+}$ä¸ºhat matrixã€‚</p><p>è¿™é‡Œæˆ‘ä»¬æ€è€ƒ$\hat Y = XW$åœ¨åšä»€ä¹ˆã€‚Yæ˜¯ä¸€ä¸ªNç»´å‘é‡ï¼Œè€ŒXå¯ä»¥çœ‹åšæ˜¯mä¸ªNç»´å‘é‡æ„æˆçš„çŸ©é˜µï¼Œè€Œå®é™…ä¸ŠWçš„ä½œç”¨ï¼Œæ˜¯ç»™æ¯ä¸ªçŸ©é˜µä¸­çš„Nç»´å‘é‡åˆ†é…ä¸€ä¸ªå‚æ•°ï¼Œè®©ä»–ä»¬åšçº¿æ€§ç»„åˆï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæ–°çš„Nç»´å‘é‡ã€‚ä¸ºäº†ä½¿å¾—$\hat Y$å°½é‡æ¥è¿‘äº$Y$ï¼Œä»å¦ä¸€ä¸ªæ–¹é¢æ¥è¯´ï¼Œå°±æ˜¯è®©$Y - \hat Y \bot Xâ€™s span$ï¼Œä¹Ÿå°±æ˜¯è®©ä»–ä»¬çš„å·®å°½é‡å‚ç›´äºXæ‰€å±•å¼€çš„ç©ºé—´ï¼Œå½“å‚ç›´æ—¶ï¼Œ$\hat Y$ç­‰äºYåœ¨Xä¸Šçš„æŠ•å½±ï¼Œè¿™æ—¶å€™äºŒè€…ç›¸å·®æ˜¯æœ€å°çš„.</p><p>å®é™…ä¸Šï¼ŒYä¸€èˆ¬ä¸å¯èƒ½è¢«Xå®Œç¾è¡¨ç¤ºï¼Œå› ä¸ºä¸€èˆ¬æ¥è®²$N&gt;&gt;d$.æ‰€ä»¥æˆ‘ä»¬èƒ½åšçš„å°±æ˜¯ä¸Šé¢è¯´çš„ã€‚</p><p>æ‰€ä»¥ï¼ŒHatçš„ä½œç”¨å°±æ˜¯å°†YæŠ•å½±åˆ°Xä¸Šã€‚è€Œ$(I-Hat)$å°±æ˜¯å°†Yè½¬æ¢ä¸º$(Y-\hat Y)$çš„çŸ©é˜µã€‚<br>è€Œä¸”æˆ‘ä»¬å¯ä»¥è®¡ç®—å‡ºæ¥$trace (I - Hat) = n - (d+1)$ï¼Œæ„å‘³ç€$(I- Hat)$æœ‰$N - d- 1$ä¸ªè‡ªç”±åº¦ã€‚</p><p>å¦‚æœ$Y$æ¥è‡ªä¸ä¸€äº›ç†æƒ³çš„$f(X)$ä¸å™ªå£°çš„ç»„åˆï¼Œé‚£ä¹ˆå¦‚æœåªæ˜¯ç†æƒ³çš„å‡½æ•°ï¼Œåˆ™ä¸Šè¿°å¾—åˆ°çš„å·®è·å®é™…ä¸Šæ˜¯ç”±å™ªå£°é€ æˆçš„ï¼Œå› æ­¤ï¼š</p><script type="math/tex; mode=display">E_{in} = \frac 1 N ||Y - \hat Y||^2  = \frac 1 N ||(I - Hat)noise||^2 = \frac 1 N (N - d - 1)||noise||^2</script><p>è€Œ$\overline {E_{in}} = ( 1 - \frac{d+1}{N})noise level$<br>åˆ©ç”¨ç±»ä¼¼çš„åŠæ³•ï¼Œå¯ä»¥æ¨æ–­å‡ºæ¥ï¼Œ$\overline {E_{out}} = ( 1 + \frac{d+1}{N})noise level$.</p><p>ä¹Ÿå°±æ˜¯å¹³å‡æ¥è¯´ï¼Œæˆ‘ä»¬é€šè¿‡æ‹Ÿåˆæ ·æœ¬ï¼Œå¯ä»¥å¾—åˆ°æ›´å°çš„é”™è¯¯ç‡ï¼Œä½†æ˜¯æ³›åŒ–ä¹‹åçš„é”™è¯¯ç‡å¾€å¾€æ›´å¤§ä¸€ç‚¹ã€‚</p><p>å› æ­¤å¹³å‡æ¥è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å°†$E_{in}$ä¸$E_{out}$ç”»åœ¨ä¸€å¼ å›¾ä¸Šï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/8P%292%25K%7DDKX0VSSRIW5WS%29JU.png" alt=""></p><p>å¦‚æœå¯¹åº”åˆ°å®é™…çš„æœºå™¨å­¦ä¹ ï¼Œå¯¹äºä¸€äº›å¾ˆå°‘çš„æ ·æœ¬é‡çš„æœºå™¨å­¦ä¹ è¿‡ç¨‹ï¼Œå¾ˆå®¹æ˜“æ‹ŸåˆæˆåŠŸï¼Œå¾—åˆ°å¾ˆå¥½çš„$E_{in}$ï¼Œç”šè‡³æ˜¯0ï¼Œä½†æ˜¯è¿™æ—¶å€™æ³›åŒ–èƒ½åŠ›å´å¾ˆå·®ã€‚æœºå™¨å­¦ä¹ ï¼Œä¸æ˜¯ä¸€å‘³åœ°å¢åŠ ç‰¹å¾é‡å‡å°‘$E_{in}$ï¼Œè€Œçº æ­£è¿‡æ‹Ÿåˆçš„ä¸€ç§åŠæ³•ï¼Œå°±æ˜¯å¢åŠ æ ·æœ¬é‡ã€‚</p><p>æœ€åï¼Œæˆ‘æƒ³è¯´çš„æ˜¯ï¼Œä¸Šé¢çš„â€è¯æ˜â€éå¸¸ä¸ä¸¥æ ¼ï¼Œç”šè‡³æœ‰äº›åœ°æ–¹è®©äººè´¹è§£ã€‚å¯¹äºæœºå™¨å­¦ä¹ ï¼Œç†è®ºéƒ¨åˆ†ä¸¥æ ¼çš„è¯æ˜æ›´å¤šæ˜¯æ•°å­¦å’Œç»Ÿè®¡çš„äº‹æƒ…ï¼Œè€Œå­¦ä¹ è®¡ç®—æœºçš„äººæ›´å¤šçš„æ˜¯æŒæ¡å„ç§ç®—æ³•ï¼Œå­¦ä¹ åˆ©ç”¨å®ƒæ¥è§£å†³é—®é¢˜ã€‚å³ä½¿è¿™æ ·ï¼Œå°½å¯èƒ½å¤šåœ°äº†è§£ç†è®ºéƒ¨åˆ†ï¼Œå¯¹äºå®é™…çš„åº”ç”¨æœ‰å¾ˆå¤§çš„å¸®åŠ©ã€‚è¿™ä¸ªä¸–ç•Œå¾ˆå¤æ‚ï¼Œæ°¸è¿œä¿æŒä¸€ä¸ªå¥½å¥‡å¿ƒå§ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> regression </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”ï¼ˆåŸºçŸ³ï¼‰ä½œä¸šäºŒ</title>
      <link href="/2018/08/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%EF%BC%88%E5%9F%BA%E7%9F%B3%EF%BC%89%E4%BD%9C%E4%B8%9A2/"/>
      <url>/2018/08/14/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%EF%BC%88%E5%9F%BA%E7%9F%B3%EF%BC%89%E4%BD%9C%E4%B8%9A2/</url>
      
        <content type="html"><![CDATA[<p>æ€»å…±20é“é¢˜ç›®ã€‚<br><a id="more"></a></p><p>Questions 1-2 are about noisy targets.</p><p><strong>1.Consider the bin model for a hypothesis $h$ that makes an error with probability $\mu$ in approximating a deterministic target function $f$ (both $h$ and $f$ outputs $\{-1, +1\}$).</strong><br> If we use the same $h$ to approximate a noisy version of $f$ given by</p><script type="math/tex; mode=display">P({\bf{x}},y) = P({\bf{x}})P(y|{\bf{x}})P(x,y)=P({\bf{x}})P(yâˆ£ {\bf{x}})</script><script type="math/tex; mode=display">P(y|{\bf{x}}) = \left \{ \begin{matrix} \lambda & {y=f(x)} \\1-\lambda & \text{otherwise}\end{matrix} \right.â€‹</script><p>What is the probability of error that $h$ makes in approximating the noisy target $y$?</p><p>a. $1-\lambda$</p><p>b. $\mu$</p><p>c. $\lambda(1-\mu)+(1-\lambda)\mu$</p><p>d. $\lambda\mu+(1-\lambda)(1-\mu)$</p><p>e. none of the other choices</p><p>è¿™ä¸ªé¢˜ç›®åŠå¤©çœ‹ä¸æ‡‚ï¼Œå®é™…ä¸Šæ„æ€æ˜¯å™ªå£°çš„å‡ ç‡æ˜¯($1-\lambda$)ã€‚ç®—æœ€åçš„é”™è¯¯ç‡ã€‚æ‰€ä»¥ï¼Œå½“é¢„æµ‹é”™è¯¯æ—¶å€™ï¼Œå¦‚æœæ˜¯éå™ªå£°ï¼Œåˆ™æœ€åè¿˜æ˜¯é”™è¯¯ï¼›å½“é¢„æµ‹æ­£ç¡®æ—¶å€™ï¼Œç»“æœè¯¥æ ·æœ¬æ˜¯å™ªå£°ï¼Œåˆ™ä¼šé€ æˆé”™è¯¯ï¼Œå°†ä¸¤ç§æƒ…å†µåŠ èµ·æ¥ï¼Œå› æ­¤ç­”æ¡ˆæ˜¯ $\mu \lambda + (1-\lambda)(1-\mu)$ï¼Œé€‰d.</p><p><strong>2. Following Question 1, with what value of $\lambda$ will the performance of $h$ be independent of $\mu$?</strong></p><p>a. 0</p><p>b. 0 or 1</p><p>c. 1</p><p>d. 0.5</p><p>e. none of the other choices</p><p>è¿™é“é¢˜ç›®å¾ˆç®€å•ï¼Œæ„æ€æ˜¯$\lambda$çš„å€¼æ˜¯å¤šå°‘çš„æ—¶å€™ï¼Œhçš„æ€§èƒ½ä¸$\mu$æ— å…³ã€‚<br>å¾ˆç®€å•ï¼Œå°†é”™è¯¯ç‡å±•å¼€ï¼š$\mu(2 \lambda - 1) + 1 - \lambda$ï¼Œå¯ä»¥å¾ˆå®¹æ˜“çœ‹å‡ºæ¥ï¼Œ$\lambda = 0.5$.</p><p>Questions 3-5 are about generalization error, and getting the feel of the bounds numerically.</p><p><strong>3. Please use the simple upper bound $N^{d_{\text{vc}}}$ on the growth function $m_{\mathcal{H}}(N)$,assuming that $N \geq 2$ and $d_{vc} \geq 2$.<br>For an $\mathcal{H}$ with $d_{\text{vc}} = 10$, if you want $95\%$ confidence that your generalization error is at most 0.05, what is the closest numerical approximation of the sample size that the VC generalization bound predicts?</strong></p><p>a. 420,000</p><p>b. 440,000</p><p>c. 460,000</p><p>d. 480,000</p><p>e. 500,000</p><p>è¿™ä¸ªé¢˜ç›®è€ƒéªŒçš„æ˜¯VC bound.ç¿»çœ‹ç›´æ¥æˆ‘ä»¬æ¨å¯¼å‡ºæ¥çš„æœ€ç»ˆç»“æœï¼š</p><script type="math/tex; mode=display">\epsilon = \sqrt {\frac 8 N \ln {(\frac {4(2N)^{d_{vc}}} {\delta })}}</script><p>ä¸Šå¼ä¸­ï¼Œ$\epsilon = 0.05(generalization error), \delta = 0.05 (confidence)$,å¸¦å…¥ä¸Šå¼ä¸­ï¼Œå¯ä»¥è®¡ç®—å‡ºæ¥ä»¥ä¸‹ç»“æœï¼š<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\\Îµ^<span class="number">2</span> = (<span class="number">8</span>/N)*ln(((<span class="number">2</span>*N)^<span class="number">10</span>*<span class="number">4</span>)/<span class="number">0.05</span>) $\approx$ <span class="number">0.0025</span></span><br><span class="line">N = <span class="number">420</span>,<span class="number">000</span>  Îµ = <span class="number">0.0026817828255785</span></span><br><span class="line">N = <span class="number">440</span>,<span class="number">000</span>  Îµ = <span class="number">0.0025683417908949</span></span><br><span class="line">N = <span class="number">460</span>,<span class="number">000</span>  Îµ = <span class="number">0.0024644054978248</span></span><br><span class="line">N = <span class="number">480</span>,<span class="number">000</span>  Îµ = <span class="number">0.0023688152044852</span> </span><br><span class="line">N = <span class="number">500</span>,<span class="number">000</span>  Îµ = <span class="number">0.0022805941154291</span></span><br></pre></td></tr></table></figure></p><p>å¯ä»¥çœ‹åˆ°ç­”æ¡ˆä¸º 460ï¼Œ000.</p><p><strong>4. There are a number of bounds on the generalization error $\epsilon$, all holding with probability at least $1 - \delta$. Fix $d_{\text{vc}} = 50$d and $\delta = 0.05$ and plot these bounds as a function of N. Which bound is the tightest (smallest) for very large N, say N=10,000?<br>Note that Devroye and Parrondo &amp; Van den Broek are implicit bounds in $\epsilon$.</strong></p><p>a. Original VC bound: $ \epsilon \le \sqrt{\frac{8}{N}\ln\frac{4m_{\mathcal{H}}(2N)}{\delta}}$</p><p>b. Rademacher Penalty Bound: $ \epsilon \le \sqrt{\frac{2\ln(2Nm_{\mathcal{H}}(N))}{N}} + \sqrt{\frac{2}{N}\ln\frac{1}{\delta}} + \frac{1}{N}$</p><p>c. Parrondo and Van den Broek: $ \epsilon \le \sqrt{\frac{1}{N}(2\epsilon + \ln\frac{6m_{\mathcal{H}}(2N)}{\delta})}$</p><p>d. Devroye: $\epsilon \le \sqrt{\frac{1}{2N} (4\epsilon(1 + \epsilon) + \ln \frac{4m_{\mathcal{H}}(N^2)}{\delta})}$</p><p>e. Variant VC bound: $\epsilon \le \sqrt{\frac{16}{N}\ln\frac{2m_{\mathcal{H}}(N)}{\sqrt{\delta}}}$</p><p>ä»£å…¬å¼çš„é—®é¢˜ï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a. (8/10000*ln((4*(2*10000)^50)/0.05))^(0.5) = 0.63217491520084</span><br><span class="line"></span><br><span class="line">b. ((2*ln(2*10000*10000^50))/10000)^0.5+(2/10000*ln(1/0.05))^0.5+1/10000 = 0.33130878596164</span><br><span class="line"></span><br><span class="line">c. (1/10000*(2*Îµ+ln(6*(20000)^50/0.05)))^0.5 å½“Îµç­‰äº0.223å·¦å³çš„æ—¶å€™å–ç­‰å·,å½“Îµå¤§äº0.223æ—¶å€™ï¼Œä¸Šå¼å·²ç»ä¸å†æˆç«‹ï¼Œå½“å°äº0.223æ—¶å€™æ˜¯æˆç«‹çš„ï¼Œæ‰€ä»¥boundåœ¨æ˜¯0.223å·¦å³</span><br><span class="line"></span><br><span class="line">d. (1/20000*(4*Îµ*(1+Îµ)+ln(4*1000000^(50)/0.05)))^0.5 åŒä¸Šï¼Œboundåœ¨0.186å·¦å³</span><br><span class="line"></span><br><span class="line">e. (16/10000*ln(2*10000^50/0.5))^0.5 = 0.85967743993657</span><br></pre></td></tr></table></figure></p><p>ç­”æ¡ˆä¸ºDevroye,é€‰d.</p><p><strong>5. Continuing from Question 4, for small N, say N=5, which bound is the tightest (smallest)?</strong></p><p>ç­”æ¡ˆä¸ä¸Šé¢è§£ç­”è¿‡ç¨‹ç±»ä¼¼ã€‚<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a. (8/5*ln((4*(2*5)^50)/0.05))^0.5 = 13.828161484991</span><br><span class="line"></span><br><span class="line">b. ((2*ln(2*5*5^50))/5)^0.5+(2/5*ln(1/0.05))^0.5+1/5 = 7.0487765641837</span><br><span class="line"></span><br><span class="line">c. ç­”æ¡ˆä¸º5.0å·¦å³</span><br><span class="line"></span><br><span class="line">d. ç­”æ¡ˆä¸º5.5å·¦å³</span><br><span class="line"></span><br><span class="line">e. (16/5*ln(2*5^50/0.5))^0.5 = 16.184752328814</span><br></pre></td></tr></table></figure></p><p>æ˜¾ç„¶ç­”æ¡ˆé€‰Parrondo and Van den Broek.</p><p>In Questions 6Â­-11, you are asked to play with the growth function or VC-dimension of some hypothesis sets.</p><p><strong>6. What is the growth function $m_{\mathcal{H}}(N)$ of â€œpositive-and-negative intervals on $\mathbb{R}$â€? The hypothesis set $\mathcal{H}$ of â€œpositive-and-negative intervalsâ€ contains the functions which are $+1$ within an interval $[\ell,r]$ and âˆ’1 elsewhere, as well as the functions which are âˆ’1 within an interval $[\ell,r]$ and +1 elsewhere.<br>For instance, the hypothesis $h_1(x)=sign(x(xâˆ’4))$ is a negative interval with -1 within $[0, 4]$ and +1 elsewhere, and hence belongs to $\mathcal{H}$. The hypothesis $h_2(x)=sign((x+1)(x)(xâˆ’1))$ contains two positive intervals in $[-1, 0]$ and $[1, \infty)$ and hence does not belong to $\mathcal{H}$.</strong></p><p>a. $N^2-N+2$</p><p>b. $N^2$</p><p>c. $N^2+1$</p><p>d. $N^2+N+2$</p><p>e. none of the other choices.</p><p>è¿™ä¸ªé¢˜ç›®é¢˜æ„æè¿°å¾ˆé•¿ï¼Œä½†æ˜¯çœ‹æ‡‚äº†å¹¶ä¸éš¾ã€‚å®é™…ä¸Šå°±æ˜¯positive intervalsçš„æ‹“å±•ï¼Œåªä¸è¿‡åŸæ¥æ˜¯ä¸­é—´æ˜¯æ­£çš„ï¼Œä¸¤è¾¹æ˜¯è´Ÿçš„,è¿™æ—¶å€™æƒ…å†µä¸ä¹‹å‰å°±ä¸ä¸€æ ·äº†ã€‚<br>ä¹‹å‰ï¼ŒNä¸ªæ ·æœ¬å°†è¿™ä¸ªç›´çº¿åˆ’åˆ†æˆäº†N+1ä¸ªåŒºåŸŸï¼Œä»ä¸­å–ä¸¤ä¸ªï¼Œä¸­é—´æ˜¯æ­£ï¼Œå¤–é¢æ˜¯è´Ÿï¼ŒåŒæ—¶è¿˜åŒ…å«ä¸€ç§å…¨æ˜¯è´Ÿçš„æƒ…å†µï¼Œæ¯”å¦‚é€‰çš„ä¸¤ä¸ªç‚¹åœ¨ä¸€ä¸ªåŒºåŸŸå†…ï¼Œå°±ä¼šæœ‰å…¨è´Ÿçš„æƒ…å†µï¼Œå› æ­¤ç»“æœæ˜¯$C_{N+1}^2+1 =  frac{1}{2} N^2+ \frac{1}{2}N+1$;<br>è€Œæœ¬é¢˜å°±è¦æ³¨æ„ä¸€äº›é—®é¢˜äº†ï¼Œå¾ˆç›´è§‰çš„æƒ³æ³•æ˜¯å¯¹ä¸Šé¢çš„åšæ³•ç¿»å€ï¼Œä½†æ˜¯å®é™…ä¸Šä»”ç»†æƒ³æƒ³ï¼Œå¦‚æœæˆ‘ä»¬å–åˆ°æœ€è¾¹ä¸Šçš„ä¸¤ä¸ªç‚¹ï¼Œé‚£ä¹ˆå®é™…ä¸Šå°±åŒ…å«äº†å…¨æ˜¯æ­£å’Œå…¨æ˜¯è´Ÿçš„ç»“æœï¼Œå¦ä¸€æ–¹é¢ï¼Œåªè¦æˆ‘ä»¬å–åˆ°äº†æœ€è¾¹ä¸Šçš„åŒºåŸŸæŸä¸ªç‚¹ï¼Œå°±ä¼šæœ‰é‡å¤çš„ç»“æœï¼ˆä¸å–å¦ä¸€ç«¯çš„ç«¯ç‚¹æ˜¯ä¸€æ ·çš„ï¼‰ï¼Œå› æ­¤å–åˆ°æœ€è¾¹ä¸Šçš„ç‚¹åº”å½“åªç®—ä¸€æ¬¡ã€‚<br>æ‰€ä»¥æˆ‘ä»¬è¦æ¢ä¸ªæ€è·¯ï¼Œä¸€æ˜¯ä¸¤ä¸ªç‚¹éƒ½ä¸æ˜¯ç«¯ç‚¹åŒºåŸŸçš„ï¼š$C_{N-1}^2$,<br>ç¬¬äºŒä¸ªæ˜¯ä¸¤ä¸ªç‚¹æœ‰ä¸€ä¸ªæ˜¯ç«¯ç‚¹åŒºåŸŸçš„ï¼š$C_{N-1}^1 \times C_2^1 $,<br>æœ€åä¸€ç§æƒ…å†µæ˜¯ä¸¤ä¸ªç«¯ç‚¹åŒºåŸŸçš„ï¼Œæœ‰ä¸¤ç§æƒ…å†µï¼Œå…¨æ­£æˆ–è€…å…¨è´Ÿï¼š2.<br>è‡³äºå–ç›¸åŒåŒºåŸŸçš„æƒ…å†µå¾—åˆ°çš„ç»“æœæ˜¯ä¸æœ€åä¸€ç§æƒ…å†µä¸€è‡´çš„ã€‚<br>æ‰€ä»¥æœ€åç»“æœï¼š$m_H(N) = N^2-N+2 $.</p><p>å¦ä¸€ç§è®¨å·§çš„åšæ³•ï¼šå½“N = 3çš„æ—¶å€™ï¼Œå…¶ä»–çš„ç­”æ¡ˆéƒ½å¤§äº8ï¼Œè¿™æ˜¯ä¸å¯èƒ½å‘ç”Ÿçš„ã€‚</p><p><strong>7. Continuing from the previous problem, what is the VC-dimension of the hypothesis set of â€œpositive-and-negative intervals on $\mathbb{R}$â€?</strong></p><p>æ—¢ç„¶ä¸Šé¢éƒ½å¾—åˆ°æˆé•¿å‡½æ•°äº†ï¼Œå¾ˆè½»æ˜“å¯ä»¥å¾—åˆ°ç»“æœï¼Œç­”æ¡ˆæ˜¯3ï¼Œå½“ä¸ºN = 4æ—¶å€™ï¼Œ$N^2-N+2 = 14&lt;16$.</p><p><strong>8. What is the growth function $m_{\mathcal{H}}(N)$ of â€œpositive donuts in $\mathbb{R}^2$â€?</strong></p><p>The hypothesis set $\mathcal{H}$ of â€œpositive donutsâ€ contains hypotheses formed by two concentric circles centered at the origin. In particular, each hypothesis is +1 within a â€œdonutâ€ region of $a^2 \leq x_1^2+x_2^2 \leq b^2$ and âˆ’1 elsewhere. Without loss of generality, we assume $0 \lt a \lt b \lt \infty$.</p><p>a. $N+1$</p><p>b. $C_{N+1}^2+1$</p><p>c. $C_{N+1}^3+1$</p><p>d. none of the other choices.</p><p>e. $C_N^2+1$</p><p>è¿™é“é¢˜ç›®æ˜¯è¦åœ¨ä»¥åŸç‚¹ä¸ºä¸­å¿ƒç”»ä¸¤ä¸ªåœ†ï¼Œåˆ†å¸ƒåœ¨ç¯ä¸Šçš„ç‚¹ä¸ºæ­£ï¼Œå…¶ä½™ä¸ºè´Ÿã€‚çœ‹ä¸Šå»ç»´åº¦ä¼¼ä¹å˜æˆäº†äºŒç»´ï¼Œå®é™…ä¸Šè¿˜æ˜¯ä¸€ç»´çš„ï¼šè¿™ä¸ªç»´åº¦å°±æ˜¯ä¸åŸç‚¹çš„è·ç¦»ã€‚å¦‚æœä¸åŸç‚¹è·ç¦»ä¸€è‡´ï¼Œå®ƒä»¬çš„åˆ†ç±»ä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç®€åŒ–ä¸€ä¸‹è¿™ä¸ªé—®é¢˜ï¼Œå°†ä¸åŸç‚¹çš„è·ç¦»ç”»åˆ°ä¸€æ¡çº¿ä¸Šï¼Œç«‹é©¬è¿™ä¸ªé—®é¢˜å°±æˆä¸ºä¸€èˆ¬çš„positive intervalsé—®é¢˜äº†ï¼Œç­”æ¡ˆä¹Ÿæ˜¯ä¸€æ ·çš„ï¼š$C_{N+2}^2+1$ã€‚</p><p><strong>9. Consider the â€œpolynomial discriminantâ€ hypothesis set of degree $D$ on $\mathbb{R}$, which is given by</strong></p><script type="math/tex; mode=display">\begin{eqnarray}\mathcal{H} = \left\{ h_{\bf{c}} \; \middle| \; h_{\bf{c}}(x) = {\rm{sign}}\left(\sum_{i=0}^D c_ix^i\right) \right\}\end{eqnarray}</script><p>What is the VC-dimension of such an $\mathcal{H}$?</p><p>è¿™ä¸ªä¸å°±æ˜¯perceptronå—ï¼Ÿç­”æ¡ˆæ˜¯$D+1$.</p><p><strong>10.Consider the â€œsimplified decision treesâ€ hypothesis set on $\mathbb{R}^d$, which is given by</strong></p><script type="math/tex; mode=display">\begin{eqnarray}\mathcal{H}= \{h_{\mathbf{t},\mathbf{S}} \; | & \; h_{\mathbf{t},\mathbf{S}}(\mathbf{x}) = 2 [[\mathbf{v}\in S]] - 1,\text{ where} \; v_i = [[x_i>t_i]], & \\& \mathbf{S} \text{ a collection of vectors in } \{0,1\}^d,\mathbf{t} \in \mathbb{R}^d &\}\end{eqnarray}</script><p>That is, each hypothesis makes a prediction by first using the $d$ thresholds $t_i$ to locate $\mathbf{x}$ to be within one of the $2^d$ hyper-rectangular regions, and looking up $\mathbf{S}$ to decide whether the region should be +1 or âˆ’1.</p><p>What is the VC-dimension of the â€œsimplified decision treesâ€ hypothesis set?</p><p>a. $2^d$</p><p>b. $2^{d+1}-3$</p><p>c. $\infty$</p><p>d. none of the other choices.</p><p>e. $2^{d+1}$</p><p>è¿™ä¸ªé¢˜ç›®çœ‹ä¸å¤§æ‡‚â€¦</p><p><strong>11. Consider the â€œtriangle wavesâ€™â€™ hypothesis set on $\mathbb{R}$, which is given by</strong></p><script type="math/tex; mode=display">\begin{eqnarray}\mathcal{H} = \{h_{\alpha} \; | & \; h_{\alpha}(x) = \text{sign}(| (\alpha x) \mbox{ mod } 4 - 2| - 1), \alpha \in \mathbb{R} \}\end{eqnarray}</script><p>Here $(z mod 4)$ is a number $z - 4k$ for some integer $k$ such that $z - 4k \in [0, 4)$. For instance, $(11.26 mod 4)$ is 3.26, and $(âˆ’11.26 mod 4)$ is 0.74. What is the VC-dimension of such an $\mathcal{H}$?</p><p>a. 1</p><p>b. 2</p><p>c. âˆ</p><p>d. none of the other choices</p><p>e. 3</p><p>è¿™ä¸ªé—®é¢˜çœ‹ä¸Šå»å¾ˆå¤æ‚ï¼Œæ‰€ä»¥ä¸€æ­¥ä¸€æ­¥æ‹†å¼€æ¥è§£å†³ã€‚<br>ç¬¬ä¸€ï¼Œè¿™ä¸ªç‚¹æ˜¯åˆ†å¸ƒåœ¨å®æ•°è½´ä¸Šçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦é¦–å…ˆå¼„æ¸…æ¥šè½´ä¸Šçš„é‚£éƒ¨åˆ†çš„ç‚¹æ˜¯+1ï¼Œå“ªéƒ¨åˆ†çš„ç‚¹æ˜¯-1.<br>å¦‚æœæ˜¯-1ï¼Œåˆ™$|(\alpha x) mod 4 - 2| &lt; 1 $,å¯ä»¥æ¨å‡ºæ¥$(\alpha x) mod 4 \in (1,3)$,åŒç†å¯ä»¥é€€å‡ºæ¥å¦‚æœæ˜¯+1ï¼Œåˆ™ $(\alpha x) mod 4 \in (0,1) \bigcup (3,4)$ ,æ ¹æ®é¢˜ä¸­è´Ÿæ•°å–ä½™æ•°çš„å®šä¹‰ï¼Œæ€»ç»“ä¸€ä¸‹å¦‚ä¸‹ï¼š</p><script type="math/tex; mode=display"> h_{\alpha}(x) = \left \{\begin{matrix}+1& \alpha x \in (-1+4k,1+4k) \\-1 & \alpha x \in (1+4k,3+4k)\end{matrix} \right.</script><p>å¯¹äºN = 1å’ŒN = 2çš„æ—¶å€™ï¼Œå¾ˆå®¹æ˜“å¯ä»¥çŸ¥é“å„ç§æƒ…å†µéƒ½æ˜¯å¯ä»¥shatterçš„ã€‚</p><p>ï¼ˆä¸¾ä¸ªN=2çš„ä¾‹å­ï¼Œå¦‚</p><p>$[0.6,0.7]â€”[+1,+1]; [0.6 \times \frac 9 6, 0.7 \times \frac 9 6 ]â€”[+1,-1];[0.6 \times \frac 29 6,0.7 \times \frac 29 6]â€”[-1,+1];[0.6 \times \frac 29 7,0.7 \times \frac 29 7]â€”[-1,-1]$ï¼‰. </p><p>å½“Nç­‰äº3çš„æ—¶å€™ï¼Œä¹Ÿæ˜¯å¯ä»¥è¢«shatterã€‚</p><p>å®é™…ä¸Šï¼Œå–ä½™çš„è¿‡ç¨‹ä¸­æœ‰è¿™ä¹ˆä¸€ä¸ªæ€§è´¨ï¼š$\alpha x mod 4 = [\alpha (x mod 4)] mod 4$ï¼Œè¿™æ„å‘³ç€(å‡è®¾æœ‰3ä¸ªæ ·æœ¬)ï¼Œå¯¹äºä»»ä½•å¤§å°çš„$x_n$,æˆ‘ä»¬éƒ½å¯ä»¥å°†å®ƒç¼©æ”¾åˆ°$[0,4)$çš„èŒƒå›´æ¥è¿›è¡Œå¤„ç†ã€‚è¿™ä¸ªé¢˜ç›®çš„ç­”æ¡ˆæ˜¯âˆã€‚ä½†æ˜¯å¦‚ä½•è¯æ˜æˆ‘è¿˜ä¸æ˜¯å¾ˆæ¸…æ¥šã€‚</p><p>In Questions 12-15, you are asked to verify some properties or bounds on the growth function and VC-dimension.</p><p><strong>12. Which of the following is an upper bounds of the growth function $m_\mathcal{H}(N)$ for $N \ge d_ \ge 2$?</strong></p><p>a. $m_H(âŒŠN/2âŒ‹)$</p><p>b. $2^{d_{vc}}$</p><p>c. $ \min _{1 \leq i \leq N-1} 2^im_H(N-i)$</p><p>d. $\sqrt {N^{d_{vc}}}$</p><p>e. none of the other choices.</p><p>è¿™ä¸ªé¢˜ç›®é—®çš„æ˜¯æˆé•¿å‡½æ•°ã€‚å¯¹äºæˆé•¿å‡½æ•°çš„ç•Œé™ï¼Œä¹‹å‰çš„åšå®¢å·²ç»æœ‰äº†ä»¥ä¸‹çš„è¯´æ˜ï¼š</p><script type="math/tex; mode=display">B(N,k) \leq \sum _{i=0} ^{k-1} C_N^i</script><p>è€Œä¸Šå¼ä¸­ï¼Œ$k = d+1$ã€‚<br>æ ¹æ®ä¸Šå¼ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆè½»æ˜“çš„æ’é™¤a,bä¸¤é¡¹ã€‚åŒæ—¶ï¼Œå¦‚æœä¸¾ä¾‹è®¡ç®—ï¼Œäº¦å¯ä»¥æ’é™¤é€‰é¡¹dã€‚å¦‚ï¼Œ$B(6,3) = 22 ï¼ \sqrt {6^2}$.</p><p>å› æ­¤ç­”æ¡ˆæ˜¯c.è‡³äºå¯¹cçš„è¯æ˜ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¹‹å‰vc boundçš„è¡¨æ ¼é‡Œå‘ç°ï¼Œ </p><p>$B(N,d) = B(N-1,d-1)+B(N-1,d) \leq 2 \times B(N-1,d) \leq 4 \times B(N-2,d) \leq 2^i \times B(N-i,d)$ï¼Œå› æ­¤ï¼Œä»»ä½• $2^im_H(N-i)$éƒ½æ˜¯å¤§äºç­‰äº$m_H(N)$çš„ï¼Œé€‰æ‹©ä¸€ä¸ªæœ€å°çš„å³å¯ã€‚</p><p><strong>13. Which of the following is not a possible growth functions $m_{\mathcal{H}}(N)$for some hypothesis set?</strong></p><p>a. $2^N$</p><p>b. $2^{âŒŠ \sqrt {N} âŒ‹}$</p><p>c. 1</p><p>d. $N^2 -N +2$</p><p>e. none of the other choices.</p><p>ç­”æ¡ˆæ˜¯b. é¦–å…ˆï¼Œa,dçš„æƒ…å†µæˆ‘ä»¬éƒ½é‡åˆ°è¿‡ï¼Œè€Œcçš„æƒ…å†µä¹Ÿæ˜¯å¾ˆç®€å•çš„ï¼Œæ¯”å¦‚è¿™ä¸ªHå¯¹æ‰€æœ‰çš„æ ·æœ¬éƒ½å–æ­£ã€‚è‡³äºbä¸ºä»€ä¹ˆé”™äº†ï¼Œå½“N = 1çš„æ—¶å€™ï¼Œ$2^1 = 2$ï¼Œè€Œå½“N = 2çš„æ—¶å€™ï¼Œ$m_H(2) = 2$ï¼Œ<br>$m_H(3) =2$, $m_H(4) = 4$. å®é™…ä¸Šæ˜¯ä¸å¯èƒ½å‡ºç°æˆé•¿å‡½æ•°å‘ˆç°å‡ºè¿™æ ·çš„è§„å¾‹å¢é•¿çš„ï¼Œå› ä¸ºNä¸ªç‚¹ä¸­éšæ„å–N-1ä¸ªå‡ºæ¥ï¼Œå¿…ç„¶è¦æ»¡è¶³ä¹‹å‰çš„N-1ä¸ªæ—¶å€™çš„æ‰€æœ‰è¦æ±‚ï¼ˆå‡ºç°çš„æƒ…å†µä¸ä¹‹å‰çš„N-1çš„å„ç§æƒ…å†µä¸€è‡´ï¼Œå¯ä»¥æœ‰é‡å¤ï¼Œä½†æ˜¯ä¸èƒ½å¤šä¹Ÿä¸èƒ½å°‘ï¼‰ï¼Œè¿™ä¿è¯äº†æˆé•¿å‡½æ•°è¦ä¹ˆæ˜¯ä¸¥æ ¼å•è°ƒå¢çš„ï¼Œè¦ä¹ˆæ˜¯ä¸å˜çš„ï¼ˆæˆ‘çš„ç†è§£ï¼‰ã€‚</p><p><strong>14. For hypothesis sets $\mathcal{H}_1, \mathcal{H}_2, â€¦, \mathcal{H}_K$ with finite, positive VC-dimensions d_(\mathcal{H}_k), some of the following bounds are correct and some are not.</strong></p><p>Which among the correct ones is the tightest bound on $d_(\bigcap_{k=1}^{K}!\mathcal{H}_k)$, the VC-dimension of the $\bf{intersection}$ of the sets?</p><p>(The VC-dimension of an empty set or a singleton set is taken as zero.)</p><p>è¿™ä¸ªé¢˜ç›®æ˜¯æœ‰Kä¸ªHé›†åˆï¼Œæ¯ä¸ªé›†åˆéƒ½å¯¹åº”ä¸€ä¸ªvc dimensionï¼Œé—®é¢˜æ˜¯è¿™äº›é›†åˆçš„äº¤é›†æ„æˆçš„é›†åˆçš„vc dimensionçš„èŒƒå›´ã€‚</p><p>a. $ 0 \leq d_{vc}({\bigcap _{k=1}}^K H_k) \leq \sum _{k=1} ^K d_{vc}(H_k)$</p><p>b. $0 \leq d_{vc}({\bigcap _{k=1}}^K H_k) \leq \min\{d_{vc}(H_k) \}_{k=1}^K $</p><p>c. $0 \leq d_{vc}({\bigcap _{k=1}}^K H_k) \leq \max\{d_{vc}(H_k) \}_{k=1}^K $</p><p>d. $ \min\{d_{vc}(H_k) \}_{k=1}^K  \leq d_{vc}({\bigcap _{k=1}}^K H_k) \leq \max\{d_{vc}(H_k) \}_{k=1}^K $</p><p>e. $ \min\{d_{vc}(H_k) \}_{k=1}^K  \leq d_{vc}({\bigcap _{k=1}}^K H_k) \leq \sum _{k=1} ^K d_{vc}(H_k) $</p><p>å¦‚æœäº¤é›†ä¸ºç©ºï¼Œé‚£ä¹ˆvc dimensionä¸º0ã€‚åŒæ—¶ï¼Œä¸ç®¡æ€ä¹ˆè¯´ï¼ŒHçš„å¤§å°ä¸å¯èƒ½æ˜¯æ¯”ä¹‹å‰ä»»ä½•ä¸€ä¸ª<br>$H_n$å¤§ï¼Œè€Œä¸”ä¸€å®šæ˜¯ä¹‹å‰ä»»ä½•ä¸€ä¸ªé›†åˆçš„ä¸€éƒ¨åˆ†ã€‚å› æ­¤å®ƒçš„vc dimensionä¹Ÿä¸ä¼šè¶…è¿‡ä¹‹å‰ä»»ä½•ä¸€ä¸ªé›†åˆï¼Œæ‰€æœ‰ç­”æ¡ˆå¾ˆæ˜æ˜¾ï¼Œæ˜¯b.</p><p><strong>15. For hypothesis sets $\mathcal{H}_1, \mathcal{H}_2, â€¦, \mathcal{H}_K$ with finite, positive VC-dimensions d_(\mathcal{H}_k), some of the following bounds are correct and some are not.</strong></p><p>Which among the correct ones is the tightest bound on $d_(\bigcup_{k=1}^{K}!\mathcal{H}_k)$, the VC-dimension of the $\bf{union}$ of the sets?</p><p>a.  $ 0 \leq d_{vc}({\bigcap _{k=1}}^K H_k) \leq K-1+\sum _{k=1} ^K d_{vc}(H_k)$</p><p>b. $ \min\{d_{vc}(H_k) \}_{k=1}^K  \leq d_{vc}({\bigcap _{k=1}}^K H_k) \leq \sum _{k=1} ^K d_{vc}(H_k) $</p><p>c. $ \max\{d_{vc}(H_k) \}_{k=1}^K  \leq d_{vc}({\bigcap _{k=1}}^K H_k) \leq \sum _{k=1} ^K d_{vc}(H_k) $</p><p>d. $ \max\{d_{vc}(H_k) \}_{k=1}^K  \leq d_{vc}({\bigcap _{k=1}}^K H_k) \leq K-1+\sum _{k=1} ^K d_{vc}(H_k) $</p><p>e. $0 \leq d_{vc}({\bigcap _{k=1}}^K H_k) \leq \sum _{k=1} ^K d_{vc}(H_k) $</p><p>è¿™é“é¢˜ç›®ä¸ä¸Šä¸€é“åˆšå¥½ç›¸åã€‚é¦–å…ˆï¼Œå¹¶é›†æ˜¯åŒ…å«æ‰€æœ‰çš„ï¼Œå› æ­¤å®ƒçš„vc dimensionä¸€å®šæ˜¯å¤§äºæœ€å¤§çš„ã€‚æ‰€ä»¥å°±æ’é™¤äº†aï¼Œbï¼Œdã€‚ç„¶åï¼Œå†cä¸dä¹‹é—´åšé€‰æ‹©.æƒ³è±¡ä¸€ä¸ªæƒ…å†µï¼Œ$H_1$æ˜¯å°†æ‰€æœ‰çš„ç‚¹åˆ’åˆ†ä¸ºæ­£ï¼Œ$H_2$æ˜¯å°†æ‰€æœ‰çš„ç‚¹åˆ’åˆ†ä¸ºè´Ÿï¼Œ$H_1+H_2$çš„vc dimensionæ˜¯1ï¼Œä½†æ˜¯å„è‡ªçš„vc dimensionä¸º0.è¿™æ ·è¶³ä»¥é€‰å‡ºè¿™ä¸ªç­”æ¡ˆæ˜¯dã€‚å¦‚ä½•è¯æ˜ï¼Ÿè§‚å¯Ÿä¹‹å‰çš„é‚£ä¸ªè¡¨,å¯ä»¥ä¸¾å‡ºæ›´å¤šçš„ä¾‹å­ã€‚ä½†æ˜¯å¦‚ä½•å¾—åˆ°è¿™ä¸ªå…·ä½“çš„ç•Œé™ï¼Œéœ€è¦æ›´ä¸¥æ ¼çš„æ•°å­¦è¯æ˜ã€‚</p><p>For Questions 16-20, you will play with the decision stump algorithm.</p><p>16-20é¢˜ç›®ä¾ç„¶æ˜¯ç¼–ç¨‹é—®é¢˜ã€‚</p><p><strong>16. In class, we taught about the learning model of â€œpositive and negative raysâ€ (which is simply one-dimensional perceptron) for one-dimensional data. The model contains hypotheses of the form:</strong></p><script type="math/tex; mode=display">h_{s, \theta}(x) = s \cdot \mbox{sign}(x - \theta).</script><p>The model is frequently named the â€œdecision stumpâ€™â€™ model and is one of the simplest learning models. As shown in class, for one-dimensional data, the VC dimension of the decision stump model is 2.</p><p>In fact, the decision stump model is one of the few models that we could easily minimize $E_{in}$ efficiently by enumerating all possible thresholds. In particular, for $N$ examples, there are at most $2N$ dichotomies (see page 22 of lecture 5 slides), and thus at most $2N$ different $E_{in}$ values. We can then easily choose the dichotomy that leads to the lowest $E_{in}$, where ties an be broken by randomly choosing among the lowest $E_{in}$ ones. The chosen dichotomy stands for a combination of some â€œspotâ€ (range of $\theta$) and $s$, and commonly the median of the range is chosen as the $\theta$ that realizes the dichotomy.</p><p>In this problem, you are asked to implement such and algorithm and run your program on an artificial data set. First of all, start by generating a one-dimensional data by the procedure below:</p><p>(a) Generate $x$ by a uniform distribution in $[-1, 1]$.</p><p>(b) Generate $y$ by $f(x) = \tilde{s}(x)$+$noise$ where $ \tilde{s}(x) = sign(x)$ and the noise flips the result with $20%$ probability.</p><p>For any decision stump $h_{s, \theta}$ with $\theta \in [-1, 1]$, express $E_{out}(h_{s, \theta})$ as a function of $\theta$ and $s$.</p><p>a. $0.3+0.5s(|\theta| - 1)$</p><p>b. $0.3+0.5s(1 - |\theta|)$</p><p>c. $0.5+0.3s(|\theta| - 1)$</p><p>d. $0.5+0.3s(1 - |\theta|)$</p><p>e. none of the other choices.</p><p>è™½ç„¶æ˜¯ç¼–ç¨‹é¢˜ç›®ï¼Œä½†æ˜¯æœ¬é“é¢˜ç›®è¿˜æ²¡æœ‰æ¶‰åŠåˆ°ä»£ç ç¼–å†™ï¼Œè€Œæ˜¯ä»ç†è®ºåˆ†æè¿™ä¸ªé—®é¢˜ã€‚æœ¬é¢˜ä¸­æ•°æ®ç”Ÿæˆæ˜¯åˆ©ç”¨$sign(x)+noise$ï¼Œå…¶ä¸­noiseå‡ºç°çš„æ¦‚ç‡æ˜¯20%ã€‚<br>æˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼Œå½“$h_{s,\theta}(x)$åœ¨æ²¡æœ‰å™ªå£°çš„æƒ…å†µä¸‹ï¼Œé”™è¯¯ç‡æ˜¯$\frac \theta 2$.</p><p>ç”±ç¬¬ä¸€é¢˜çš„åˆ†æå¯ä»¥çŸ¥é“ï¼Œ$E_{out} =  \frac {|\theta|} 2 \times (1 - 0.2) + (1 - \frac {|\theta|} 2) \times 0.2 = 0.3 |\theta| + 0.2$, çœ‹äº†ä¸‹ä¼¼ä¹æ²¡æœ‰è¿™ä¸ªç­”æ¡ˆï¼Œè¿™æ˜¯å› ä¸ºæˆ‘ä»¬æ²¡æœ‰è€ƒè™‘åˆ°ç¬¦å·çš„é—®é¢˜ã€‚å¦‚æœè€ƒè™‘åˆ°ç¬¦å·ï¼Œsæ˜¯è´Ÿçš„ï¼Œé‚£ä¹ˆåŸå…ˆçš„æ­£ç¡®ç‡åè€Œå˜æˆé”™è¯¯ç‡äº†, å³ $0.8 - 0.3 |\theta|$å¯ä»¥çœ‹åˆ°ï¼Œç­”æ¡ˆé€‰cã€‚</p><p><strong>17. Generate a data set of size 20 by the procedure above and run the one-dimensional decision stump algorithm on the data set. Record $E_{in}$ and compute $E_{out}$ with the formula above. Repeat the experiment (including data generation, running the decision stump algorithm, and computing $E_{in}$ and $E_{out}$) 5,000 times. What is the average $E_{in}$? Please choose the closest option.</strong></p><p>a. 0.05</p><p>b. 0.15</p><p>c. 0.25</p><p>d. 0.35</p><p>e. 0.45</p><p>è¿™é“é¢˜ç›®éœ€è¦ç¼–ç¨‹å®ç°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆæ•°æ®å’Œå™ªéŸ³ï¼š<br>ä¸‹é¢çš„ä»£ç ç”Ÿæˆ20ä¸ªæ•°æ®ï¼Œå¹¶ç”¨0.2çš„æ¦‚ç‡æŠ½å‡ºæ¥ä½œä¸ºå™ªéŸ³ã€‚</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sign</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x &lt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    <span class="keyword">else</span> : <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateXY</span><span class="params">()</span>:</span></span><br><span class="line">    x = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span>  range(<span class="number">0</span>,<span class="number">20</span>):</span><br><span class="line">        x.append([random.random()*<span class="number">2</span><span class="number">-1</span>])</span><br><span class="line">    noise = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">20</span>):</span><br><span class="line">        ran = random.random()</span><br><span class="line">        <span class="comment">#print(ran)</span></span><br><span class="line">        <span class="keyword">if</span> ran &lt;= <span class="number">0.2</span>:</span><br><span class="line">            noise+=<span class="number">1</span></span><br><span class="line">            x[i].append(-sign(x[i][<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">else</span> :x[i].append(sign(x[i][<span class="number">0</span>]))</span><br><span class="line">    <span class="comment">#print("noise:",noise)</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>ç„¶åå°±æ˜¯å®ç°ç®—æ³•äº†ã€‚è¿™ä¸ªç®—æ³•å¾ˆç®€å•ï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆè½»æ˜“å¾—æšä¸¾å‡ºæ¥å„ç§è¿‡ç¨‹ã€‚åŒæ—¶ä¸ºäº†ç®€åŒ–ç®—æ³•ï¼Œæˆ‘æ²¡æœ‰å®ç°sä¸ºè´Ÿçš„åœºæ™¯ï¼Œå› ä¸ºä¸ºè´Ÿçš„åœºæ™¯æœ€åå¤§æ¦‚ç‡æ˜¯é€‰ä¸åˆ°çš„ã€‚</p><p>é¦–å…ˆï¼Œå°†éšæœºæ•°æ®æ’åºï¼Œç„¶åæ¯æ¬¡é€‰æ‹©ä¸€ä¸ªé—´éš”ï¼Œç»Ÿè®¡å…¶ä¹‹å‰ä¸ä¹‹åé”™è¯¯çš„åˆ†ç±»ä¸ªæ•°ã€‚é€‰æ‹©é—´éš”çš„æ—¶å€™ï¼Œé¦–å…ˆé€‰å–d[i]ï¼Œæ„å‘³ç€ç°åœ¨é€‰æ‹©çš„åŒºåŸŸæ˜¯(d[i-1],d[i])ï¼Œå°†d[i]ä¹‹å‰çš„ä½œä¸º-1ï¼Œd[i]ä¹‹ååŒ…æ‹¬d[i]çš„ä½œä¸º+1ï¼Œè¿™æ ·å¯ä»¥ç®€åŒ–ç®—æ³•ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯iå°†ä¼šç­‰äºlen(d)ï¼Œå› ä¸ºé—´éš”æœ‰len(d)+1ä¸ªã€‚</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decision_stump</span><span class="params">(dataset)</span>:</span></span><br><span class="line"></span><br><span class="line">    sort_d = sorted(dataset)</span><br><span class="line">    min_pos = []</span><br><span class="line"></span><br><span class="line">    err = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    min_err = len(dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(dataset)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>,i):</span><br><span class="line">            <span class="keyword">if</span> sort_d[k][<span class="number">1</span>]&gt;<span class="number">0</span>:</span><br><span class="line">                err+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(i,len(dataset)):</span><br><span class="line">            <span class="keyword">if</span> sort_d[k][<span class="number">1</span>]&lt;<span class="number">0</span>:</span><br><span class="line">                err+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> err &lt; min_err:</span><br><span class="line">            min_pos = []</span><br><span class="line">            min_pos.append(i)</span><br><span class="line">            min_err = err</span><br><span class="line">        <span class="keyword">elif</span> err == min_err:</span><br><span class="line">            min_pos.append(i)</span><br><span class="line">        err = <span class="number">0</span></span><br><span class="line"><span class="comment"># choose the lowest Ein randomly</span></span><br><span class="line">    choosen = int(len(min_pos)*random.random())</span><br><span class="line">    <span class="keyword">if</span> min_pos[choosen] &lt; len(sort_d):</span><br><span class="line">        <span class="keyword">return</span> [sort_d[min_pos[choosen]][<span class="number">0</span>],min_err]</span><br><span class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> [(sort_d[min_pos[choosen]<span class="number">-1</span>][<span class="number">0</span>]+<span class="number">1</span>)/<span class="number">2</span>,min_err]</span><br></pre></td></tr></table></figure><p>ç»“æœï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">average Ein: 0.1713600000000006</span><br></pre></td></tr></table></figure></p><p>å› æ­¤ç­”æ¡ˆé€‰bã€‚</p><p><strong>18. Continuing from the previous question, what is the average E_{out}? Please choose the closest option.</strong></p><p>a. 0.05</p><p>b. 0.15</p><p>c. 0.25</p><p>d. 0.35</p><p>e. 0.45</p><p>å¯¹äºEoutçš„è®¡ç®—ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨16ä¸­çš„å…¬å¼å¸¦å…¥ã€‚ç»“æœå¦‚ä¸‹ï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">average Eout: 0.25962811866336116</span><br></pre></td></tr></table></figure></p><p>å› æ­¤ç­”æ¡ˆé€‰C.</p><p><strong>19. Decision stumps can also work for multi-dimensional data. In particular, each decision stump now deals with a specific dimension $i$, as shown below.</strong></p><script type="math/tex; mode=display">h_{s, i, \theta}(\mathbf{x}) = s \cdot \mbox{sign}(x_i - \theta).</script><p>Implement the following decision stump algorithm for multi-dimensional data:</p><p>a) for each dimension $i = 1, 2, \cdots, d$, find the best decision stump $h_{s, i, \theta}$ using the one-dimensional decision stump algorithm that you have just implemented.</p><p>b) return the â€œbest of bestâ€ decision stump in terms of $E_{in}$. If there is a tie , please randomly choose among the lowest-$E_{in}$ ones.</p><p>The training data $\mathcal{D}_{train}$ is available at:</p><p><a href="https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_math/hw2_train.dat" target="_blank" rel="noopener">https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_math/hw2_train.dat</a></p><p>The testing data $\mathcal{D}_{test}$ is available at:</p><p><a href="https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_math/hw2_test.dat" target="_blank" rel="noopener">https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_math/hw2_test.dat</a></p><p>Run the algorithm on the $\mathcal{D}_{train}$. Report the $E_{\text{in}}$â€‹ of the optimal decision stump returned by your program. Choose the closest option.</p><p>åœ¨æœ¬ä¾‹ä¸­ï¼Œæ˜¯å°†ä¹‹å‰çš„ç®—æ³•ç”¨åˆ°å¤šç»´åº¦çš„æ•°æ®ä¸Šï¼Œåˆ†ä¸¤æ­¥ï¼š1.å¯¹æ¯ä¸ªç»´åº¦çš„æ•°æ®è¿ç”¨ä¸Šé¢çš„ç®—æ³•é€‰å‡ºæœ€ä½³çš„$E_in$;2.åœ¨æ‰€æœ‰çš„ç»´åº¦ä¸­é€‰æ‹©ä¸€ä¸ªæœ€å¥½çš„å‡ºæ¥ã€‚</p><p>è¿™ä¸ªå¯¹åº”åˆ°å®é™…ä¸­å¯èƒ½ä¼šå‡ºç°ï¼Œæ¯”å¦‚æŸä¸ªç»´åº¦æ˜¯çœŸæ­£èµ·ä½œç”¨çš„ï¼Œè€Œå…¶ä½™çš„ç‰¹å¾çš„ä½œç”¨ä¸å¤§ã€‚</p><p>å®é™…ä¸Šç”¨åˆ°çš„ç®—æ³•ä¸ä¹‹å‰çš„ä¸€è‡´ã€‚ä½†æ˜¯éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå› ä¸ºè¿™æ¬¡æˆ‘ä»¬å¯¹çœŸå®çš„$\theta,s$å€¼ä¸€æ— æ‰€çŸ¥ï¼Œå› ä¸ºä¸èƒ½å¿½ç•¥sä¸ºè´Ÿçš„æƒ…å†µã€‚æ”¹è¿›ç®—æ³•çš„æ­¥éª¤å¾ˆç®€å•ï¼Œå› ä¸ºsä¸ºè´Ÿçš„æƒ…å†µå‡ºé”™çš„ä¸ªæ•°å°±æ˜¯æ‰€æœ‰æ ·æœ¬ä¸ªæ•°å‡å»sä¸ºæ­£çš„æƒ…å†µå‡ºé”™çš„ä¸ªæ•°ã€‚</p><p>æ”¹æ­£åçš„ç®—æ³•ï¼š<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decision_stump</span><span class="params">(dataset)</span>:</span></span><br><span class="line"></span><br><span class="line">    sort_d = sorted(dataset)</span><br><span class="line">    min_pos = []</span><br><span class="line"></span><br><span class="line">    err = <span class="number">0</span></span><br><span class="line">    isNeg = <span class="keyword">False</span></span><br><span class="line">    min_err = len(dataset)</span><br><span class="line">    size = len(dataset)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(dataset)+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>,i):</span><br><span class="line">            <span class="keyword">if</span> sort_d[k][<span class="number">1</span>]&gt;<span class="number">0</span>:</span><br><span class="line">                err+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(i,len(dataset)):</span><br><span class="line">            <span class="keyword">if</span> sort_d[k][<span class="number">1</span>]&lt;<span class="number">0</span>:</span><br><span class="line">                err+=<span class="number">1</span></span><br><span class="line">        isNeg = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">if</span> err &lt; min_err:</span><br><span class="line">            min_pos = []</span><br><span class="line">            min_pos.append([i,isNeg])</span><br><span class="line">            min_err = err</span><br><span class="line">        <span class="keyword">elif</span> err == min_err:</span><br><span class="line">            min_pos.append([i,isNeg])</span><br><span class="line">        isNeg = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">if</span> (size - err) &lt; min_err:</span><br><span class="line">            min_pos = []</span><br><span class="line">            min_pos.append([i,isNeg])</span><br><span class="line">            min_err = size - err</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> (size - err) == min_err:</span><br><span class="line">            min_pos.append([i,isNeg])</span><br><span class="line">        err = <span class="number">0</span></span><br><span class="line"><span class="comment"># choose the lowest Ein randomly</span></span><br><span class="line">    <span class="comment">#print(min_pos)</span></span><br><span class="line">    choosen = int(len(min_pos)*random.random())</span><br><span class="line">    <span class="keyword">if</span> min_pos[choosen][<span class="number">0</span>] &lt; len(sort_d):</span><br><span class="line">        <span class="keyword">return</span> [sort_d[min_pos[choosen][<span class="number">0</span>]][<span class="number">0</span>],min_err,min_pos[choosen][<span class="number">1</span>]]</span><br><span class="line">    <span class="keyword">else</span>: <span class="keyword">return</span> [(sort_d[min_pos[choosen][<span class="number">0</span>]<span class="number">-1</span>][<span class="number">0</span>]+<span class="number">1</span>)/<span class="number">2</span>,min_err,min_pos[choosen][<span class="number">1</span>]]</span><br></pre></td></tr></table></figure></p><p>æˆ‘ä»¬å¢æ·»äº†ä¸€ä¸ªisNegçš„å˜é‡ï¼Œæ¥ä»£è¡¨sæ˜¯å¦æ˜¯-1.</p><p>æœ€åmultiç®—æ³•å°±æ˜¯åœ¨ä¸åŒç»´åº¦ä¸Šè¿è¡Œè¯¥ç®—æ³•ï¼ŒæŒ‘å‡ºé”™è¯¯æœ€å°çš„ç»´åº¦ä¸$\theta$ã€‚<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiDDecision_stump</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    min_err_d = []</span><br><span class="line">    min_err = <span class="number">0x7fffffff</span></span><br><span class="line">    err = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(dataset)):<span class="comment">#</span></span><br><span class="line">        temp = decision_stump(dataset[i])</span><br><span class="line">        err = temp[<span class="number">1</span>]</span><br><span class="line">        <span class="comment">#print(err)</span></span><br><span class="line">        <span class="keyword">if</span> err &lt; min_err:</span><br><span class="line">            min_err = err</span><br><span class="line">            min_err_d = []</span><br><span class="line">            min_err_d.append([temp[<span class="number">0</span>],i,min_err,temp[<span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> err == min_err:</span><br><span class="line">            min_err_d.append([temp[<span class="number">0</span>],i,min_err,temp[<span class="number">2</span>]])</span><br><span class="line">    choosen = int(random.random()*len(min_err_d))</span><br><span class="line">    <span class="keyword">return</span> min_err_d[choosen]</span><br></pre></td></tr></table></figure></p><p>è¿™é“é¢˜ç›®ç”¨åˆ°çš„æ•°æ®æ˜¯è¯¾ç¨‹æä¾›çš„ï¼Œå› æ­¤å†™å…¥è¯»å–æ•°æ®çš„è¿‡ç¨‹ï¼š</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readDataFrom</span><span class="params">(filename)</span>:</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">with</span> open (filename) <span class="keyword">as</span> f:</span><br><span class="line">        line = f.readline()[<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">while</span> line:</span><br><span class="line">            temp = line.split(<span class="string">' '</span>)</span><br><span class="line">            <span class="comment">#print(temp)</span></span><br><span class="line">            <span class="keyword">if</span> len(result) == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">for</span> x_i <span class="keyword">in</span> range(len(temp)<span class="number">-1</span>):</span><br><span class="line">                    result.append([[float(temp[x_i]),float(temp[<span class="number">-1</span>])]])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> x_i <span class="keyword">in</span> range(len(temp) - <span class="number">1</span>):</span><br><span class="line">                    result[x_i].append([float(temp[x_i]),float(temp[<span class="number">-1</span>])])</span><br><span class="line">            line = f.readline()[<span class="number">1</span>:<span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p>æœ€åå¾—åˆ°ç»“æœï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dimension: 3</span><br><span class="line">theta: 1.774</span><br><span class="line">Ein: 0.25</span><br></pre></td></tr></table></figure></p><p><strong>20. Use the returned decision stump to predict the label of each example within $\mathcal{D}_{test}$. Report an estimate of $E_{\text{out}}$ by $E_{\text{test}}$. Please choose the closest option.</strong></p><p>ä½¿ç”¨é¢˜ç›®ç»™çš„æ•°æ®æ¥åšæµ‹è¯•ï¼Œä¼°è®¡$E_{out}$ï¼Œéœ€è¦ä¸€ä¸ªæ£€æµ‹é”™è¯¯çš„å‡½æ•°ï¼š<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkout</span><span class="params">(min_err_d,dataset)</span>:</span></span><br><span class="line">    err = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> dataset[min_err_d[<span class="number">1</span>]]:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> sign(i[<span class="number">0</span>] - min_err_d[<span class="number">0</span>]) != sign(i[<span class="number">1</span>]):</span><br><span class="line">            err += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> min_err_d[<span class="number">3</span>] == <span class="keyword">True</span>:</span><br><span class="line">        err =  len(dataset[<span class="number">0</span>]) - err</span><br><span class="line">    <span class="keyword">return</span> err</span><br></pre></td></tr></table></figure></p><p>æœ€åç»“æœï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Eout: 0.36</span><br></pre></td></tr></table></figure></p><p>p.s. 10ï¼Œ11ï¼Œ15é¢˜ç›®ç•™æœ‰ç–‘é—®ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> homework </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”Noise and Error</title>
      <link href="/2018/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Noise-and-Error/"/>
      <url>/2018/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Noise-and-Error/</url>
      
        <content type="html"><![CDATA[<p>ä¸Šæ¬¡çš„åšå®¢ä»‹ç»äº†VC boundï¼Œç”¨çš„æ˜¯äºŒå…ƒåˆ†ç±»æ¥è¯æ˜ã€‚å®é™…ä¸Šæ¨å¹¿åˆ°å…¶ä»–çš„çº¿æ€§å›å½’ç­‰é—®é¢˜ï¼Œæˆ‘ä»¬åªéœ€è¦ä¿®æ”¹ä¸€äº›VC boundé‡Œç›¸å…³çš„å®šä¹‰ï¼Œæœ€ç»ˆä¸€æ ·å¯ä»¥å¾—åˆ°ç±»ä¼¼çš„ç»“æœã€‚<br><a id="more"></a><br>åœ¨æœ‰noiseçš„æƒ…å†µä¸‹ï¼ŒVC boundä¹Ÿæ˜¯æˆç«‹çš„ï¼Œå­¦ä¹ ä¸€æ ·æ˜¯å¯è¡Œçš„ã€‚</p><p>æœ¬ç¯‡åšå®¢å°‘äº†å¯¹ä¸Šé¢è¿™äº›è®ºç‚¹çš„è¯æ˜ï¼Œå› ä¸ºæˆ‘ä¹Ÿä¸ä¼šã€‚</p><p>è¿™é‡Œä¸»è¦ä»‹ç»çš„æ˜¯ä»¥äº›error measureçš„æ–¹æ³•ã€‚ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬æƒ³æ‰¾åˆ°ä¸€ä¸ªå®é™…ä¸Š$E_{in}$æœ€å°çš„è§£æ˜¯ä¸€ä¸ªNP-hardé—®é¢˜ï¼Œå› æ­¤åªèƒ½å°½å¯èƒ½å»æ‰¾åˆ°è¾ƒå¥½çš„è§£;å¦ä¸€æ–¹é¢ï¼Œå¯¹ä¸åŒçš„åº”ç”¨æƒ…æ™¯ï¼Œå¯ä»¥å®šä¹‰ä¸åŒçš„$E_{in}(h)$ã€‚ç”¨ä¸åŒçš„å®šä¹‰æ¥è¡¡é‡é”™è¯¯ã€‚</p><p>æˆ‘ä»¬ä¹‹å‰çš„è¡¡é‡gè¡¨ç°æ—¶å€™æœ‰3ä¸ªç‰¹å¾ï¼š</p><ol><li>out of sampleï¼ˆé€šè¿‡å¯¹æœªè§è¿‡çš„æ•°æ®çš„é¢„æµ‹è¿›è¡Œè¡¡é‡ï¼‰</li><li>point wiseï¼ˆé€ç‚¹è¡¡é‡ï¼‰</li><li>classificationï¼ˆäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼‰</li></ol><p>æ¥ç€ä¸Šé¢ï¼Œæˆ‘ä»¬å·²ç»çŸ¥é“äºŒå…ƒåˆ†ç±»æœ‰ä¸€ä¸ªè¡¡é‡æ–¹æ³•ï¼Œå¦‚ä¸‹:</p><p>$E_{out}(g) = \epsilon _{x~P}[g(x) \neq f(x)]$</p><p>å®é™…ä¸Šä¹Ÿå°±æ˜¯ç»Ÿè®¡é¢„æµ‹é”™è¯¯çš„ä¸ªæ•°ã€‚</p><p>åœ¨ä»¥åçš„å­¦ä¹ ä¸­æˆ‘ä»¬è¿˜æ˜¯ä¼šä½¿ç”¨point wiseè¿™ä¸ªç­–ç•¥ï¼Œæ¯ä¸ªç‚¹æ¯ä¸ªç‚¹çš„æ¥è¿›è¡Œè®¡ç®—ã€‚æˆ‘ä»¬å°†è¡¡é‡æ¯ä¸ªç‚¹çš„é”™è¯¯çš„åŠæ³•è®°ä¸º$err(yâ€™,y)$ï¼Œé‚£ä¹ˆä¸Šè¿°è¡¡é‡åŠæ³•å°±æ˜¯$err(yâ€™,y) = [yâ€™ \neq y]$<br>å¦å¤–ä¸€ç§è¡¡é‡é”™è¯¯çš„æ–¹æ³•ï¼š</p><p>$err(yâ€™,y) = (y - yâ€™)^2$</p><p>è¿™ä¸ªè¡¡é‡é”™è¯¯çš„åŠæ³•é€‚ç”¨äºçº¿æ€§å›å½’ï¼Œå› ä¸ºå®ƒå¾—åˆ°çš„yâ€™æ˜¯å®æ•°ï¼Œå› æ­¤å¯ä»¥å®šä¹‰ä¸çœŸå®å€¼çš„è·ç¦»æ¥è¡¡é‡é”™è¯¯ã€‚</p><p>è¿˜æœ‰å¾ˆå¤šåˆ«çš„å®šä¹‰ï¼Œå¦‚$err(yâ€™,y) = |y - yâ€™|$.</p><p>å¯¹äºä¸åŒçš„è¡¡é‡é”™è¯¯çš„æ–¹æ³•ï¼Œå¾—åˆ°çš„æœ€ä½³çš„å­¦ä¹ ç®—æ³•å¾ˆå¯èƒ½æ˜¯ä¸ä¸€è‡´çš„ã€‚</p><p>åœ¨å®é™…æƒ…å†µä¸­ï¼Œå³ä½¿æ˜¯äºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬ä¹Ÿå¯èƒ½æœ‰ä¸åŒçš„è¡¡é‡é”™è¯¯ç®—æ³•ï¼Œä¸‹é¢ä»‹ç»åŠ æƒåˆ†ç±»ã€‚å› ä¸ºé”™è¯¯çš„æƒ…å†µæœ‰ä¸¤ç§ï¼Œå‡æ­£å’Œå‡è´Ÿï¼Œå®ƒä»¬å¯¹äºå®é™…åº”ç”¨é€ æˆçš„ä»£ä»·å¯èƒ½æ˜¯ä¸ä¸€è‡´çš„ã€‚æ¯”å¦‚ä¸€é—´è¶…å¸‚æä¿ƒé”€ï¼Œå¯¹äºé¢„æµ‹ä¸ºæ­£çš„é¡¾å®¢è®¤ä¸ºæ˜¯å›å¤´å®¢ï¼Œä¼šç»™äºˆæ‰“æŠ˜æ´»åŠ¨ã€‚è¿™æ—¶å€™å‡è´Ÿä¾‹çš„ä»£ä»·æ˜¯å¾ˆå¤§çš„ï¼Œå› ä¸ºå¯èƒ½ä¼šæŸå¤±å›å¤´å®¢ï¼Œå†å¦‚æœæ˜¯CIAæƒ…æŠ¥å±€çš„é—¨ç¦ç³»ç»Ÿï¼Œå¯¹äºé¢„æµ‹ä¸ºå·¥ä½œäººå‘˜çš„å‡†è®¸è¿›å…¥ï¼Œå‡æ­£çš„ä»£ä»·ä¼šéå¸¸å¤§ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥å†™å‡ºä¸‹é¢æ ·å­çš„ä¸¤ä¸ªè¡¨æ ¼ï¼Œä»£è¡¨ä¸åŒé”™è¯¯çš„æƒé‡ï¼š</p><div class="table-container"><table><thead><tr><th style="text-align:center">R\P</th><th style="text-align:center">+1</th><th style="text-align:center">-1</th><th style="text-align:center"></th><th style="text-align:center">R\P</th><th style="text-align:center">+1</th><th style="text-align:center">-1</th></tr></thead><tbody><tr><td style="text-align:center">+1</td><td style="text-align:center">0</td><td style="text-align:center">1000</td><td style="text-align:center"></td><td style="text-align:center">+1</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">-1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center"></td><td style="text-align:center">-1</td><td style="text-align:center">10000</td><td style="text-align:center">0</td></tr></tbody></table></div><p>å› æ­¤ï¼Œå¯¹äºåŠ æƒåˆ†ç±»çš„é”™è¯¯è¡¡é‡åŠæ³•ï¼Œå¯ä»¥å†™æˆï¼š</p><script type="math/tex; mode=display">err(y',y) = \frac {(y + 1)(y - y')} 4 a_1 + \frac {(1-y)( y' - y)} 4 a_2</script><p>ä¸Šå¼ä¸­ï¼Œ$a_1$æ˜¯é¢„æµ‹ä¸ºå‡æ­£çš„æƒé‡ï¼Œ$a_2$æ˜¯é¢„æµ‹ä¸ºå‡è´Ÿçš„æƒé‡.</p><p>æˆ‘ä»¬éœ€è¦å°†é”™è¯¯è¡¡é‡æ–¹æ³•åŠ å…¥å­¦ä¹ ç®—æ³•ï¼Œæ‰èƒ½ä½¿å¾—æœ€ç»ˆçš„ç»“æœè®©$E_{in}$å°½é‡å°.</p><p>ä¸¾ä¸ªä¾‹å­ï¼Œå¯¹äºpocketï¼Œå‡å¦‚é‡‡ç”¨ä¸Šé¢å›å¤´å®¢çš„ä¾‹å­ä¸­çš„æƒé‡æ¥è¿›è¡Œçº¦æŸï¼Œé‚£ä¹ˆpocketç®—æ³•ä¸­ï¼Œå‡è´Ÿçš„ä»£ä»·å¾ˆé«˜ï¼Œå½“é‡åˆ°å‡è´Ÿçš„æƒ…å†µæ—¶å€™ï¼Œç­‰ä»·äºå¤åˆ¶äº†1000ä¸ªç›¸åŒçš„ç‚¹ï¼Œæ¯ä¸ªç‚¹æƒé‡ä¸€è‡´ã€‚è¿™è¦æ±‚æˆ‘ä»¬åœ¨å®é™…å†™ç®—æ³•æ—¶å€™ï¼Œä¸å…‰å¯¹äºè¯¥ç‚¹çš„æƒ©ç½šç¿»äº†1000å€ï¼ŒåŒæ—¶è¿˜è¦è®©è¿™ä¸ªç‚¹ä¸‹æ¬¡è¢«é€‰ä¸­çš„æ¦‚ç‡å˜å¤§ã€‚å…¶ä»–ç®—æ³•ä¸­ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œå¦‚æœä¸€ä¸ªæƒ…å†µçš„é”™è¯¯ä»£ä»·å¾ˆå¤§ï¼Œæˆ‘ä»¬ä¸å…‰è¦å¯¹ä»£ä»·å¢åŠ ï¼Œä¹Ÿè¦å°½å¯èƒ½åœ°æ”¹æ­£è¿™ä¸ªé”™è¯¯ã€‚</p><p>æœ€åï¼Œè¦è¯´æ˜é™¤æ­¤ä¹‹å¤–çš„ä¸€ç§æƒ…å†µã€‚æœ‰ä¸€ç§æ•°æ®æ˜¯unbalanced dataï¼Œè¿™æ ·çš„æ•°æ®åŠ ä¸Šäº†æƒé‡ï¼Œä¾ç„¶å¯èƒ½ä¼šç»™ä¸€ä¸ªå¾ˆçƒ‚çš„å­¦ä¹ ç®—æ³•å¾ˆä½çš„é”™è¯¯è¯„ä»·ï¼Œæ¯”å¦‚ciaçš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æœ‰999 990ä¸ªå‘˜å·¥çš„æ ·æœ¬ï¼Œåªæœ‰10ä¸ªå…¥ä¾µè€…çš„æ ·æœ¬ï¼Œé‚£ä¹ˆå³ä½¿å‡æ­£çš„æƒé‡æå‡åˆ°10000ï¼Œå¯¹äºä¸€ä¸ªæ€»æ˜¯é¢„æµ‹æ­£ç¡®çš„ç®—æ³•ï¼Œé”™è¯¯è¡¡é‡ä¾ç„¶åªæœ‰0.1ï¼Œä¼¼ä¹è¿˜ä¸é”™çš„è¯„ä»·ï¼Œè€Œè¿™ä¸ªç®—æ³•ç”šè‡³ç®—ä¸ä¸Šä¸€ä¸ªå­¦ä¹ ç®—æ³•ã€‚è¿™è¯´æ˜è¯„ä»·ç®—æ³•è¿˜æœ‰åˆ«çš„æ–¹é¢éœ€è¦è€ƒè™‘ï¼Œå¦‚ä»¥åå¯èƒ½æåˆ°çš„æŸ¥å‡†ç‡ä¸æŸ¥å…¨ç‡ã€‚</p><p>ä»¥ä¸Šã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”VC bound</title>
      <link href="/2018/08/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94VC-bound/"/>
      <url>/2018/08/11/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94VC-bound/</url>
      
        <content type="html"><![CDATA[<p>ä¸Šæ¬¡çš„Hoeffdingä¸ç­‰å¼é‚£ç¯‡ï¼Œè¯æ˜äº†ä¸€ä¸ªhypothesisé›†åˆæ˜¯æœ‰é™é›†åˆï¼Œé‚£ä¹ˆå­¦ä¹ æ˜¯å¯è¡Œçš„ã€‚<br><a id="more"></a><br>å¦‚æœå®šä¹‰$E_{in}$æ˜¯èµ„æ–™ä¸Šçš„é”™è¯¯ç‡ï¼Œ$E_{out}$æ˜¯æ•´ä½“çš„é”™è¯¯ç‡ï¼Œæˆ‘ä»¬è¯æ˜çš„ç»“æœï¼Œå¦‚æœNè¶³å¤Ÿå¤§ï¼Œé‚£ä¹ˆå¾ˆå¤§æ¦‚ç‡ä¸Šï¼Œ$E_{in} \approx E_{out}$.æˆ‘ä»¬åªéœ€è¦åœ¨æœ‰é™çš„é›†åˆåˆ©ç”¨é‡Œå­¦ä¹ ç®—æ³•é€‰å‡ºä¸€ä¸ª$E_{in}$æœ€ä½çš„ï¼Œå°±å¯ä»¥å®ç°å­¦ä¹ ï¼Œå› ä¸ºå¾ˆå¤§æ¦‚ç‡ä¸Šå®ƒå¯¹æ•´ä½“åˆ†ç±»åçš„é”™è¯¯ç‡ä¹Ÿæ˜¯ä¸$E_{in}$å·®ä¸å¤ªå¤šçš„ã€‚</p><p>å…ˆæ€è€ƒä¸€ä¸ªé—®é¢˜ï¼ŒHçš„å¤§å°å½±å“çš„äº†ä»€ä¹ˆï¼Ÿå­¦ä¹ éœ€è¦åšçš„æœ‰ä¸¤ä¸ªï¼š1. ä¿è¯$E_{in} \approx E_{out}$ 2.æ‰¾åˆ°ä¸€ä¸ªhä½¿å¾—$E_{in}å¾ˆå°$ã€‚<br>å¦‚æœHé›†åˆè¿‡å¤§ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¸å®¹æ˜“ä¿è¯ç¬¬ä¸€ä¸ªæ¡ä»¶ï¼Œä½†æ˜¯å¦‚æœé›†åˆè¿‡å°ï¼Œæˆ‘ä»¬ä¸ä¸€å®šèƒ½æ‰¾åˆ°ä¸€ä¸ªhä½¿å¾—å®ƒç”šè‡³åœ¨æµ‹è¯•æ•°æ®ä¸Šæœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚</p><p>ä¸Šæ¬¡åšå®¢ç•™ä¸‹æ¥äº†ä¸€ä¸ªé—®é¢˜ï¼šå¦‚æœè¿™ä¸ª$H$é›†åˆæ˜¯æ— é™é›†åˆå‘¢ï¼Ÿä¾‹å¦‚ä¹‹å‰å®ç°çš„PLAç®—æ³•ã€‚é‚£æˆ‘ä»¬æ€ä¹ˆä¿è¯åœ¨æ— é™çš„é›†åˆä¸Šï¼Œå­¦ä¹ æ˜¯å¯è¡Œçš„å‘¢ï¼Ÿ</p><p>é¦–å…ˆï¼Œæˆ‘ä»¬æ¥è§‚å¯Ÿä¸Šæ¬¡å¾—åˆ°çš„hoeffdingä¸ç­‰å¼ï¼š$P_{baddata} \leq 2te^{-2\epsilon ^2N}.å¦‚æœå…¶ä¸­t-&gt;$\lnfty$ï¼Œé‚£ä¹ˆè¿™ä¸ªä¸ç­‰å¼å®é™…ä¸Šæ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼Œå› ä¸ºå³è¾¹çš„å€¼å°†ä¼šè¿œå¤§äº1ï¼Œä½†æ˜¯è¯´ä¸€ä¸ªæ¦‚ç‡å°äºç­‰äº1é‚£æ˜¯åºŸè¯ã€‚</p><p>ä»”ç»†æƒ³æƒ³ï¼Œé‚£æ˜¯å› ä¸ºæˆ‘ä»¬çš„union boundå¤ªå®½æ¾äº†ã€‚å®ƒä»¬å®é™…ä¸Šä¼šæœ‰å¾ˆå¤šé‡å çš„éƒ¨åˆ†ï¼Œæ¯”å¦‚å¯¹äºæŸä¸ªhypothesisæ˜¯bad dataï¼Œå¯¹äºå¦ä¸€ä¸ªå®ƒå¯èƒ½ä¹Ÿæ˜¯ã€‚è¿™å°±è¦æ±‚æˆ‘ä»¬å°†è¿™ä¸ªunion boundç»§ç»­å‹ç¼©ã€‚</p><p>åˆ©ç”¨2Dçš„perceptron learning algorithmæ¥ä¸¾ä¾‹ï¼Œå¦‚æœN = 1ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œé‚£ä¹ˆå®ƒè¦ä¹ˆæ˜¯æ­£è¦ä¹ˆæ˜¯è´Ÿï¼Œè™½ç„¶å¹³é¢ä¸Šæœ‰æ— æ•°æ¡çº¿ï¼Œä½†æ˜¯ä¼¼ä¹åªæœ‰è¿™ä¹ˆä¸¤ä¸ªæ•ˆæœï¼Œä¹Ÿå°±æ˜¯åªæœ‰è¿™ä¹ˆä¸¤ç±»çº¿ï¼Œåœ¨è¿™ä¸¤ç±»çº¿ä¸Šï¼Œå®ƒä»¬çš„$E_{in}$æ˜¯ä¸€è‡´çš„ã€‚</p><p>åŒæ ·çš„é“ç†ï¼Œå¦‚æœå¹³é¢ä¸Šæœ‰ä¸¤ä¸ªç‚¹ï¼Œæˆ‘ä»¬åˆ©ç”¨å¹³é¢ä¸Šçš„ç›´çº¿æœ€å¤šä¹Ÿå°±åªèƒ½åˆ†æˆ4ç§æƒ…å†µ,æˆ‘ä»¬å°†æ¯ä¸€ç§æƒ…å†µç§°ä¸ºä¸€ä¸ªdichotomyã€‚</p><p>å½“Nä¸º3çš„æ—¶å€™ï¼Œåœ¨çº¸ä¸Šæˆ‘ä»¬å¯ä»¥ç”»å‡ºï¼Œå¹³é¢ä¸Šå¯ä»¥æœ‰8ç§dichotomyï¼Œä½†æ˜¯ä¹Ÿä¼šæœ‰æ„å¤–ï¼Œä¾‹å¦‚å¦‚æœ3ä¸ªç‚¹æ‹æˆä¸€æ¡ç›´çº¿ï¼Œé‚£ä¹ˆâ€œÃ— â—‹ Ã—â€çš„æƒ…å†µï¼Œæˆ‘ä»¬ä¼¼ä¹æ— æ³•ç”¨ä¸€æ¡ç›´çº¿åˆ†å¼€äº†ã€‚</p><p>å½“Nä¸º4çš„æ—¶å€™ï¼Œå³ä½¿4ä¸ªç‚¹æ˜¯æ¯ä¸€ä¸ªç‚¹éƒ½æ˜¯å‡¸å››è¾¹å½¢çš„é¡¶ç‚¹ï¼Œæˆ‘ä»¬ä¾ç„¶æ— æ³•å°†æ‰€æœ‰çš„æƒ…å†µéƒ½è¡¨ç¤ºå‡ºæ¥ï¼Œå¦‚ä¸‹é¢è¿™ç§æƒ…å†µï¼š</p><p>Ã— â—‹</p><p>â—‹ Ã—</p><p>å®é™…ä¸Šï¼Œå½“Nä¸º4çš„æ—¶å€™ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†å‡ºæ¥çš„dichotomyå…±æœ‰14ç§ã€‚è€Œæ‰€æœ‰çš„æƒ…å†µæœ‰$2^4=16$ç§ï¼Œå¾ˆæ˜æ˜¾å¯ä»¥çœ‹å‡ºdichotomyçš„æ•°é‡æ˜¯å°‘äº$2^N$ã€‚</p><p>æˆ‘ä»¬å°†æŸä¸ªå¤§å°ä¸ºNçš„datasetæ‰€æœ‰æƒ…å†µéƒ½å¯ä»¥ç”¨è¿™ä¸ªHåšå‡ºæ¥(dichotomyçš„æ•°é‡ä¸º$2^N$)ï¼Œæˆä¸ºè¢«H shatterã€‚</p><p>å½“N&gt;4çš„æ—¶å€™ï¼Œè¿™ä¸ªdichotomyåˆæœ‰å¤šå°‘ï¼Ÿç°åœ¨æˆ‘ä»¬å¾ˆéš¾æ‰¾åˆ°2d perceptronå…¶ä¸­è¿™ä¸ªè§„å¾‹ã€‚å¹¸è¿çš„æ˜¯æœ€åæˆ‘ä»¬ä¹Ÿä¸éœ€è¦å…³æ³¨å®ƒå…·ä½“æ˜¯å¤šå°‘ã€‚</p><p>åœ¨è¿™é‡Œæˆ‘ä»¬è€ƒè™‘å‡ ç§ä¸åŒçš„ç®€å•çš„Hï¼Œæ¥æ›´åŠ ç†Ÿæ‚‰è¿™ä¸ªæ¦‚å¿µï¼š</p><ol><li>Positive Ray</li></ol><p>æ ·æœ¬ä¸º1ç»´çš„ç‚¹ï¼Œè¿™ä¸ªhypothesis setæ˜¯åœ¨ç›´çº¿ä¸Šæ‰€æœ‰çš„éæ ·æœ¬ç‚¹ï¼Œé€‰å–ä¸€ä¸ªç‚¹ï¼Œè¯¥ç‚¹åæ ‡ä¹‹å‰çš„ä¸ºpositiveï¼Œä¹‹åçš„ä¸ºnegativeã€‚å®¹æ˜“çœ‹å‡ºæ¥ï¼Œå½“æ ·æœ¬ä¸ªæ•°ä¸ºNæ—¶å€™ï¼Œæœ€å¤šå¯ä»¥æœ‰N+1ä¸ªdichotomyï¼ˆNä¸ªç‚¹å°†è¯¥è½´åˆ†ä¸ºN+1ä¸ªéƒ¨åˆ†ï¼Œæ¯ä¸ªéƒ¨åˆ†çš„ç‚¹æ˜¯ä¸€ç±»ï¼‰ã€‚</p><ol><li>Positive Intervals</li></ol><p>æ ·æœ¬ä¾ç„¶æ˜¯1ç»´çš„ç‚¹ï¼Œè¿™ä¸ªhypothesis setæ˜¯é€‰å–ä¸€ä¸ªèŒƒå›´ï¼ŒèŒƒå›´å†…çš„ä¸ºpositiveï¼ŒèŒƒå›´ä¹‹å¤–çš„ä¸ºnegativeã€‚å½“æ ·æœ¬ä¸ªæ•°ä¸ºNçš„æ—¶å€™ï¼Œæœ€å¤šå¯ä»¥æœ‰$\frac {(N+1)N} {2}+1$ä¸ªdichotomyï¼ˆNä¸ªç‚¹å°†è¯¥è½´åˆ†ä¸ºN+1ä¸ªéƒ¨åˆ†ï¼Œä»N+1ä¸ªéƒ¨åˆ†ä¸­ä»»ä¸¤ä¸ªå–ä¸€ä¸ªç‚¹å³å¯ï¼Œä½†æ˜¯è¿™æ ·è¿˜ç¼ºä¸€ç§ï¼Œå°±æ˜¯å…¨æ˜¯positiveçš„æƒ…å†µï¼Œæˆ‘ä»¬ä¾ç„¶å¯ä»¥åšåˆ°å°†è¿™ä¸ªæƒ…å†µï¼Œåªéœ€è¦å°†ä¸¤æ¬¡çš„ç‚¹é€‰åœ¨åŒä¸€ä¸ªéƒ¨åˆ†å³å¯ï¼‰ã€‚</p><ol><li>Convex Sets</li></ol><p>æ ·æœ¬æ˜¯äºŒç»´çš„ç‚¹ï¼Œå¹¶ä¸”æ˜¯å‡¸Nè¾¹å½¢çš„é¡¶ç‚¹ã€‚é€‰å–ä¸€ä¸ªå‡¸å¤šè¾¹å½¢çš„èŒƒå›´ï¼Œä½¿å¾—å¤šè¾¹å½¢å†…éƒ¨ä¸ºpositiveï¼Œå¤–éƒ¨ä¸ºnegativeã€‚å¯ä»¥çœ‹åˆ°ä»»ä½•æ—¶å€™è¿™ä¸ªdatasetéƒ½å¯ä»¥è¢«H shatterï¼Œæ‰€ä»¥å®ƒçš„dichotomyä¸ªæ•°æ˜¯$2^N$.</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/SEKLYM%60EURGS%40%5D4F%247%5B347X.png" alt="1"></p><ol><li>1D perceptronï¼ˆpositive/negative rayï¼‰</li></ol><p>ä¸1ç±»ä¼¼ï¼Œé™¤äº†æœ€ç«¯ç‚¹çš„ä¸¤ä¸ªéƒ¨åˆ†ï¼Œå…¶ä»–çš„åˆ†å‰²ä¹‹åéƒ½æœ‰ä¸ªä¸ä¹‹å¯¹ç«‹çš„dichotomyï¼Œè€Œç«¯ç‚¹çš„éƒ¨åˆ†å¾—åˆ°çš„æ˜¯å…¨pæˆ–è€…å…¨nï¼Œæ‰€æœ‰æ˜¯$2(N+1-2)+2 = 2N$.</p><p>è€Œè¿™ä¸ª2Nï¼ŒN+1ç­‰ç­‰ï¼Œæˆ‘ä»¬ä¹˜å…¶ä¸ºæˆé•¿å‡½æ•°ã€‚å‡è®¾æˆ‘ä»¬å¸Œæœ›ç”¨$m_H$æ¥ä»£æ›¿ä¹˜è¿›å»çš„é‚£ä¸ªé›†åˆçš„å¤§å°ï¼Œç”¨$m_H(N)$æ¥è¡¨ç¤ºæˆé•¿å‡½æ•°ï¼Œä¾‹å¦‚ï¼šå¯¹äºpositive rayæ¥è¯´ï¼Œ$m_H(N) = N+1$ã€‚</p><h2 id="Break-Point"><a href="#Break-Point" class="headerlink" title="Break Point"></a>Break Point</h2><p>æˆ‘ä»¬å¼•å…¥ä¸€ä¸ªæ–°çš„å®šä¹‰ï¼Œå«åšBreak Pointï¼Œå®ƒè¡¨ç¤ºç¬¬ä¸€ä¸ªæ‰€æœ‰æƒ…å†µä¸‹éƒ½ä¸èƒ½è¢«shatterçš„æ ·æœ¬ä¸ªæ•°ã€‚æˆ‘ä»¬å°†break pointç®€å†™ä¸ºkï¼Œä¸¾ä¸ªä¾‹å­ï¼Œpositive rayçš„k = 2ï¼Œå› ä¸º$2+1!=2^2$ï¼ŒåŒæ ·çš„é“ç†ï¼Œpositive intervalsçš„k = 3ï¼Œ1D perceptronçš„k = 3ï¼Œconvex setsçš„kä¸å­˜åœ¨ã€‚</p><p>å¦‚æœç”¨2D perceptronä¸ºä¾‹ï¼Œä»–çš„k = 4ï¼Œä½†æ˜¯æˆ‘ä»¬å¾ˆéš¾å¾—åˆ°å®ƒçš„æˆé•¿å‡½æ•°ã€‚æˆ‘ä»¬å¸Œæœ›å®ƒçš„æˆé•¿å‡½æ•°å¯ä»¥æ˜¯ä¸€ä¸ªå¤šé¡¹å¼ï¼Œè¿™æ ·éšç€Nçš„å¢åŠ ï¼Œ$E_{in}$ä¸$E_{out}$è¿˜æ˜¯ä¼šå¾ˆå¤§å¯èƒ½ç›¸å·®ä¸å¤šçš„ã€‚</p><p>æ‰¾ä¸åˆ°æˆé•¿å‡½æ•°ï¼Œå¦ä¸€ä¸ªå¸Œæœ›æ˜¯å¯ä»¥æ‰¾åˆ°æˆé•¿å‡½æ•°çš„ä¸Šé™ã€‚æ¯”å¦‚ï¼Œåœ¨k = 4çš„æƒ…å†µä¸‹ï¼ŒNä¸ªæ ·æœ¬æœ€å¤šèƒ½äº§ç”Ÿå‡ ä¸ªdichotomyï¼Ÿæˆ‘ä»¬å°†è¿™ä¸ªç®€å†™ä¸ºB(N,k).<br>k = 4ï¼Œæ„å‘³ç€ä»»æ„3ä¸ªæ ·æœ¬éƒ½ä¸èƒ½è¢«shatterã€‚æˆ‘ä»¬è¯•å›¾å»å¡«å†™ä¸‹é¢è¿™æ ·çš„ä¸€ä¸ªè¡¨æ ¼ï¼š</p><div class="table-container"><table><thead><tr><th>B(N,k)</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th><th style="text-align:center">5</th><th style="text-align:center">â€¦</th><th>N</th></tr></thead><tbody><tr><td>1</td><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">â€¦</td><td>2</td></tr><tr><td>2</td><td style="text-align:center">1</td><td style="text-align:center">3</td><td style="text-align:center">4</td><td style="text-align:center">4</td><td style="text-align:center">4</td><td style="text-align:center">â€¦</td><td>4</td></tr><tr><td>3</td><td style="text-align:center">1</td><td style="text-align:center"></td><td style="text-align:center">7</td><td style="text-align:center">8</td><td style="text-align:center">8</td><td style="text-align:center">â€¦</td><td>8</td></tr><tr><td>4</td><td style="text-align:center">1</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">15</td><td style="text-align:center">16</td><td style="text-align:center">â€¦</td><td>16</td></tr><tr><td>5</td><td style="text-align:center">1</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">31</td><td style="text-align:center">â€¦</td><td>32</td></tr></tbody></table></div><p>è¡¨æ ¼ä¸­çš„å·²ç»å¡«å†™çš„éƒ¨åˆ†æˆ‘ä»¬å¾ˆå®¹æ˜“å°±çŸ¥é“äº†ï¼Œå¦‚æœN &lt; kï¼Œé‚£ä¹ˆå¯ä»¥shatterï¼Œç­”æ¡ˆå°±æ˜¯$2^N$ï¼Œå¦‚æœN = kï¼Œé‚£ä¹ˆæ°å¥½ä¸èƒ½shatterï¼Œæ‰€ä»¥æœ€å¤šå°±æ˜¯$2^N-1$,æ¥ä¸‹æ¥æˆ‘ä»¬å°è¯•ä¸€ä¸ªç®€å•çš„,N = 3,k = 2çš„æƒ…å†µã€‚æˆ‘ä»¬ä¸€ä¸ªä¸ªåˆ—ä¸¾æ‰€çœ‹åˆ°çš„æƒ…å†µï¼Œå¾ˆå®¹æ˜“å‘ç°æœ€å¤šæœ€å¤šï¼Œå¯ä»¥å†™å‡º4ä¸ªdichotomyï¼Œä»»æ„ä¸¤ä¸ªéƒ½æ²¡æœ‰è¢«shatter,å¦‚ä¸‹ï¼š</p><p>o o o</p><p>o o x</p><p>o x x</p><p>x o o </p><p>æˆ‘ä»¬å†æ·»åŠ ä»»ä½•ä¸€ç§ï¼Œéƒ½ä¼šå¯¼è‡´æœ‰ä¸¤ä¸ªæ ·æœ¬è¢«shatterã€‚</p><p>å°† 4 å¡«å…¥è¡¨ä¸­åï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªæœ‰è¶£çš„è§„å¾‹ï¼Œåœ¨å·²ç»å¡«å¥½çš„æ•°æ®é‡Œï¼Œä»»ä½•ä¸€ä¸ª$B(N,k) = B(N-1,k)+B(N-1,k-1)$ï¼Œä¸çŸ¥é“è¿™æ˜¯å¦æ˜¯ä¸ªå·§åˆï¼Ÿ</p><p>åˆ©ç”¨ç¨‹åº$^{è§p.s1.}$å°†B(4,3)çš„æƒ…å†µè·‘å‡ºæ¥ï¼Œå‘ç°B(4,3)=11:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">11</span><br><span class="line">[0, 0, 0, 0]</span><br><span class="line">[1, 0, 0, 0]</span><br><span class="line">[0, 1, 0, 0]</span><br><span class="line">[1, 1, 0, 0]</span><br><span class="line">[0, 0, 1, 0]</span><br><span class="line">[1, 0, 1, 0]</span><br><span class="line">[0, 1, 1, 0]</span><br><span class="line">[0, 0, 0, 1]</span><br><span class="line">[1, 0, 0, 1]</span><br><span class="line">[0, 1, 0, 1]</span><br><span class="line">[0, 0, 1, 1]</span><br></pre></td></tr></table></figure></p><p>æˆ‘ä»¬å°†0æ ‡ä¸ºnegativeï¼Œ1æ ‡ä¸ºpositiveï¼Œç»è¿‡æ•´ç†å¯ä»¥å¾—åˆ°ä¸‹é¢çš„æ ·å­ï¼š</p><p>2Î±</p><script type="math/tex; mode=display">\begin{Bmatrix}X & X & X & X \\X & X & X & O \\X & X & O & X \\X & X & O & O \\X & O & X & X \\X & O & X & O \\O & X & X & X \\O & X & X & O\end{Bmatrix}</script><p>Î²</p><script type="math/tex; mode=display">\begin{Bmatrix}O & O & X & X \\O & X & O & X \\X & O & O & X \end{Bmatrix}</script><p>é¦–å…ˆï¼Œå‰2Î±ä¸­æ¯ä¸€ç»„ç§æ¯ä¸ªdichotomyå‰3ä¸ªæ˜¯ä¸€è‡´çš„ï¼Œå› æ­¤åªçœ‹å‰3åˆ—ï¼Œ$\alpha + \beta \leq B(3,3)$ï¼Œå†çœ‹å‰Î±ç»„çš„ç¬¬ä¸€è¡Œçš„å‰3ä¸ªï¼Œå®ƒä»¬æ¯ä¸¤ä¸ªå¿…ç„¶ä¸èƒ½shatterï¼Œå¦åˆ™åŠ ä¸Šç¬¬å››åˆ—çš„å°±ä¼šå‡ºç°3ä¸ªæ ·æœ¬è¢«shatterçš„æƒ…å†µï¼Œå› æ­¤$\alpha \leq B(3,2)$.</p><p>æ€»çš„æ¥è¯´å…±æœ‰$2\alpha + \beta$ç§ï¼Œå®ƒæ˜¯å°äºç­‰äºB(3,3)+B(3,2)çš„ã€‚æ¨å¹¿åˆ°æ›´å¤§çš„Nï¼Œè¿™ä¸ªä¹Ÿä¾ç„¶æ˜¯æˆç«‹çš„ï¼Œæˆ‘ç®€å•è¯´æ˜ä¸€ä¸‹å…¶ä¸­çš„é“ç†ï¼š</p><p>B(N-1,k-1)çš„dichotomyæ¯ä¸ªåé¢éƒ½å¢åŠ ä¸€ä¸ªOæˆ–è€…Xï¼Œé‚£ä¹ˆä¸ªæ•°ä¼šç¿»å€ï¼Œè€Œä¸”å¯ä»¥shatterçš„æ ·æœ¬ä¸ªæ•°åŠ ä¸€ï¼Œè¿™å°±æ˜¯B(N,k)çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä½™éƒ¨åˆ†çš„å‰N-1ä¸ªå…ƒç´ ä¸ä¼šå‡ºç°ç›¸åŒçš„æƒ…å†µï¼Œå¦‚æœç›¸åŒï¼Œåˆ™å‰N-1ä¸ªå…ƒç´ ä¸ä¹‹å‰çš„2*B(N-1,k-1)ä¸ªå¿…ç„¶ä¼šæœ‰k-1ä¸ªè¢«shatterï¼ŒåŠ ä¸Šæœ€åçš„ä¸€åˆ—ä¼šæœ‰kä¸ªè¢«shatterï¼Œè¿™ä¸å‰ææ˜¯çŸ›ç›¾çš„ï¼Œè€Œä¸”å‰©ä½™çš„ä¸ªæ•°æ˜¯å°äº$B(N-1,k) - B(N-1,k-1)$çš„ï¼Œä¸ç„¶ä¾ç„¶ä¼šä¸æ¡ä»¶çŸ›ç›¾ã€‚</p><p>å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¡«æ»¡è¿™å¼ è¡¨æ ¼äº†ï¼š</p><div class="table-container"><table><thead><tr><th>B(N,k)</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th><th style="text-align:center">4</th><th style="text-align:center">5</th><th style="text-align:center">â€¦</th><th>N</th></tr></thead><tbody><tr><td>1</td><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">2</td><td style="text-align:center">â€¦</td><td>2</td></tr><tr><td>2</td><td style="text-align:center">1</td><td style="text-align:center">3</td><td style="text-align:center">4</td><td style="text-align:center">4</td><td style="text-align:center">4</td><td style="text-align:center">â€¦</td><td>4</td></tr><tr><td>3</td><td style="text-align:center">1</td><td style="text-align:center">4</td><td style="text-align:center">7</td><td style="text-align:center">8</td><td style="text-align:center">8</td><td style="text-align:center">â€¦</td><td>8</td></tr><tr><td>4</td><td style="text-align:center">1</td><td style="text-align:center">5</td><td style="text-align:center">11</td><td style="text-align:center">15</td><td style="text-align:center">16</td><td style="text-align:center">â€¦</td><td>16</td></tr><tr><td>5</td><td style="text-align:center">1</td><td style="text-align:center">6</td><td style="text-align:center">16</td><td style="text-align:center">26</td><td style="text-align:center">31</td><td style="text-align:center">â€¦</td><td>32</td></tr></tbody></table></div><p>é‚£ä¹ˆB(N,k) = B(N-1,k-1) +B(N-1,k) ,åˆ©ç”¨ä¸Šé¢çš„è¡¨æ ¼ä¸€è·¯ä¸Šå»ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ•°å­¦å½’çº³æ³•è¯æ˜ä¸‹å¼æˆç«‹ï¼š</p><script type="math/tex; mode=display">B(N,k) \leq \sum _{i=0} ^{k-1} C_N^i</script><p>å®é™…ä¸Šç­‰å·ä¹Ÿæ˜¯æˆç«‹çš„ï¼Œä½†æ˜¯è¯æ˜éœ€è¦æ›´åŠ å¤æ‚çš„æ•°å­¦ç†è®ºã€‚</p><p>è€Œ$C_N^i$çš„ä¸Šé™æ˜¯$N^i$ï¼Œé‚£ä¹ˆ$B(N,k)$é¦–é¡¹æœ€é«˜é¡¹å°±æ˜¯$N^{k-1}$ï¼Œè¿™æ˜¯ä¸€ä¸ªå¥½æ¶ˆæ¯ï¼Œå› ä¸ºå®ƒçš„å¢é•¿é€Ÿåº¦ä¸å¤Ÿå¿«ã€‚æ‰€ä»¥$m_H(N)$æˆ‘ä»¬å¯ä»¥ä½¿ç”¨$N^{k-1}$æ¥ä»£æ›¿äº†ï¼ˆå½“$N \leq 2,k \leq 3$æ—¶ï¼‰ã€‚</p><p>ä½†æ˜¯å®ƒèƒ½å¦ç›´æ¥å¸¦å…¥åŸæ¥çš„ä¸ç­‰å¼å‘¢ï¼Ÿè¿˜æ˜¯æœ‰ç‚¹é—®é¢˜ï¼Œå®é™…ä¸Šï¼Œæˆ‘ä»¬æ— æ³•ä¿è¯</p><script type="math/tex; mode=display">P[âˆƒh \ln H s.t. |E_{in}(h) - E_{out}(h)|>\epsilon] \leq 2 m_H(N) e^{-2\epsilon ^2N}</script><p>æˆ‘ä»¬æœ€ç»ˆå¾—åˆ°çš„æ˜¯ä¸‹é¢çš„æ ·å­ï¼š</p><script type="math/tex; mode=display">P[âˆƒh \ln H s.t. |E_{in}(h) - E_{out}(h)|>\epsilon] \leq 2 \cdot 2 m_H(2N) \cdot e^{-2 \cdot \frac 1 {16} \epsilon ^2 N}</script><p>ä¸¥æ ¼çš„è¯æ˜éœ€è¦å¾ˆé«˜çš„æ•°å­¦æŠ€å·§ä»¥åŠæ•°å­¦ç†è®ºï¼Œä½†æ˜¯å¯ä»¥ä»ä»¥ä¸‹3ä¸ªæ–¹å‘ç®€å•è§£é‡Šä¸‹åŸå› ï¼š</p><h4 id="1-finite-E-in-and-infinite-E-out"><a href="#1-finite-E-in-and-infinite-E-out" class="headerlink" title="1. finite $E_{in}$ and infinite $E_{out}$"></a>1. finite $E_{in}$ and infinite $E_{out}$</h4><p>æˆ‘ä»¬çš„è¿™äº›è¯æ˜éƒ½æ˜¯åœ¨åªè€ƒè™‘äº†$E_{in}$çš„åŸºç¡€ä¸Šï¼Œåœ¨æ³›åŒ–çš„è¿‡ç¨‹ä¸­æ˜¯æœ‰é—®é¢˜çš„ã€‚é¦–å…ˆï¼Œå¯¹äºdatasetï¼Œ$E_{in}$çš„ä¸ªæ•°æ˜¯æœ‰é™çš„ï¼Œå› ä¸ºåªè¦æœ‰break pointï¼Œæˆ‘ä»¬ä¸€å®šå¯ä»¥æ ¹æ®Nä¸kæ‰¾åˆ°hç§ç±»çš„ä¸Šé™ï¼Œä½†æ˜¯$E_{out}$çš„ä¸ªæ•°æ˜¯æ— é™çš„ã€‚è™½ç„¶åŒä¸€ç±»hå®ƒä»¬çš„$E_{in}$å¯èƒ½ä¸€è‡´ï¼Œä½†æ˜¯å®ƒä»¬çš„$E_{out}$å¹¶ä¸ä¸€è‡´ã€‚</p><p>å¦‚ä½•å¯¹ä»˜è¿™ä¸ªæ— é™çš„$E_{out}$ï¼Ÿæˆ‘ä»¬å¯ä»¥å†ä»æ€»ä½“ç§æŠ½å‡ºä¸€ä¸ªæ•°ç›®ä¸ºNçš„datasetï¼Œå®ƒç”¨Hå¾—åˆ°çš„é”™è¯¯ç‡è®°ä¸º$Eâ€™_{in}$ï¼Œç„¶åæˆ‘ä»¬ç”¨$E_{in}$ä¸$Eâ€™_{in}$æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå› ä¸ºåŒæ ·ï¼Œ$Eâ€™_{in}çš„ä¸ªæ•°æ˜¯æœ‰é™çš„$ã€‚</p><p>ä»ä¸‹å›¾ä¸­å¯ä»¥çœ‹å‡ºæ¥ï¼Œå½“$|E_{in}-E_{out}| \geq \epsilon$æ—¶å€™ï¼Œ$|E_{in}-Eâ€™_{in}| \geq \epsilon$çš„æ¦‚ç‡å¤§æ¦‚ä¸º1/2ï¼Œå½“ç„¶å¯èƒ½ä¼šæ›´å¤§ã€‚</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/XU%5DTY%7DQ%24SSE4X%40B%24_A1ZORN.png" alt=""></p><p>ä¸è¿‡å®é™…ä¸Šçš„å…¶ä»–æƒ…å†µä¸‹ï¼Œ$|E_{in}-Eâ€™_{in}| \geq \epsilon$ä¹Ÿæ˜¯æœ‰å¯èƒ½ä¼šå‘ç”Ÿçš„ï¼Œå› æ­¤</p><script type="math/tex; mode=display">\frac 1 2 P[âˆƒh \ln H s.t. |E_{in}(h) - E_{out}(h)| > \epsilon] \leq P[âˆƒh \ln H s.t. |E_{in}(h) - E'_{in}(h)| > \frac {\epsilon} 2]</script><p>ä¸ºä»€ä¹ˆè¦å¯¹${\epsilon}$é™¤ä»¥2ï¼Œæˆ‘ä¹Ÿä¸æ¸…æ¥šã€‚$E_{out}$æ˜¯æ— é™çš„ï¼Œå¦‚æœ$E_{out}$ä¸$E_{in}$æ˜¯ä¸€ä¸€å¯¹åº”çš„å…³ç³»ï¼Œé‚£ä¹ˆä¸é™¤ä»¥äºŒä¸Šå¼ä¹Ÿæ˜¯æˆç«‹çš„ã€‚ä¹Ÿè®¸å› ä¸ºæ˜¯æ›´ä¸¥æ ¼çš„æ•°å­¦é™åˆ¶ï¼Œä½†æ˜¯ä¸ç®¡æ€ä¹ˆè¯´ç»è¿‡å¤æ‚çš„æ•°å­¦è¯æ˜ï¼ˆè¶…å‡ºæˆ‘çš„èƒ½åŠ›ç•Œé™ï¼Œäº¤ç»™ç»Ÿè®¡å­¦å®¶ä¸æ•°å­¦å®¶å§ï¼‰ï¼Œä¸Šå¼ä¸€å®šæ˜¯æˆç«‹çš„ã€‚</p><p>å› æ­¤æˆ‘ä»¬å°†æ— é™çš„æ¢æˆäº†æœ‰é™çš„ï¼Œè¿™æ ·ç¦»ç»ˆç‚¹å°±è¿›äº†ä¸€æ­¥ã€‚æˆ‘ä»¬å¯ä»¥æºç¨‹ä¸‹é¢çš„æ ·å­ï¼š</p><p>$P[Baddata] \leq 2P[âˆƒh \ln H s.t. |E_{in}(h) - Eâ€™_{in}(h)| &gt; \frac {\epsilon} 2]$</p><h4 id="2-decompose-H-by-kind"><a href="#2-decompose-H-by-kind" class="headerlink" title="2. decompose H by kind"></a>2. decompose H by kind</h4><p>è¿™ä¸€æ­¥ï¼Œéœ€è¦ä½¿ç”¨$m_H(N)$æ¥å¤„ç†ä¸Šå¼çš„$âˆƒh \ln H$ï¼Œä½†æ˜¯å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå› ä¸ºæˆ‘ä»¬åæ¥åˆå–äº†Nä¸ªæ ·æœ¬æ¥åš$Eâ€™_{in}$ï¼Œå› æ­¤æ‰€æœ‰çš„æ ·æœ¬é‡æ˜¯2Nï¼Œéœ€è¦æ›¿æ¢ä¸º$m_H(2N)$,å¾—åˆ°ä¸‹é¢çš„ç»“æœï¼š</p><p>$P[Baddata] \leq 2 m_H(2N) P[fixed h s.t. |E_{in}(h) - Eâ€™_{in}(h)| &gt; \frac {\epsilon} 2]$</p><h4 id="3-hoeffding-without-replacement"><a href="#3-hoeffding-without-replacement" class="headerlink" title="3. hoeffding without replacement"></a>3. hoeffding without replacement</h4><p>ç¬¬ä¸‰ä¸ªï¼Œå°±è¦ç”¨æ¥å¤„ç†$P[fixed h s.t. |E_{in}(h) - Eâ€™_{in}(h)| &gt; \frac {\epsilon} 2]$äº†ã€‚å®é™…ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸Šå¼å†™æˆä¸‹é¢çš„æ ·å­ï¼š</p><script type="math/tex; mode=display">P[fixed h s.t. |E_{in}(h) - \frac {E'_{in}(h)+E_{in}(h)} 2 | > \frac {\epsilon} 4 ]</script><p>ä»”ç»†è§‚å¯Ÿï¼Œä¸Šé¢å…¶å®å°±æ˜¯hoeffdingä¸ç­‰å¼çš„ä¸€ç§ï¼Œåªä¸è¿‡è¿™æ—¶å€™çš„binä¸æ˜¯æ— é™å¤§äº†ï¼Œä½†æ˜¯æœ€åç»“æœæ˜¯ä¸€æ ·çš„ã€‚ï¼ˆä»2Nä¸ªæŠ½å‡ºNä¸ªï¼Œç®—å‡ºé”™è¯¯çš„æ¯”ç‡ï¼Œä¸å®é™…ä¸Š2Nçš„é”™è¯¯çš„æ¯”ç‡çš„å·®ï¼‰ï¼ˆ<font color="red">å®é™…ä¸Šæˆ‘å¯¹è¿™ä¸ªè§£é‡Šæ˜¯å­˜æœ‰ç–‘è™‘çš„ï¼Œè¿™ä¸ªéšæœºæŠ½å‡º2Nä¸ªåº”è¯¥æ˜¯ä»æ•´ä½“å‡ºå‘çš„ï¼Œè€Œä¸æ˜¯ä»2Nä¸ªä¸­æŠ½å‡ºæ¥Nä¸ªï¼Œç®—è¿™ä¸ªæœŸæœ›å·®ï¼Œä¹Ÿè®¸å¯ä»¥ä»æ•°å­¦ä¸Šè¯æ˜äºŒè€…æ¦‚ç‡æ˜¯ä¸€è‡´çš„å§</font>ï¼‰ã€‚</p><p>ä»£å…¥hoeffdingä¸ç­‰å¼å¯ä»¥å¾—åˆ°æœ€ç»ˆçš„ç»“æœï¼š</p><script type="math/tex; mode=display">P[âˆƒh \ln H s.t. |E_{in}(h) - E_{out}(h)|>\epsilon] \leq 2 \cdot 2 m_H(2N) \cdot e^{-2 \cdot \frac 1 {16} \epsilon ^2 N}</script><p>è¿™å°±æ˜¯å¯¹æ€ä¹ˆå¾—åˆ°æœ€ç»ˆç»“æœçš„ç®€å•çš„è¯´æ˜ã€‚ä¸¥æ ¼çš„è¯æ˜æ˜¯éå¸¸å¤æ‚çš„ã€‚ä¸è¿‡æˆ‘ä»¬å¥½æ­¹ä¼¼ä¹æ˜ç™½äº†é‚£ä¹ˆä¸€ç‚¹ç‚¹å…¶ä¸­çš„é“ç†ã€‚</p><h2 id="VC-bound"><a href="#VC-bound" class="headerlink" title="VC bound"></a>VC bound</h2><p>ä¸Šé¢çš„ç®€å•è¯æ˜å¾—åˆ°çš„ç»“æœï¼Œå«åšVapnik Chervonenkis Boundï¼Œç®€ç§°ä¸ºVC boundã€‚</p><p>å¼•å…¥ä¸€ä¸ªæ–°çš„å®šä¹‰ï¼Œå«åšVC dimensionï¼Œå®ƒçš„å®šä¹‰ä¸break pointéå¸¸ç±»ä¼¼ï¼ŒVC dimension = k - 1ï¼Œä¹Ÿå°±æ˜¯æœ€åä¸€ä¸ªå¯ä»¥åœ¨æŸç§datasetä¸‹è¢«shatterçš„datasetçš„å¤§å°ã€‚</p><p>ç°åœ¨æˆ‘ä»¬å°è¯•æ¨ç®—ä¸€ä¸‹ perceptrons çš„ VC dimension.</p><p>å¯¹äº1ç»´çš„æ¥è¯´å¾ˆç®€å•ï¼Œ å®ƒçš„VC dimension æ˜¯ 2.</p><p>å¯¹äº2ç»´çš„æ¥è¯´ï¼Œç”±ä¹‹å‰çš„ä¹Ÿå¯ä»¥å¾—åˆ°æ˜¯ 3.</p><p>é‚£ä¹ˆå¯¹äºdç»´çš„perceptronï¼Œæˆ‘ä»¬å¯ä»¥çŒœæµ‹ï¼Œå®ƒçš„vc dimension éš¾é“æ˜¯ d+1å—ï¼Ÿ</p><p>ä¸ºäº†è¯æ˜V(d) = d+1,æˆ‘ä»¬éœ€è¦è¯æ˜ä¸¤ç‚¹ï¼š1. $V(d) \geq d+1$ 2. $ V(d) \leq d+1$.</p><p><strong>è¯æ˜$V(d) \geq d+1$ï¼š</strong></p><p>é¦–å…ˆï¼Œæ„é€ ä¸‹é¢ä¸€ä¸ªd+1*d+1çš„çŸ©é˜µï¼š</p><script type="math/tex; mode=display">\begin{bmatrix}1&0&0&0&0&...&0 \\1&1&0&0&0&...&0 \\1&0&1&0&0&...&0 \\...\\1&0&0&0&0&...&1\end{bmatrix}</script><p>ä¸Šè¿°çŸ©é˜µæ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ªæ ·æœ¬çš„ï¼Œæ˜¯dç»´çš„ï¼Œä¸è¿‡ä¼šåŠ ä¸Šé¢å¤–çš„$x_0$ç»´åº¦ã€‚<br>å…±æœ‰d+1ä¸ªæ ·æœ¬ã€‚</p><p>å›æƒ³perceptronï¼Œ$XW = Y$(åœ¨æœ¬ä¾‹ä¸­),è€Œä¸Šè¿°çŸ©é˜µæ˜¯å¯é€†çš„ï¼Œåˆ™$W = YX_{-1}$ï¼Œå› æ­¤ä¸ç®¡Yæ€ä¹ˆå˜ï¼Œéƒ½æœ‰Wå¯ä»¥ä½¿å¾—å®ƒæˆç«‹ï¼Œå› æ­¤è‡³å°‘ä¸Šé¢çš„è¿™ä¸ªdatasetå¯ä»¥è¢«H shatterï¼Œæ‰€æœ‰$V(d) \geq d+1$.</p><p><strong>è¯æ˜$V(d) \leq d+1$ï¼š</strong></p><p>ä¸ºäº†è¯æ˜ä¸Šå¼ï¼Œæˆ‘ä»¬è¦å†åŠ å…¥ä¸€ä¸ªæ ·æœ¬ï¼Œè¯æ˜æ— è®ºå¦‚ä½•d+2ä¸ªæ ·æœ¬æ˜¯ä¸èƒ½è¢«shatterçš„ã€‚</p><p>æˆ‘ä»¬å†ä¸Šé¢çš„çŸ©é˜µé‡Œå†åŠ ä¸€ä¸ªéé›¶çš„è¡Œå‘é‡$X_{d+2}$ï¼Œé‚£ä¹ˆç”±çº¿æ€§ä»£æ•°å¯ä»¥çŸ¥é“:</p><script type="math/tex; mode=display">X_{d+2} = \sum _{i = 1}^{d+1} a_iX_i</script><p>å› æ­¤ $X_{d+2}W = \sum _{i = 1}^{d+1} a_iX_iW$.</p><p>åˆ™ y = $\{sign(a_1),sign(a_2),â€¦sign(a_{d+1}) ,-1 \}$è¿™ç§æƒ…å†µå°±ä¸€å®šæ˜¯ä¸èƒ½ç”Ÿæˆçš„($a_iX_iW$åæ¯ä¸€é¡¹éƒ½æ˜¯å¤§äºç­‰äº0çš„)ã€‚<br>æ‰€ä»¥d+2ä¸ªæ ·æœ¬æ˜¯æ— æ³•è¢«shatterçš„.</p><p>å¦‚æœå‰d+1ä¸ªæ ·æœ¬éƒ½ä¸èƒ½è¢«shatterï¼Œå°±æ›´ä¸ç”¨è¯´d+2ä¸ªå¯ä»¥è¢«shatteräº†ã€‚</p><p>æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼ŒV(d) = d+1.</p><p>VC dimension å®é™…ä¸Šæ˜¯è‡ªç”±åº¦ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œå®ƒæ˜¯äº’ä¸ä¾èµ–çš„å¯ä»¥å˜åŠ¨çš„å‚æ•°ä¸ªæ•°ï¼ˆå¹¶ä¸ä¸€å®šæ€»æ˜¯è¿™æ ·ï¼‰ã€‚</p><h2 id="Interpreting-of-VC-dimension"><a href="#Interpreting-of-VC-dimension" class="headerlink" title="Interpreting of VC dimension"></a>Interpreting of VC dimension</h2><p>Hoeffding å‘Šè¯‰æˆ‘ä»¬åäº‹æƒ…å‘ç”Ÿçš„æ¦‚ç‡ï¼Œæˆ‘ä»¬ç°åœ¨åæ¨ï¼Œå¥½äº‹æƒ…å‘ç”Ÿçš„æ¦‚ç‡ï¼Œå¾ˆç®€å•å¦‚ä¸‹ï¼š</p><p>$P[|E_{in}(g) - E_{out}(g)|&lt; \epsilon ] \geq 1 -  4(2N)^{d_{vc}}e^{- \frac 1 8 \epsilon ^2 N} $</p><p>å¦‚æœå°†å¤§äºç­‰äºåå¤æ‚çš„é‚£ä¸€éƒ¨åˆ†ï¼ˆVC boundï¼‰åˆ—ä¸º$\delta$ï¼Œé‚£ä¹ˆç»è¿‡æ¨ç®—å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">\epsilon = \sqrt {\frac 8 N \ln  {(\frac {4(2N)^{d_{vc}}} {\delta })}}</script><p>é‚£ä¹ˆæˆ‘ä»¬å¯ä»¥åœ¨$1 - \delta$çš„æ¦‚ç‡ä¸‹è·å¾—ä¿è¯$E_{out}$åœ¨è¿™ä¸ªèŒƒå›´å†…ï¼š</p><script type="math/tex; mode=display">\left [ E_{in}(g) - \sqrt {\frac 8 N \ln {(\frac {4(2N)^{d_{vc}}} {\delta })}}, E_{in}(g) + \sqrt {\frac 8 N \ln {(\frac {4(2N)^{d_{vc}}} {\delta })}} \right ]</script><p>æˆ‘ä»¬æ¯”è¾ƒé‡è§†å³è¾¹çš„éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯$E_{out}$æœ€åæ˜¯å¤šå°‘ã€‚æˆ‘ä»¬ç§°$\sqrt {â€¦}$ä¸ºpenalty for model complexityï¼Œè®°ä¸º${\Omega (N,H,\delta)}$.</p><p>ä¸€èˆ¬æ¥è¯´ï¼Œæœ‰ä¸ªä»¥ä¸‹çš„å…³ç³»å›¾ï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/M%29P32DW%29EE9%7BWB%246A08T8%29X.png" alt=""></p><p>ä»ä¸Šé¢å¯ä»¥çœ‹å‡ºæ¥ï¼Œå¦‚æœæ ·æœ¬ä¸ªæ•°ä¸€å®šè€Œä¸”ä¿è¯å¾ˆé«˜çš„probabilityï¼Œä¸€å‘³å¢åŠ ç»´åº¦ï¼ˆå¢åŠ æ–°çš„ç‰¹å¾ï¼‰å¯èƒ½ä¼šå‡ºç°è¿‡æ‹Ÿåˆçš„æƒ…å†µï¼Œå› ä¸ºå®ƒå¢åŠ äº†æ¨¡å‹å¤æ‚åº¦ã€‚è¿™å¯å‘äº†æˆ‘ä»¬åœ¨æœºå™¨å­¦ä¹ æ—¶å€™ä¸ä¸€å®šéè¦å¢åŠ è¿‡å¤šçš„ç‰¹å¾é‡ï¼Œæˆ–è€…ä¸€å‘³åœ°å»é™ä½$E_{in}$ï¼Œä»è€Œå¯¼è‡´æ³›åŒ–èƒ½åŠ›ä¸å¼ºã€‚</p><p>æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜éœ€è¦æ³¨æ„ä¸€ç‚¹ï¼Œå¦‚æœæˆ‘ä»¬åˆ©ç”¨VC boundå»æ±‚æ‰€éœ€è¦çš„æ•°æ®é‡ï¼Œå¾€å¾€å¾—åˆ°ä¸€ä¸ªå¾ˆå¤§çš„å€¼ï¼Œä½†æ˜¯å®é™…ä¸Šä¸€èˆ¬æ¥è¯´åªè¦10$d_{vc}$å°±å·®ä¸å¤šè¶³å¤Ÿäº†ï¼Œè¿™è¯´æ˜VC boundæ˜¯å¾ˆå®½æ¾çš„ã€‚å› ä¸ºæˆ‘ä»¬ä¸€ç›´å–çš„éƒ½æ˜¯ä¸Šé™ï¼Œä½†æ˜¯æˆ‘ä»¬ä¹Ÿå¾ˆéš¾åœ¨åŒ…å®¹è¿™ä¹ˆå¤šåˆ†å¸ƒçš„æƒ…å†µä¸‹æ‰¾åˆ°ä¸€ä¸ªæ›´å¥½çš„ç•Œé™ã€‚</p><p>åˆ°è¿™é‡Œï¼Œå°±è¯´çš„å·®ä¸å¤šäº†ï¼Œæˆ‘ä»¬è¯æ˜äº†å¦‚æœæœ‰VC dimensionï¼Œé‚£ä¹ˆåœ¨Nè¶³å¤Ÿå¤§çš„æƒ…å†µï¼Œå¯ä»¥å–å¾—ä¸é”™çš„å­¦ä¹ æ•ˆæœã€‚åŒæ—¶ä¹Ÿå¯å‘äº†ä»¥åæˆ‘ä»¬åœ¨æœºå™¨å­¦ä¹ ä¸Šçš„ä¸€äº›åšæ³•ã€‚</p><h2 id="p-s"><a href="#p-s" class="headerlink" title="p.s."></a>p.s.</h2><ol><li><p>ç”¨ç¨‹åºç”ŸæˆB(4,3)ï¼Œæˆ‘ä½¿ç”¨çš„æ˜¯å¾ˆç®€å•çš„ç¨‹åºï¼Œä½†æ˜¯åº”è¯¥å¯ä»¥è¯æ˜è¿™æ ·ç”Ÿæˆçš„dichotomyä¸ªæ•°å°±æ˜¯æœ€å¤§çš„ä¸ªæ•°ã€‚ç¨‹åºå¦‚ä¸‹ï¼š</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">(result,l)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> in_a <span class="keyword">in</span> [[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]]:</span><br><span class="line">        exist = [<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">8</span>)]</span><br><span class="line">        size = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> result:</span><br><span class="line">            temp = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> bit <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">                temp+=(i[in_a[bit]]&lt;&lt;bit)</span><br><span class="line">            <span class="keyword">if</span> exist[temp] == <span class="number">0</span>:</span><br><span class="line">                exist[temp] = <span class="number">1</span></span><br><span class="line">                size+=<span class="number">1</span></span><br><span class="line">        temp = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> bit <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">            temp += (l[in_a[bit]] &lt;&lt; bit)</span><br><span class="line">        <span class="keyword">if</span> exist[temp] == <span class="number">0</span>:</span><br><span class="line">            exist[temp] = <span class="number">1</span></span><br><span class="line">            size += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> size == <span class="number">8</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">four_three</span><span class="params">()</span>:</span></span><br><span class="line">    l = []</span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">16</span>):</span><br><span class="line">        temp = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">4</span>):</span><br><span class="line">            temp.append((i&gt;&gt;j)&amp;<span class="number">1</span>)</span><br><span class="line">        l.append(temp)</span><br><span class="line"></span><br><span class="line">    result.append(l[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">16</span>):</span><br><span class="line">        <span class="keyword">if</span> check(result,l[i]):</span><br><span class="line">            result.append(l[i])</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    result = four_three()</span><br><span class="line">    print(len(result))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> result:</span><br><span class="line">        print(i)</span><br></pre></td></tr></table></figure></li><li><p>hoeffdingä¸ç­‰å¼æ˜¯æ— éœ€çŸ¥é“æ•°æ®åˆ†å¸ƒæƒ…å†µçš„ï¼Œä¹Ÿå°±æ˜¯å¯¹äºä»»ä½•åˆ†å¸ƒå®ƒéƒ½é€‚ç”¨ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä½•VC boundå¾ˆå®½æ¾çš„ä¸€ä¸ªåŸå› ã€‚</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> mathematics </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”ï¼ˆåŸºçŸ³ï¼‰ä½œä¸š1</title>
      <link href="/2018/08/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%EF%BC%88%E5%9F%BA%E7%9F%B3%EF%BC%89%E4%BD%9C%E4%B8%9A1/"/>
      <url>/2018/08/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94%EF%BC%88%E5%9F%BA%E7%9F%B3%EF%BC%89%E4%BD%9C%E4%B8%9A1/</url>
      
        <content type="html"><![CDATA[<p>æ€»å…±20é“é¢˜ç›®ã€‚<a id="more"></a></p><p><strong>1. Which of the following problems are best suited for machine learning?</strong></p><p>(i) Classifying numbers into primes and non-primes</p><p>(ii) Detecting potential fraud in credit card charges</p><p>(iii) Determining the time it would take a falling object to hit the ground</p><p>(iv) Determining the optimal cycle for traffic lights in a busy intersection</p><p>(v) Determining the age at which a particular medical test is recommended</p><p>è¿™ä¸ªé¢˜ç›®æ¯”è¾ƒç®€å•ï¼Œå…¶ä¸­1å’Œ3å¾ˆæ˜æ˜¾ä¸æ˜¯æœºå™¨å­¦ä¹ é—®é¢˜ï¼Œæˆ‘ä»¬æ¸…æ¥šè´¨æ•°ä¸éè´¨æ•°çš„è§„åˆ™ï¼Œä¹ŸçŸ¥é“ç‰©ä½“ä¸‹è½å…¬å¼ï¼Œä¸éœ€è¦æœºå™¨å»å­¦ä¹ ï¼Œå…¶ä»–æ­£ç¡®ï¼Œç­”æ¡ˆæ˜¯2ï¼Œ4ï¼Œ5.</p><p>For Questions 2Â­-5, identify the best type of learning that can be used to solve each task below.</p><p><strong>2. Play chess better by practicing different strategies and receive outcome as feedback.</strong></p><p>reinforcement learningâ€”â€”â€”åŠ å¼ºå­¦ä¹ ï¼Œå› ä¸ºéœ€è¦ä¸æ–­åŠ å¼ºï¼Œå­¦ä¹ è¿‡ç¨‹æ˜¯è¿ç»­çš„ã€‚</p><p><strong>3. Categorize books into groups without pre-defined topics.</strong></p><p>unsupervised learningâ€”â€”â€”â€”å¾ˆæ˜æ˜¾æ˜¯æ— ç›‘ç£å­¦ä¹ ã€‚</p><p><strong>4. Recognize whether there is a face in the picture by a thousand face pictures and ten thousand nonÂ­face pictures.</strong></p><p>supervised learningâ€”â€”â€”â€”æ•°æ®å·²ç»æ ‡å¥½æ ‡ç­¾ã€‚</p><p><strong>5. Selectively schedule experiments on mice to quickly evaluate the potential of cancer medicines.</strong></p><p>active learningâ€”â€”â€”â€”å®éªŒçš„æ¬¡æ•°æ˜¯æœ‰é™çš„ï¼Œå¯èƒ½æ— æ³•åšå‡ºæµ·é‡æ¬¡æ•°çš„å®éªŒï¼Œå› æ­¤éœ€è¦æ ¹æ®å°‘æ•°å®éªŒç»“æœï¼ˆå³éƒ¨åˆ†æ ·æœ¬æœ‰æ ‡ç­¾ï¼‰ï¼Œè¿™å®é™…ä¸Šæ˜¯åŠç›‘ç£å­¦ä¹ çš„ä¸€ç§ï¼Œå½“é‡åˆ°æœºå™¨æ— æ³•å†³æ–­çš„æ—¶å€™å†å»äººå·¥æ ‡ç­¾ï¼Œä¹Ÿå°±æ˜¯ä¸»åŠ¨å­¦ä¹ ã€‚</p><p>Question 6-8 are about Off-Training-Set error.</p><p>6.<img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/B%247SX%5BIV_SSX4LWNA%5BO7%7EM6.png" alt="6"></p><p>è¿™ä¸ªé¢˜ç›®ä¸­æ ·æœ¬$x_1,â€¦x_n$ä¸ºè®­ç»ƒé›†ï¼Œè€Œå…¶ä½™Lä¸ªä¸ºæµ‹è¯•é›†ã€‚æ­£ç¡®çš„åˆ†ç±»æ˜¯æ‰€æœ‰çš„éƒ½æ˜¯+1ï¼Œè€Œæˆ‘ä»¬å¾—åˆ°çš„$g(x)$æ˜¯æ ·æœ¬ä¸­ä¸‹æ ‡ä¸ºå¥‡æ•°çš„æ˜¯+1ï¼Œä¹Ÿå°±æ˜¯å¤§æ¦‚ä¼šé”™ä¸€åŠã€‚å…·ä½“é”™å¤šå°‘ï¼Ÿ<br>æ€»ä½“æ ·æœ¬é”™çš„ä¹Ÿå°±æ˜¯âŒŠ$\frac {N+L} 2â€‹â€‹$âŒ‹ï¼Œè€Œé™¤å»è®­ç»ƒé›†ä¸­ä¼šé”™çš„âŒŠ$\frac {N} 2â€‹â€‹$âŒ‹ç­”æ¡ˆæ˜¯ç¬¬äº”ä¸ªã€‚</p><p><strong>7. We say that a target function $f$ can â€œgenerateâ€ $\mathcal{D}$ in a noiseless setting if $f(x_n)=y_n$â€‹ for all ($x_n$, $y_n$) $\in \mathcal{D}$.<br>For all possible f$ \colon \mathcal{X} \rightarrow \mathcal{Y}$, how many of them can generate $\mathcal{D}$ in a noiseless setting?</strong></p><p>è¿™ä¸ªé¢˜ç›®æ„æ€å®¹æ˜“è®©äººæ›²è§£ï¼Œå®é™…ä¸Šé—®çš„æ˜¯å¤–é¢æµ‹è¯•é›†å¤§å°ä¸ºLï¼Œé‚£ä¹ˆæœ‰å¤šå°‘ç§å¯èƒ½çš„fï¼Œfæ»¡è¶³Dä¸­çš„æ ·æœ¬ï¼Œä½†æ˜¯å¯¹äºæµ‹è¯•é›†ä¸­æ˜¯æ— æ‰€è°“çš„ï¼Œå› æ­¤å¯èƒ½çš„fæœ‰$2^L$ä¸ªã€‚</p><p>8.<img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/22%24ZSJYPDBCLC0U%7EC7%7DP87Q.png" alt="8"><br>è¿™ä¸ªé¢˜ç›®ä¸»è¦åœ¨è€ƒå¯Ÿçš„æ˜¯NFLå®šç†ã€‚å¦‚æœæ‰€æœ‰å¯ä»¥generateDçš„fæ˜¯ç­‰æ¦‚ç‡çš„ï¼Œä¹Ÿå°±æ˜¯å¯¹äºæµ‹è¯•é›†ä¸­æ ·æœ¬çš„+1ï¼Œ-1æ˜¯å®Œå…¨éšæœºçš„ï¼Œé‚£ä¹ˆæ‰€æœ‰å­¦ä¹ ç®—æ³•å¾—åˆ°çš„è¯¯å·®æœŸæœ›éƒ½æ˜¯ä¸€è‡´çš„ã€‚æ‰€ä»¥é€‰2.</p><p>For Questions 9-12, consider the bin model introduced in class. </p><p><strong>9. Consider a bin with infinitely many marbles, and let Î¼ be the fraction of orange marbles in the bin, and Î½ is the fraction of orange marbles in a sample of 10 marbles. If Î¼=0.5, what is the probability of Î½=Î¼?</strong></p><p>è¿™æ˜¯ä¸€é“æ¯”è¾ƒç®€å•çš„æ¦‚ç‡é¢˜ã€‚u = 0.5ï¼Œæ±‚é€‰å‡º10ä¸ªå‡ºæ¥æœ‰5ä¸ªæ˜¯æ©˜è‰²çš„æ¦‚ç‡ã€‚å› ä¸ºè¿™ä¸ªä»“åº“ç®±å­é‡Œå¼¹ç çš„ä¸ªæ•°æ˜¯æ— ç©·çš„ï¼Œæ‰€ä»¥å³ä½¿ä¸æ”¾å›ï¼Œæ¯æ¬¡å–å‡ºæ¥ä¾ç„¶è¿‘ä¼¼äºç‹¬ç«‹é‡å¤å®éªŒã€‚åˆ™<br>$p = C_{10}^5 {\frac 1 2}^{10} \approx 0.246$.</p><p><strong>10. If Î¼=0.9, what is the probability of Î½=Î¼?</strong></p><p>ä¸ä¸Šé¢ç±»ä¼¼ï¼š$p = C_{10}^9 {0.9}^9 \times 0.1 \approx 0.387$.</p><p><strong>11. If Î¼=0.9, what is the actual probability of Î½â‰¤0.1?</strong></p><p>è¿™ä¸ªé—®é¢˜å°±ä¸Šé¢æ¥è¯´ç¨å¾®å¤æ‚äº†ä¸€ç‚¹ï¼Œä½†æ˜¯ä¹Ÿä¸éš¾ã€‚vâ‰¤0.1ä¹Ÿå°±æ˜¯10ä¸ªä¸­æŠ½åˆ°äº†1ä¸ªæˆ–è€…0ä¸ªã€‚<br>$p = C_{10}^9 {0.9}^1 \times 0.1^9 + C_{10}^{10} {0.1}^{10} = 9.1 \times 10^{-9}$.</p><p><strong>12. If Î¼=0.9, what is the bound given by Hoeffdingâ€™s Inequality for the probability of Î½â‰¤0.1?</strong></p><p>ç”±é¢˜ç›®å¯ä»¥çŸ¥é“$\epsilon$ = 0.8,å¸¦å…¥å…¬å¼å¯ä»¥å¾—åˆ°æ¦‚ç‡ä¸Šç•Œä¸º$2e^{-12.8} \approx 5.52 \times 10^{-6}$</p><p>Questions 13Â­-14 illustrate what happens with multiple bins using dice to indicate 6 bins. Please note that the dice is not meant to be thrown for random experiments in this problem. They are just used to bind the six faces together. The probability below only refers to drawing from the bag.</p><p><strong>13. Consider four kinds of dice in a bag, with the same (super large) quantity for each kind.</strong></p><p>A: all even numbers are colored orange, all odd numbers are colored green</p><p>B: all even numbers are colored green, all odd numbers are colored orange</p><p>C: all small (1~Â­3) are colored orange, all large numbers (4Â­~6) are colored green</p><p>D: all small (1Â­~3) are colored green, all large numbers (4~Â­6) are colored orange</p><p>If we pick 5 dice from the bag, what is the probability that we get 5 orange 1â€™s?</p><p>ç®€å•ç¿»è¯‘ä¸‹é¢˜ç›®ï¼šè¢‹å­é‡Œæœ‰4ç§éª°å­ï¼Œç¬¬ä¸€ç§2ï¼Œ4ï¼Œ6é¢ä¸ºæ©˜è‰²ï¼Œç¬¬äºŒç§1ï¼Œ3ï¼Œ5é¢ä¸ºæ©˜è‰²ï¼Œç¬¬ä¸‰ç§1ï¼Œ2ï¼Œ3é¢ä¸ºæ©˜è‰²ï¼Œç¬¬å››ç§4ï¼Œ5ï¼Œ6é¢ä¸ºæ©˜è‰²ã€‚4ç§ç­›å­æ¯”ä¾‹ç›¸åŒï¼Œéª°å­æ•°ç›®å¾ˆå¤šã€‚ç¬¬ä¸€é“é¢˜ç›®é—®åˆ°ï¼Œæ‹¿5ä¸ªéª°å­å‡ºæ¥ï¼Œ5ä¸ªéª°å­ç¬¬ä¸€é¢éƒ½æ˜¯æ©˜è‰²çš„æ¦‚ç‡ï¼Ÿ</p><p>1é¢æ˜¯æ©˜è‰²ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä¸Šé¢4ç±»åˆ†æˆ2ç±»äº†ï¼Œå…¶ä¸­ç¬¬äºŒä¸ç¬¬ä¸‰åˆå¹¶ï¼Œæ¯æ¬¡å–å‡ºæ¥1é¢ä¸ºæ©˜è‰²çš„æ¦‚ç‡æ˜¯0.5ï¼Œæ‰€ä»¥5ä¸ªéƒ½æ˜¯çš„æ¦‚ç‡æ˜¯$\frac 1 {32}$ã€‚</p><p><strong>14. If we pick 5 dice from the bag, what is the probability that we get â€œsome numberâ€ that is purely orange?</strong></p><p>æˆ‘ä»¬æ‹¿å‡º5ä¸ªéª°å­ï¼Œè‡³å°‘æœ‰ä¸€é¢å…¨éƒ¨éƒ½æ˜¯æ©˜è‰²çš„æ¦‚ç‡ã€‚</p><p>è¿™ä¸ªå°±æ˜¯ç¨å¾®å¤æ‚çš„ä¸€ä¸ªé—®é¢˜ã€‚é¦–å…ˆè§‚å¯Ÿéª°å­ç§ç±»ï¼Œæˆ‘ä»¬å‘ç°ï¼Œåªè¦ç¬¬ä¸€ç§ä¸ç¬¬äºŒç§ç¢°é¢ï¼Œæˆ–è€…ç¬¬ä¸‰ç§ä¸ç¬¬å››ç§ç¢°é¢ï¼Œé‚£ä¹ˆå°±ä¸å¯èƒ½æœ‰ä¸€é¢å…¨éƒ½æ˜¯æ©˜è‰²ã€‚æ‰€ä»¥æˆ‘ä»¬è¦æ±‚çš„å°±æ˜¯ä¸Šé¢ä¸¤ç§æƒ…å†µä¸å‘ç”Ÿçš„æ¦‚ç‡ã€‚</p><p>å¦‚æœæŠ½å‡ºçš„5ä¸ªéª°å­ï¼Œå äº†4ç§éª°å­ç§çš„3ç§æˆ–è€…4ç§ï¼Œé‚£ä¹ˆä¸Šé¢çš„æƒ…å†µè‡³å°‘ä¼šæœ‰ä¸€ç§ä¼šå‘ç”Ÿã€‚<br>è€Œå–5æ¬¡å–å‡º3ç§çš„æƒ…å†µæœ‰2 2 1ä¸ 3 1 1ä¸¤ç§å¯èƒ½ã€‚</p><p>é¦–å…ˆï¼Œä»4ç§é‡Œé€‰3ç§å‡ºæ¥$C_4^3$,å…¶æ¬¡ï¼Œè€ƒè™‘2 2 1çš„æƒ…å†µ$C_5^2C_3^2$,è€Œ2 2 1åˆä¼šæœ‰3ç§åˆ†å¸ƒï¼Œå› æ­¤2 2 1çš„æ‰€æœ‰å¯èƒ½æƒ…å†µæ˜¯$3C_4^3C_5^2C_3^2 = 360$.</p><p>3 1 1ä¸ä¸Šè¿°ç±»ä¼¼$3C_4^3C_5^3C_2^1 = 240$.</p><p>ç„¶åè€ƒè™‘ä»4ç§å–4ç§çš„æƒ…å†µï¼Œåªä¼šæœ‰ä¸€ç§åˆ†å¸ƒï¼š1 1 1 2ï¼Œå¯ä»¥å¾—åˆ°$4C_5^2C_3^1C_2^1 = 240$.</p><p>æœ€åæˆ‘ä»¬å°±è¦è€ƒè™‘åˆ°ä»4ç§ä¸­å–å‡ºæ¥ä¸¤ç§ï¼Œè€Œä¸”æ°å¥½æ˜¯ç¬¬ä¸€ç§ä¸ç¬¬äºŒç§ï¼Œæˆ–è€…ç¬¬ä¸‰ç§ä¸ç¬¬å››ç§çš„æ¦‚ç‡ã€‚éª°å­æœ‰ä¸¤ç§çš„æ¬¡æ•°åˆ†å¸ƒæœ‰2ç§æƒ…å†µï¼š1 4ï¼Œ2 3.</p><p>1 4: 2$C_5^4 = 10$</p><p>2 3: $2C_5^3 = 20$</p><p>è€ƒè™‘åˆ°ç¬¬ä¸€ç§ä¸ç¬¬äºŒç§ï¼Œç¬¬ä¸‰ç§ä¸ç¬¬å››ç§ï¼Œå› æ­¤ç»“æœè¿˜åº”ä¹˜2ï¼Œæœ€åç»“æœæ˜¯ï¼š60.</p><p>æ‰€ä»¥ï¼Œæ‰€æœ‰ä¸ç¬¦åˆçš„æƒ…å†µå…±æœ‰240+240+60+360 = 900ï¼Œå› æ­¤è¿™é“é¢˜ç›®æœ€åç»“æœæ˜¯$1 - \frac {900} {4^5} = \frac {31} {256}$.</p><p>For Questions 15-20, you will play with PLA and pocket algorithm. </p><p>15-20ä¸ºç¼–ç¨‹é¢˜ç›®ï¼Œéœ€è¦è‡ªå·±å†™ä»£ç éªŒè¯ï¼Œç„¶åå¾—åˆ°ç»“æœã€‚</p><p><strong>15.  First, we use an artificial data set to study PLA. The data set is in<br><a href="https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_math/hw1_15_train.dat" target="_blank" rel="noopener">data</a>.Each line of the data set contains one ($\mathbf{x}_n, y_n$) with $\mathbf{x}_n \in \mathbb{R}^4$. The first 4 numbers of the line contains the components of $\mathbf{x}_n$ orderly, the last number is $y_n$.<br>Please initialize your algorithm with $\mathbf{w} = 0$ and take $sign(0)$ as -1. Please always remember to add $x_0 = 1$ to each $\mathbf{x}_nâ€‹$.Implement a version of PLA by visiting examples in the naive cycle using the order of examples in the data set. Run the algorithm on the data set. What is the number of updates before the algorithm halts?</strong></p><p>a.&lt;10 updates</p><p>b.11 - 30 updates</p><p>c.31 - 50 updates</p><p>d.$\geq$ 201 updates</p><p>e.51 - 200 updates</p><p>è¿™é“é¢˜ç›®åªéœ€è¦åº”ç”¨ä¸Šæ¬¡å®ç°çš„PLAç®—æ³•ï¼Œéœ€è¦é¢å¤–åšçš„æ˜¯æ•°æ®çš„è¯»å–ã€‚åœ¨æ•°æ®è¯»å–æ—¶å€™æˆ‘è¿ç”¨äº†æ­£åˆ™è¡¨è¾¾å¼æ¥è¿›è¡Œåˆ†å‰²ï¼Œæ•´ä¸ªè¿‡ç¨‹æ•´åˆåœ¨ä¸€ä¸ªå«åšreadDataFromå‡½æ•°ä¸­ã€‚<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readDataFrom</span><span class="params">(filename)</span>:</span></span><br><span class="line">    result = []</span><br><span class="line">    separator = re.compile(<span class="string">'\t|\b| |\n'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(filename,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        line = f.readline()</span><br><span class="line">        <span class="keyword">while</span> line:</span><br><span class="line">            temp = separator.split(line)[<span class="number">0</span>:<span class="number">-1</span>]</span><br><span class="line">            abc = [float(x) <span class="keyword">for</span> x <span class="keyword">in</span> temp]</span><br><span class="line">            result.append(abc)</span><br><span class="line">            line = f.readline()</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure></p><p>æœ€åå¾—åˆ°ç»“æœæ˜¯ä¿®æ­£äº†61æ¬¡ã€‚<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ä¿®æ­£æ¬¡æ•°ï¼š 61</span><br></pre></td></tr></table></figure></p><p><strong>16. Implement a version of PLA by visiting examples in fixed, pre-determined random cycles throughout the algorithm. Run the algorithm on the data set. Please repeat your experiment for 2000 times, each with a different random seed. What is the average number of updates before the algorithm halts?</strong></p><p>a.&lt;10 updates</p><p>b.11 - 30 updates</p><p> c.31 - 50 updates</p><p>d.$\geq$ 201 updates</p><p>e.51 - 200 updates</p><p>è¿™ä¸ªç‰ˆæœ¬éœ€è¦å¯¹åŸæ¥çš„plaç®—æ³•è¿›è¡Œç®€å•çš„ä¿®æ”¹ï¼Œæ¯æ¬¡å¯»æ‰¾é¢„æµ‹é”™è¯¯çš„æ ·æœ¬æ—¶å€™é‡‡ç”¨éšæœºçš„é¡ºåºå»å¯»æ‰¾ã€‚å¯¹äºåºåˆ—è¿›è¡Œéšæœºçš„åŠæ³•å®ç°æ–¹æ³•å¾ˆç®€å•ï¼Œä¹Ÿå°±æ˜¯æ‰“ä¹±åºåˆ—ï¼Œåšæ³•æ˜¯ä¸éšæœºçš„åæ ‡è¿›è¡Œäº¤æ¢ï¼š<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randomIndex</span><span class="params">(n)</span>:</span></span><br><span class="line">    index = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,n)]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">swap</span><span class="params">(l,x,y)</span>:</span></span><br><span class="line">        l[x] = l[x]+l[y]</span><br><span class="line">        l[y] = l[x] - l[y]</span><br><span class="line">        l[x] = l[x] - l[y]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,n):</span><br><span class="line">        swap(index,i,int(random.random()*n))</span><br><span class="line">    <span class="keyword">return</span> index</span><br></pre></td></tr></table></figure></p><p>åº”ç”¨ä¸Šé¢çš„å‡½æ•°ç”Ÿæˆéšæœºæ‰“ä¹±çš„åºåˆ—ï¼Œç„¶åä»£æ›¿é¡ºåºæŸ¥æ‰¾ï¼Œè¿è¡Œ2000æ¬¡å¹³å‡ä¿®æ­£æ¬¡æ•°å¦‚ä¸‹ï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ä¿®æ­£æ¬¡æ•°ï¼š 61.3933</span><br></pre></td></tr></table></figure></p><p>**17. Implement a version of PLA by visiting examples in fixed, pre-determined random cycles throughout the algorithm, while changing the update rule to be </p><script type="math/tex; mode=display">\mathbf{w}_{t+1} \leftarrow \mathbf{w}_t +\eta y_{n(t)}\mathbf{x}_{n(t)}</script><p>with Î·=0.5. Note that your PLA in the previous Question corresponds to Î·=1. Please repeat your experiment for 2000 times, each with a different random seed. What is the average number of updates before the algorithm halts?**<br>a.&lt;10 updates</p><p>b.11 - 30 updates</p><p>c.31 - 50 updates</p><p>d.$\geq$ 201 updates</p><p>e.51 - 200 updates</p><p>åªéœ€è¦å¯¹ä¸Šè¿°ç®—æ³•è¿›è¡Œç®€å•æ”¹åŠ¨å³å¯.è¿è¡Œ2000æ¬¡åå¹³å‡ä¿®æ­£æ¬¡æ•°å¦‚ä¸‹:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ä¿®æ­£æ¬¡æ•°ï¼š 63.532</span><br></pre></td></tr></table></figure></p><p><strong>18. Next, we play with the pocket algorithm. Modify your PLA in Question 16 to visit examples purely randomly, and then add the â€œpocketâ€ steps to the algorithm. We will use <a href="https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_math/hw1_18_train.dat" target="_blank" rel="noopener">train</a> as the training data set $\mathcal{D}$, and <a href="https://www.csie.ntu.edu.tw/~htlin/mooc/datasets/mlfound_math/hw1_18_test.dat" target="_blank" rel="noopener">test</a> as the test set for â€œverifyingâ€™â€™ the gg returned by your algorithm (see lecture 4 about verifying). The sets are of the same format as the previous one. Run the pocket algorithm with a total of 50 updates on $\mathcal{D}$ , and verify the performance of $\mathbf{w}_{POCKET}$ using the test set. Please repeat your experiment for 2000 times, each with a different random seed. What is the average error rate on the test set?</strong></p><p>a. &lt;0.2</p><p>b. 0.2 - 0.4</p><p>c. 0.4 - 0.6</p><p>d. $\geq$ 0.8</p><p>e. 0.6 - 0.8</p><p>è¿™ä¸ªé¢˜ç›®éœ€è¦å®ç°pocketç®—æ³•ã€‚pocketç®—æ³•ä¹‹å‰æ²¡æœ‰ä»‹ç»ï¼Œå› ä¸ºå®ƒå’Œplaå¾ˆåƒï¼Œåªæ˜¯plaç®—æ³•çš„ä¸€ä¸ªå˜å½¢ã€‚å› ä¸ºæˆ‘ä»¬æ— æ³•ä¿è¯æ•°æ®ä¸€å®šæ˜¯çº¿æ€§å¯åˆ†çš„ï¼Œä¸ºæ­¤æˆ‘ä»¬æ¯æ¬¡é‡åˆ°é”™è¯¯æ›´æ–°åï¼Œä¸ä¹‹å‰çš„wè¿›è¡Œå¯¹æ¯”ï¼Œå¦‚æœé”™è¯¯ç‡æ›´ä½ï¼Œå†æ›´æ–°è¿™ä¸ªå‚æ•°ï¼ŒåŒæ—¶plaç®—æ³•é€‰æ‹©é”™è¯¯çš„æ ·æœ¬æ—¶å€™æ˜¯éšæœºé¡ºåºé€‰æ‹©çš„ï¼Œä»ä¹‹å‰çš„ä»£ç éªŒè¯ä¸­æˆ‘ä»¬ä¹Ÿå‘ç°äº†éšæœºå¯¹äºå‡å°ä¿®æ­£æ¬¡æ•°æ¥è¯´æ˜¯æœ‰å¥½å¤„çš„ï¼ˆä½†æ˜¯ç”Ÿæˆéšæœºåºåˆ—åŒæ ·ä¹Ÿä¼šå¸¦æ¥é¢å¤–å¼€é”€ï¼‰ã€‚</p><p>å®ç°pocketç®—æ³•åªéœ€è¦å¤šåŠ å‡ è¡Œä»£ç ä»¥åŠåšå°‘é‡æ”¹åŠ¨ï¼Œè¿™é‡Œå°±ä¸åˆ—å‡ºæ¥äº†ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯éœ€è¦æ·»åŠ ä¸€ä¸ªæ–°çš„å‡½æ•°ï¼Œè¿›è¡Œé”™è¯¯è¯„ä¼°ï¼š<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">computeER</span><span class="params">(para,datas)</span>:</span></span><br><span class="line">    size = len(datas)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> size &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    dms = len(datas[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> dms == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, size):</span><br><span class="line">        p = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>, dms - <span class="number">1</span>):</span><br><span class="line">            p += para[x] * datas[i][x]</span><br><span class="line">        p += para[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> p &lt;= <span class="number">0</span> <span class="keyword">and</span> datas[i][<span class="number">-1</span>] &gt; <span class="number">0</span> <span class="keyword">or</span> p &gt; <span class="number">0</span> <span class="keyword">and</span> datas[i][<span class="number">-1</span>] &lt; <span class="number">0</span>:<span class="comment">#ignore </span></span><br><span class="line">            count=count+<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> count/size</span><br></pre></td></tr></table></figure></p><p>ä¸‹é¢æ˜¯è¿è¡Œç»“æœï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">å¹³å‡é”™è¯¯ç‡ï¼š 0.12437000000000001</span><br></pre></td></tr></table></figure></p><p><strong>19. Modify your algorithm in Question 18 to return $\mathbf{w}_{50}$ (the PLA vector after 5050 updates) instead of $\hat{\mathbf{w}}$ (the pocket vector) after 50 updates.Run the modified algorithm on $\mathcal{D}$, and verify the performance using the test set.Please repeat your experiment for 2000 times, each with a different random seed. What is the average error rate on the test set?</strong></p><p>a. &lt;0.2</p><p>b. 0.2 - 0.4</p><p>c. 0.4 - 0.6</p><p>d. $\geq$ 0.8</p><p>e. 0.6 - 0.8</p><p>è¿™ä¸ªç®—æ³•è¿”å›plaå‘é‡ã€‚è¿è¡Œç»“æœå¦‚ä¸‹ï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">å¹³å‡é”™è¯¯ç‡ï¼š 0.3666599999999999</span><br></pre></td></tr></table></figure></p><p><strong>20. Modify your algorithm in Question 18 to run for 100 updates instead of 50, and verify the performance of \$mathbf{w}_{POCKET}$ â€‹using the test set. Please repeat your experiment for 2000 times, each with a different random seed. What is the average error rate on the test set?</strong></p><p>a. &lt;0.2</p><p>b. 0.2 - 0.4</p><p>c. 0.4 - 0.6</p><p>d. $\geq$ 0.8</p><p>e. 0.6 - 0.8</p><p>å¾ˆç®€å•çš„æ”¹åŠ¨ï¼Œè¿è¡Œç»“æœå¦‚ä¸‹ï¼š<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">å¹³å‡é”™è¯¯ç‡ï¼š 0.10796000000000007</span><br></pre></td></tr></table></figure></p><h2 id="Noteï¼š"><a href="#Noteï¼š" class="headerlink" title="Noteï¼š"></a>Noteï¼š</h2><ol><li><p><em>åˆšå¼€å§‹å¯¹pocketç®—æ³•çš„ç†è§£æœ‰è¯¯ï¼Œä»¥ä¸ºwtçš„æ›´æ–°æ˜¯æ¯æ¬¡éƒ½å¯¹pocketä¸­çš„wä¸ºåŸºå‡†çš„ï¼Œå®é™…ä¸Šå®ƒå°±æ˜¯æ­£å¸¸çš„plaç®—æ³•è¿è¡Œçš„è¿‡ç¨‹ï¼Œä»”ç»†æ€è€ƒè¿™æ ·ç¡®å®æ¯”åŸæ¥çš„ç®—æ³•æ›´å¿«ã€‚ç¬¬äºŒï¼Œpocketç®—æ³•ä¸­éšæœºé€‰å–é”™è¯¯æ„å‘³ç€æ¯æ¬¡éƒ½è¦éšæœºæ‰“ä¹±æ ·æœ¬ï¼Œè¿™æ ·å¯ä»¥è·å¾—æ›´ä½çš„é”™è¯¯ç‡ã€‚</em></p></li><li><p><em>pythonä¸æ”¯æŒi++,++iè¿™ç§æ“ä½œï¼Œä½†æ˜¯ï¼Œä¸ºä½•ä¸æŠ¥é”™â€¦</em></p></li><li><p><em>listç›´æ¥èµ‹å€¼åªæ˜¯å¯¹æºå¯¹è±¡è¿›è¡Œäº†å¼•ç”¨ï¼ŒåŒæ—¶æœ‰æµ…æ‹·è´ä¸æ·±æ‹·è´ä¹‹åˆ†ã€‚è¿™ä¸ªç‹ ç‹ åœ°å‘äº†æˆ‘ä¸€æŠŠã€‚</em></p></li><li><p>15ï¼Œ16ï¼Œ17é¢˜ç›®å‰è¿è¡Œä¾ç„¶æ˜¯60å¤šæ¬¡.ç­”æ¡ˆæ˜¯31åˆ°50ã€‚â€”â€”â€”<br>æ›´æ–°äº8.8ã€‚ æˆ‘å¿½ç•¥äº†cycleè¿™ä¸ªè¯ï¼Œä¹Ÿå°±æ˜¯æ‰¾é”™è¯¯çš„è¿‡ç¨‹åº”è¯¥ä»ä¸Šæ¬¡æ–­æ‰çš„åœ°æ–¹ç»§ç»­å¼€å§‹ï¼Œæ›´æ–°åå¾—åˆ°æ­£ç¡®çš„ç»“æœã€‚<br>å—åˆ°å½±å“çš„ä»£ç é‡æ–°è·‘äº†ä¸€éï¼š</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">15. ä¿®æ­£æ¬¡æ•°ï¼š 46</span><br><span class="line">16. ä¿®æ­£æ¬¡æ•°ï¼š 40.867</span><br><span class="line">17. ä¿®æ­£æ¬¡æ•°ï¼š 41.5105</span><br></pre></td></tr></table></figure></li></ol><p>ä»£ç ä¸Šä¼ è‡³<a href="https://github.com/MyEvolution/PLA" target="_blank" rel="noopener">github</a>ï¼Œä¿®æ”¹äº†ä¸Šæ¬¡å¯è§†åŒ–çš„å†…å®¹ï¼Œä¸è¿‡åªæ˜¯ä¿®æ”¹äº†æ¥å£å†…å®¹ã€‚</p><p>ä»£ç å‡ ä¹æ²¡æ€ä¹ˆå†™æ³¨é‡Šï¼Œè¿™æ˜¯éœ€è¦æ”¹è¿›çš„åœ°æ–¹ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> homework </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”Hoeffdingä¸ç­‰å¼</title>
      <link href="/2018/08/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Hoeffding%E4%B8%8D%E7%AD%89%E5%BC%8F/"/>
      <url>/2018/08/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94Hoeffding%E4%B8%8D%E7%AD%89%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>è¿™æ¬¡ç”¨éœå¤«ä¸ä¸ç­‰å¼æ¥è¯æ˜å­¦ä¹ çš„å¯è¡Œæ€§ã€‚é¦–å…ˆè¦è¯´æ˜ä¸€ä¸ªå®šç†ï¼Œå«åšâ€œNo Free Lunchâ€å®šç†ã€‚å¦‚æœçœŸæ˜¯éœ€è¦é¢„æµ‹çš„å€¼æ˜¯å®Œå…¨éšæœºçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ— è®ºæœ€åå»ºç«‹ä¸€ä¸ªä»€ä¹ˆæ ·çš„æ¨¡å‹ï¼Œè¯¯å·®æœŸæœ›éƒ½æ˜¯ä¸€è‡´çš„ã€‚è¿™æ ·å­¦ä¹ ä¼¼ä¹æ˜¯ä¸å¯è¡Œçš„ã€‚<a id="more"></a>ä½†æ˜¯å®é™…ä¸Šï¼Œäº‹ç‰©éƒ½æœ‰è‡ªå·±çš„è§„å¾‹ã€‚æˆ‘ä»¬å¯ä»¥å¥—ç”¨éœå¤«ä¸ä¸ç­‰å¼æ¥è¯´æ˜ï¼Œåœ¨æŸä¸ªéªŒè¯é›†ä¸Šè¿™ä¸ªæ¨¡å‹çš„å‡†ç¡®ç‡æ˜¯pï¼Œé‚£ä¹ˆåœ¨æ€»ä½“ä¸Šå®ƒé¢„æµ‹çš„å®é™…å‡†ç¡®ç‡å¾ˆå¤§å¯èƒ½ä¸pæ˜¯ç›¸å·®ä¸å¤§çš„ï¼ˆProbably Approximately Correctï¼‰ã€‚</p><p>Hoeffdingä¸ç­‰å¼ï¼š$P[\nu  - \upsilon|&gt; \epsilon ] \leq 2 e^{-2\epsilon ^2N}$</p><p>å…¶ä¸­$\nu$æ˜¯æˆ‘ä»¬æµ‹çš„æœŸæœ›ï¼Œè€Œ$\upsilon$æ˜¯çœŸå®æœŸæœ›ï¼Œè¿™æ˜¯æœªçŸ¥çš„ï¼ŒNæ˜¯æµ‹è¯•æŠ½å–çš„æ ·æœ¬æ•°é‡ï¼Œè€Œ$\epsilon$æ˜¯æˆ‘ä»¬å¯ä»¥å®¹å¿çš„è¯¯å·®å€¼ï¼Œå› æ­¤å®é™…ä¸Šä¹Ÿå°±æ˜¯$\upsilon âˆˆ [\nu - \epsilon , \nu + \epsilon]$çš„æ¦‚ç‡æ˜¯å¤§äºç­‰äº$2 e^{-2\epsilon ^2N}$çš„ï¼Œä¹Ÿå°±æ˜¯ç½®ä¿¡åŒºé—´ä¸ç½®ä¿¡åº¦çš„å…³ç³»ã€‚</p><p>å› æ­¤éœå¤«ä¸ä¸ç­‰å¼å¯¹ç‹¬ç«‹éšæœºå˜é‡çš„å’Œä¸å…¶æœŸæœ›å€¼åå·®çš„æ¦‚ç‡ä¸Šé™ã€‚</p><p>ä»å¦ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥è¯´ï¼šå‡å¦‚æˆ‘ä»¬æœ‰ä¸€ä¸ªç½å­é‡Œè£…äº†çº¢çƒå’Œç»¿çƒï¼Œæˆ‘ä»¬æƒ³è¦æµ‹çš„æ˜¯çº¢çƒçš„å æ¯”ã€‚æ¯æ¬¡å–å‡ºNä¸ªçƒï¼Œåˆ™è¿™Nä¸ªçƒä¸­çº¢çƒå æ¯”ä¸º$\nu$ï¼Œè€Œæ€»ä½“çº¢çƒå æ¯”ä¸º$\upsilon$ï¼Œæˆ‘ä»¬å‡è®¾å¯ä»¥æ¥å—çš„è¯¯å·®èŒƒå›´ä¸º$\epsilon$ï¼Œé‚£ä¹ˆè¿™å‡ ä¸ªé‡æ»¡è¶³éœå¤«ä¸ä¸ç­‰å¼ã€‚æ€»ä½“çƒä¸­å–å‡ºNä¸ªæœ‰mä¸­ä¸åŒçš„ç»„åˆï¼Œè€Œè¿™äº›ç»„åˆä¸­çº¢çƒå æ¯”è¯¯å·®å¾ˆå¤§çš„ç»„åˆå äº†æ‰€æœ‰ç»„åˆçš„æ¯”é‡æ˜¯å°äº$2 e^{-2\epsilon ^2N}$çš„ã€‚è€Œè¿™äº›è¯¯å·®å¾ˆå¤§çš„æ ·æœ¬ç»„åˆå°±æ˜¯æ‰€è°“çš„Bad Dataã€‚å› ä¸ºä»å®ƒæµ‹å‡ºæ¥çš„å€¼ä¸å®é™…å€¼åå·®å¾ˆå¤§ã€‚</p><p>ä»å¦ä¸€æ–¹é¢æ¥è®²ï¼Œå¦‚æœæˆ‘ä»¬æµ‹è¯•çš„æ•°ç›®è¶³å¤Ÿå¤šï¼Œä¾ç„¶å¾ˆæœ‰å¯èƒ½é€‰åˆ°é”™è¯¯æ ·æœ¬ã€‚å°±åƒä½ æˆ‘æ˜¯å¤©æ‰çš„æ¦‚ç‡å¾ˆå°ï¼Œä½†è¿™ä¸ªä¸–ç•Œæ€»ä¼šå‡ºç°å¤©æ‰ã€‚ç”¨éœå¤«ä¸ä¸ç­‰å¼æ¥è¯´ï¼Œå‡å¦‚æˆ‘ä»¬æµ‹è¯•äº†tæ¬¡ï¼Œé‚£ä¹ˆå‡ºç°ä¸€æ¬¡bad data çš„æ¦‚ç‡æ˜¯å°äº$2te^{-2\epsilon ^2N}$ï¼Œä¸ºä»€ä¹ˆå¯ä»¥ç®€å•ç›¸åŠ ï¼Ÿå¦‚ä¸‹ï¼š</p><p>å‡å¦‚ä¸€ä»¶äº‹å‘ç”Ÿçš„æ¦‚ç‡æ˜¯pï¼Œé‚£ä¹ˆnæ¬¡å®ƒè‡³å°‘å‘ç”Ÿä¸€æ¬¡çš„æ¦‚ç‡æ˜¯$1-(1-p)^n$ã€‚</p><p> ä»¤ $g(p) = 1- (1-p)^n - np, gâ€™(p) = n(1-p)^{n-1} - n = n[(1-p)^{n-1} - 1];$<br> ä»¤$gâ€™(p) = 0$,åˆ™$p = 0;$<br> å½“p$âˆˆ[0,1]$æ—¶å€™ï¼Œ $gâ€™(p) \leq 0, g(0) = 0$,æ‰€ä»¥$g(p)\leq 0$;<br> å› æ­¤ $1- (1-p)^n \leq np$</p><p>ä¸Šè¿°æ˜¯ä¸€ä¸ªç®€å•çš„è¯æ˜ï¼Œè€Œå®é™…ä¸Šç‹¬ç«‹éšæœºå˜é‡è‡³å°‘å‘ç”Ÿä¸€ä»¶çš„æ¦‚ç‡æ˜¯å°äºå®ƒä»¬å„è‡ªå‘ç”Ÿçš„æ¦‚ç‡ä¹‹å’Œçš„ã€‚åœ¨æ¦‚ç‡è®ºä¸­è¿™ç›¸å½“äºæ˜¯ä¸ªå¸¸è¯†ã€‚</p><p>åº”ç”¨åˆ°æœºå™¨å­¦ä¹ å½“ä¸­ï¼Œå¦‚æœHæ˜¯æœ‰é™çš„ï¼Œä¹Ÿå°±æ˜¯æˆ‘ä»¬å¯é€‰å‡è¯´ä¸ªæ•°æ˜¯æœ‰é™çš„ï¼Œåªè¦é€‰å–è¶³å¤Ÿå¤§çš„æµ‹è¯•æ ·æœ¬é‡ï¼ˆHoeffdingä¸ç­‰å¼ä¸­çš„Nè¶³å¤Ÿå¤§ï¼‰ï¼Œå‡ºç°é”™è¯¯ä¼°è®¡çš„æ¦‚ç‡ä¾ç„¶æ˜¯éå¸¸å°çš„ï¼Œè¯¥test datasetæœ‰å¾ˆå¤§çš„æ¦‚ç‡å¯¹æ¯ä¸ªå‡è¯´éƒ½èƒ½æ­£ç¡®è¯„ä¼°å®ƒåœ¨æ€»ä½“æ ·æœ¬ä¸Šçš„æ€§èƒ½ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€‰æ‹©ä¸€ä¸ªè¡¨ç°æœ€å¥½çš„æ¥å½“åšgï¼Œå®ƒå¯¹å¤§å¤šæ•°æ ·æœ¬éƒ½ä¼šä½¿ç”¨ï¼Œç›¸æ¯”ä¹‹ä¸‹ä¹Ÿå°±æ›´æ¥è¿‘fã€‚</p><p>è¿™å°±è¯´æ˜å­¦ä¹ æ˜¯å¯è¡Œçš„ã€‚</p><p>è¿™é‡Œè¿˜æœ‰ä¸ªé—®é¢˜ï¼šå¦‚æœHçš„ä¸ªæ•°æ˜¯æ— é™çš„å‘¢ï¼Ÿ</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> mathematics </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Fibonacciæ•°åˆ—â€”â€”å¿«é€ŸçŸ©é˜µå¹‚</title>
      <link href="/2018/08/01/Fibonacci%E6%95%B0%E5%88%97%E2%80%94%E2%80%94%E5%BF%AB%E9%80%9F%E7%9F%A9%E9%98%B5%E5%B9%82/"/>
      <url>/2018/08/01/Fibonacci%E6%95%B0%E5%88%97%E2%80%94%E2%80%94%E5%BF%AB%E9%80%9F%E7%9F%A9%E9%98%B5%E5%B9%82/</url>
      
        <content type="html"><![CDATA[<p>ä»Šå¤©æƒ³èµ·æ¥ä¹‹å‰ä¸€ä¸ªojé¢˜ç›®ï¼Œæ˜¯æ±‚ç±»ä¼¼ä¸æ–æ³¢é‚£å¥‘æ•°åˆ—ä¸€ä¸ªæ•°åˆ—çš„ç¬¬Nä½ã€‚é‚£æ—¶å€™æ¥è§¦åˆ°ä¸€ä¸ªç®—æ³•å«å¿«é€ŸçŸ©é˜µå¹‚ã€‚</p><p>åœ¨è¿™é‡Œæˆ‘å°±ç”¨æ–æ³¢é‚£å¥‘æ•°åˆ—çš„åˆ—å­æ¥ç®€å•è¯´æ˜ä¸€ä¸‹å¦‚ä½•ç”¨å¿«é€ŸçŸ©é˜µå¹‚æ¥è§£å†³è¿™ä¸ªé¢˜ç›®ã€‚<br><a id="more"></a></p><p>Fibonacciæ•°åˆ—å®šä¹‰ï¼š$F(0) = 1, F(1) = 1, F(2) = 1, F(3) = 2, â€¦â€¦ F(n) = F(n-1)+F(n-2)$</p><p>é¦–å…ˆè¯´æ˜ä¸€ä¸‹ï¼Œå› ä¸ºæ–æ³¢é‚£å¥‘æ•°åˆ—å¢é•¿é€Ÿåº¦éå¸¸è¿…é€Ÿï¼Œå¾—åˆ°çš„æ•°å­—å¯èƒ½è¿‡å¤§ï¼Œå› æ­¤æˆ‘ä»¬å°†ç»“æœå¯¹10000007ï¼ˆ$10^7+7$ï¼‰å–ä½™æ¥è¿›è¡Œå¯¹æ¯”ã€‚</p><h2 id="æœ€å¤©çœŸçš„åšæ³•æ˜¯ç”¨é€’å½’æ¥è§£å†³ï¼š"><a href="#æœ€å¤©çœŸçš„åšæ³•æ˜¯ç”¨é€’å½’æ¥è§£å†³ï¼š" class="headerlink" title="æœ€å¤©çœŸçš„åšæ³•æ˜¯ç”¨é€’å½’æ¥è§£å†³ï¼š"></a>æœ€å¤©çœŸçš„åšæ³•æ˜¯ç”¨é€’å½’æ¥è§£å†³ï¼š</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">long</span> <span class="keyword">long</span> <span class="title">fibNaive</span><span class="params">(<span class="keyword">long</span> <span class="keyword">long</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(n == <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">return</span> (fibNaive(n<span class="number">-1</span>)%d+fibNaive(n<span class="number">-2</span>)%d)%d;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>ä¸ç”¨è¯´äº†ï¼Œç®—æ³•ç¬¬ä¸€æ­¥å°±ä¼šä»‹ç»è¿™ä¸ªåä¾‹ï¼Œæ¥è¯´æ˜é€’å½’æ•ˆç‡ä¸ä¸€å®šä¼šé«˜(ä»–çš„ç®—æ³•çš„è¿è¡Œæ—¶é—´éšnçš„å¢é•¿ç±»ä¼¼ä¸Fibonacciæ•°åˆ—çš„å¢é•¿)ã€‚å®é™…ä¸Šè¿™ä¸ªåšæ³•åˆ°n = 40çš„æ—¶å€™å°±å·²ç»å¯ä»¥è®©äººç­‰çš„æœ‰ç‚¹æ€¥äº†ã€‚</p><h2 id="ç„¶åæ­£å¸¸çš„åšæ³•æ˜¯ç”¨ç®€å•çš„å¾ªç¯"><a href="#ç„¶åæ­£å¸¸çš„åšæ³•æ˜¯ç”¨ç®€å•çš„å¾ªç¯" class="headerlink" title="ç„¶åæ­£å¸¸çš„åšæ³•æ˜¯ç”¨ç®€å•çš„å¾ªç¯"></a>ç„¶åæ­£å¸¸çš„åšæ³•æ˜¯ç”¨ç®€å•çš„å¾ªç¯</h2><p>ç”¨ä¸¤ä¸ªæ•°æ¥ä»£è¡¨ä¹‹å‰çš„ä¸¤ä¸ªå€¼ï¼Œæ±‚å‡ºæ–°å€¼åç»§ç»­ä¾æ¬¡æ›´æ–°å‰ä¸¤ä¸ªå€¼ï¼Œç›´åˆ°å¾—åˆ°æ­£ç¡®çš„ç»“æœï¼š</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">long</span> <span class="keyword">long</span> <span class="title">fibNormal</span><span class="params">(<span class="keyword">long</span> <span class="keyword">long</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(n == <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>; </span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> last1 = <span class="number">0</span>,last2 = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> now;</span><br><span class="line">    n--;</span><br><span class="line">    <span class="keyword">while</span>(n--)</span><br><span class="line">    &#123;</span><br><span class="line">        now = (last1%d+last2%d)%d;</span><br><span class="line">        last1 = last2;</span><br><span class="line">        last2 = now;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> now;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>è¿™ä¸ªç®—æ³•æ—¶é—´å¤æ‚åº¦æ˜¯$O(n)$ã€‚$O(n)$å·²ç»æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å¤æ‚åº¦äº†ï¼Œé‚£è¿˜æœ‰æ²¡æœ‰åŠæ³•ç»§ç»­åŠ å¿«è¿™ä¸ªè¿‡ç¨‹ï¼Ÿï¼Ÿ</p><h2 id="å¿«é€ŸçŸ©é˜µå¹‚"><a href="#å¿«é€ŸçŸ©é˜µå¹‚" class="headerlink" title="å¿«é€ŸçŸ©é˜µå¹‚"></a>å¿«é€ŸçŸ©é˜µå¹‚</h2><p>è§‚å¯Ÿæ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°å®ƒä»¬å¯ä»¥è¢«å†™æˆä¸‹é¢çš„æ ·å­ï¼š</p><script type="math/tex; mode=display">F(N) = F(N-1) + F(N-2)F(N-1) = F(N-1)+0*F(N-2)</script><p>ä¸Šé¢çš„å¼å­å¯ä»¥å†™æˆçŸ©é˜µå½¢å¼ï¼š</p><script type="math/tex; mode=display">\left [\begin{matrix}F(N)\\F(N-1)\end{matrix}\right ] =\begin{bmatrix}1&1\\1&0\end{bmatrix}\begin{bmatrix}F(N-1)\\F(N-2)\end{bmatrix}</script><p>ä¸æ–­é‡å¤ä¸Šé¢è¿‡ç¨‹ï¼Œå¾€åç»§ç»­å±•å¼€ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">\left [\begin{matrix}F(N)\\F(N-1)\end{matrix}\right ] ={\begin{bmatrix}1&1\\1&0\end{bmatrix}}^{n-1}\begin{bmatrix}F(1)\\F(0)\end{bmatrix}</script><p>å› æ­¤æˆ‘ä»¬å¯ä»¥æŠŠé‡ç‚¹æ”¾åˆ°æ€ä¹ˆæ¥æ±‚ä¸­é—´è¿™ä¸ªçŸ©é˜µçš„å¹‚ã€‚è€Œå¿«é€ŸçŸ©é˜µå¹‚çš„æ€æƒ³ä¹Ÿå¾ˆç®€å•ï¼Œå°±ç±»ä¼¼ä¸å¯¹äºæ•°å­—çš„å¹‚çš„æ±‚æ³•ä¸€è‡´ã€‚æ¯”å¦‚ï¼š$X^9 = X^8 \cdot X$,è€Œ$X^8 = (X^4)^2 = ((X^2)^2)^2$ï¼Œå› æ­¤éœ€è¦3+1æ¬¡ä¹˜æ³•å°±å¯ä»¥ç®—å‡ºæ¥8æ¬¡å¹‚ï¼Œå®¹æ˜“çœ‹å‡ºæ¥å¿«é€ŸçŸ©é˜µå¹‚çš„æ—¶é—´å¤æ‚åº¦æ˜¯$O(\log (n))$ã€‚<br>å› æ­¤åˆ©ç”¨å¿«é€ŸçŸ©é˜µå¹‚ï¼Œå¯ä»¥å°†æ–æ³¢é‚£å¥‘æ•°åˆ—çš„æ±‚æ³•è¿›ä¸€æ­¥åŠ é€Ÿã€‚</p><p>è‡³äºå¦‚ä½•å®ç°å°±æ˜¯ç»†èŠ‚é—®é¢˜äº†ï¼Œéœ€è¦æ³¨æ„çš„æ—¶å€™æ˜¯ä¹˜æ³•å–ä½™æ•°çš„æ—¶å€™ï¼Œä¸¤æ–¹éƒ½å°äº10000007,ä¹˜ç§¯ä¾ç„¶å¯èƒ½ä¼šè¶…è¿‡intçš„èŒƒå›´ï¼ˆ10000007*10000007ï¼‰ï¼Œå¯¼è‡´å‡ºé”™ï¼Œå› æ­¤æˆ‘åœ¨è¿™é‡Œé€‰æ‹©long longç±»å‹ï¼Œè¿™æ ·å¯ä»¥ä¿è¯ç»“æœçš„æ­£ç¡®æ€§ã€‚</p><p>å®ç°åˆ†ä¸ºå‡ æ­¥1ï¼šå®šä¹‰çŸ©é˜µä¹˜æ³•ï¼š<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">long</span> <span class="keyword">long</span>&gt;&gt; Matrix;</span><br><span class="line"><span class="keyword">long</span> <span class="keyword">long</span> d = <span class="number">10000007</span>;</span><br><span class="line"><span class="function">Matrix <span class="title">m_mul</span><span class="params">(<span class="keyword">const</span> Matrix &amp;m,<span class="keyword">const</span> Matrix &amp;n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//check(m,n);</span></span><br><span class="line">    Matrix result = <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">long</span> <span class="keyword">long</span>&gt;&gt;(m.size(),<span class="built_in">vector</span>&lt;<span class="keyword">long</span> <span class="keyword">long</span>&gt;(n[<span class="number">0</span>].size()));</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">long</span> <span class="keyword">long</span> i = <span class="number">0</span>;i!=m.size();++i)</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">long</span> <span class="keyword">long</span> j = <span class="number">0</span>;j!=n[<span class="number">0</span>].size();++j)</span><br><span class="line">    &#123;</span><br><span class="line">         <span class="keyword">long</span> <span class="keyword">long</span> temp = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">long</span> <span class="keyword">long</span> k = <span class="number">0</span>;k!=n.size();++k )</span><br><span class="line">         temp = ((m[i][k]*n[k][j])%d + temp%d)%d;</span><br><span class="line">         result[i][j] = temp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p><p>ç¬¬äºŒï¼Œå®šä¹‰helpå‡½æ•°ï¼Œä¸“é—¨å¯¹çŸ©é˜µçš„å¹‚ä¸º2çš„æ•´æ•°æ¬¡å¹‚æ¥è®¡ç®—ï¼š<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> <span class="function">Matrix <span class="title">help</span><span class="params">(<span class="keyword">const</span> Matrix &amp; m,<span class="keyword">long</span> <span class="keyword">long</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    Matrix result;</span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> m;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span>(n == <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> m_mul(m,m);</span><br><span class="line">    result = help(m,n/<span class="number">2</span>);</span><br><span class="line">     <span class="keyword">return</span> m_mul(result,result);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>ç¬¬ä¸‰æ­¥ï¼Œå®é™…çš„quickMartrixPowerå‡½æ•°ï¼Œå®ƒå®é™…ä¸Šä¼šå°†næ¬¡å¹‚æ‹†æ•£ä¸º2çš„æ•´æ•°æ¬¡å¹‚ä¹‹å’Œï¼Œå®é™…å®ç°å°†nç”¨äºŒè¿›åˆ¶è¡¨ç¤ºï¼Œå¦‚9 = (1001)$_b$ã€‚</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Matrix <span class="title">quickMatrixPower</span><span class="params">(<span class="keyword">const</span> Matrix &amp;m,<span class="keyword">long</span> <span class="keyword">long</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//check(m);</span></span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> np = <span class="number">1</span>;</span><br><span class="line">    Matrix result = <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">long</span> <span class="keyword">long</span>&gt;&gt; (m.size(),<span class="built_in">vector</span>&lt;<span class="keyword">long</span> <span class="keyword">long</span>&gt;(m.size()));</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">long</span> <span class="keyword">long</span> i = <span class="number">0</span>;i!=m.size();++i)</span><br><span class="line">    result[i][i] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span>(n!=<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(n&amp;<span class="number">1</span>)</span><br><span class="line">        result = m_mul(result,help(m,np));</span><br><span class="line">        n = n &gt;&gt; <span class="number">1</span>;</span><br><span class="line">        np = np&lt;&lt;<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>æœ€åç”¨fibå‡½æ•°å°è£…èµ·æ¥ï¼š</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">long</span> <span class="keyword">long</span> <span class="title">fib</span><span class="params">(<span class="keyword">long</span> <span class="keyword">long</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">       &#123;</span><br><span class="line">           Matrix start = &#123;&#123;<span class="number">1</span>,<span class="number">1</span>&#125;,&#123;<span class="number">1</span>,<span class="number">0</span>&#125;&#125;;</span><br><span class="line">           Matrix m = quickMatrixPower(start,n<span class="number">-1</span>);</span><br><span class="line">           <span class="keyword">return</span> m[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line">       &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>æœ€åç”¨mainå‡½æ•°åˆ©ç”¨clockå‡½æ•°è¿›è¡Œæ—¶é—´æµ‹è¯•<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> n,result;</span><br><span class="line">    <span class="keyword">double</span> start,end;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span>&gt;&gt;n)</span><br><span class="line">    &#123;</span><br><span class="line">    start = clock();</span><br><span class="line">    result = fib(n);</span><br><span class="line">    end = clock();</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;result&lt;&lt;<span class="string">" "</span>&lt;&lt;end-start&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    start = clock();</span><br><span class="line">    result = fibNormal(n);</span><br><span class="line">    end = clock();</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;result&lt;&lt;<span class="string">" "</span>&lt;&lt;end-start&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    start = clock();</span><br><span class="line">    <span class="keyword">if</span>(n&lt;<span class="number">45</span>)</span><br><span class="line">    &#123;</span><br><span class="line">    result = fibNaive(n);</span><br><span class="line">    end = clock();</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;result&lt;&lt;<span class="string">" "</span>&lt;&lt;end-start&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>è¾“å‡ºç¬¬ä¸€ä¸ªä¸ºç»“æœï¼Œç¬¬äºŒä¸ªæ˜¯è¿è¡Œçš„clockå·®å€¼ï¼Œç»“æœå¦‚ä¸‹ï¼š<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">è¾“å…¥ï¼š<span class="number">40</span></span><br><span class="line"><span class="number">2334085</span> <span class="number">0</span></span><br><span class="line"><span class="number">2334085</span> <span class="number">0</span></span><br><span class="line"><span class="number">2334085</span> <span class="number">6956</span></span><br><span class="line">è¾“å…¥ï¼š<span class="number">1000000</span><span class="comment">//æ­¤æ—¶naiveçš„ç®—æ³•å·²ç»æ— æ³•æ±‚å‡ºæ¥</span></span><br><span class="line"><span class="number">9640841</span> <span class="number">0</span></span><br><span class="line"><span class="number">9640841</span> <span class="number">19</span></span><br><span class="line">è¾“å…¥ï¼š<span class="number">100000000</span></span><br><span class="line"><span class="number">129680</span> <span class="number">0</span></span><br><span class="line"><span class="number">129680</span> <span class="number">3295</span></span><br></pre></td></tr></table></figure></p><p>å¯ä»¥çœ‹åˆ°å¿«é€ŸçŸ©é˜µå¹‚ç®—æ³•åœ¨æ•°æ®é‡å¾ˆå¤§çš„æ—¶å€™å¾ˆç‰›é€¼ã€‚<br>ä¸è¿‡ï¼Œæ–æ³¢é‚£å¥‘æ•°åˆ—è¿˜æœ‰ä¸ªå…¬å¼ï¼š</p><script type="math/tex; mode=display">F(n) = \frac{1}{\sqrt 5}{\left [ {\left ( \frac {1+\sqrt 5}{2} \right )}^n - {\left ( \frac {1-\sqrt 5}{2} \right )}^n  \right ]}</script><p>æ‰€ä»¥å­¦è®¡ç®—æœºä¸å¦‚å­¦æ•°å­¦å•Šï¼</p>]]></content>
      
      
      <categories>
          
          <category> ç®—æ³• </category>
          
      </categories>
      
      
        <tags>
            
            <tag> algorithm </tag>
            
            <tag> Matrix </tag>
            
            <tag> code </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”PLAç®—æ³•å®ç°ä¸å¯è§†åŒ–</title>
      <link href="/2018/07/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94PLA%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
      <url>/2018/07/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94PLA%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<p>ä¸Šæ¬¡ä¸»è¦æ˜¯è¯æ˜äº†PLAç®—æ³•çš„å¯è¡Œæ€§ï¼Œè¿™æ¬¡ç”¨æ¥å®ç°PLAç®—æ³•ï¼Œå¹¶ä¸”å®ç°å¯è§†åŒ–ã€‚<br>è¿™ä¸ªç®—æ³•çš„å®ç°æ˜¯æ¯”è¾ƒç®€å•çš„ï¼Œæ¯”è¾ƒéš¾çš„éƒ¨åˆ†åœ¨äºè¦è€ƒè™‘å¯è§†åŒ–ã€‚<br><a id="more"></a><br>æˆ‘é€‰æ‹©pythonæ¥å®ç°è¿™ä¸ªç®—æ³•ï¼ŒåŒæ—¶åˆ©ç”¨äº†matplotlibæ¥è¿›è¡Œå›¾å½¢çš„ç»˜åˆ¶ã€‚<br>ä¸ºäº†å¯è§†åŒ–æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦çš„å°±ä¸èƒ½æ˜¯ä»…ä»…å®ç°é‚£ä¹ˆç®€å•å—ï¼Œè€Œä¸”è¿˜è¦è€ƒè™‘åˆ°å¯è§†åŒ–ä¹‹åçš„æ¸…æ™°ä¸ç¾è§‚ã€‚å› æ­¤è¿™éƒ¨åˆ†çš„ä»£ç ä¸»è¦åˆ†æˆ3ä¸ªéƒ¨åˆ†ï¼š</p><h2 id="éšæœºç”Ÿæˆæ•°æ®"><a href="#éšæœºç”Ÿæˆæ•°æ®" class="headerlink" title="éšæœºç”Ÿæˆæ•°æ®"></a>éšæœºç”Ÿæˆæ•°æ®</h2><p>æ•°æ®çš„ç”Ÿæˆä¸€å®šæ˜¯è¦å±€é™åœ¨æŸä¸ªèŒƒå›´å†…ï¼Œä¸ºäº†ç®€ä¾¿æˆ‘é€‰æ‹©çš„æ•°æ®ç‰¹å¾é‡èŒƒå›´åœ¨0ï¼Œ20ä¹‹é—´ã€‚è€Œä¸€ç»´æ•°æ®è¾ƒä¸ºç®€å•ï¼Œé«˜ç»´æ•°æ®ç”»ä¸å‡ºæ¥ï¼Œå› æ­¤ç”Ÿæˆæ•°æ®åº”è¯¥æ˜¯äºŒç»´æˆ–è€…ä¸‰ç»´çš„ï¼Œä»¥ä¾¿äºå¯è§†åŒ–ã€‚ä¸ºäº†ç®€ä¾¿ï¼Œæˆ‘é€‰æ‹©ç”ŸæˆäºŒç»´æ•°æ®ã€‚<br>åŒæ—¶è¿˜è¦ç”Ÿæˆä¸€ç»„å‚æ•°ï¼Œä½œä¸º$W_f$ï¼Œä¹Ÿå°±æ˜¯æœ€åˆçš„è§„åˆ™ï¼Œè¿™é‡Œéœ€è¦æ³¨æ„ï¼Œéšæœºç”Ÿæˆçš„å‚æ•°ç¡®å®šçš„åˆ†å‰²çº¿å¯èƒ½ä¸ä¼šç»è¿‡ä¸Šè¿°èŒƒå›´çš„æ•°æ®ï¼Œè¿™æ ·å¯¼è‡´æ‰€æœ‰çš„æ ·æœ¬éƒ½å½’ä¸ºä¸€ç±»ï¼Œè¿™å°±å¤±å»äº†å¯è§†åŒ–çš„æ„ä¹‰ï¼Œå› ä¸ºç”Ÿæˆå‚æ•°æ—¶ï¼Œæˆ‘é€‰æ‹©äº†åœ¨èŒƒå›´å†…éšæœºç”Ÿæˆä¸¤ä¸ªç‚¹ï¼Œç”¨è¿™ä¸¤ä¸ªç‚¹æ¥ç¡®å®šåˆ†å‰²çº¿ï¼Œå†è®¡ç®—å‡ºå¯¹åº”çš„å‚æ•°å‡ºæ¥ã€‚</p><h2 id="PLAç®—æ³•"><a href="#PLAç®—æ³•" class="headerlink" title="PLAç®—æ³•"></a>PLAç®—æ³•</h2><p>plaç®—æ³•æ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼Œå‚æ•°åˆå§‹è®¾ä¸º0ï¼Œç„¶åæ¯æ¬¡é‡åˆ°ä¸€ä¸ªåç‚¹ï¼Œå°±å¼€å§‹æ›´æ­£ï¼Œç›´åˆ°æ²¡æœ‰åç‚¹ã€‚æˆ‘ä»¬éœ€è¦ä¿è¯ä¼ å…¥çš„æ•°æ®æ˜¯çº¿æ€§å¯åˆ†çš„ã€‚</p><h2 id="å¯è§†åŒ–"><a href="#å¯è§†åŒ–" class="headerlink" title="å¯è§†åŒ–"></a>å¯è§†åŒ–</h2><p>å¯è§†åŒ–ä½¿ç”¨matplotlibæ¥å®ç°ï¼Œä½¿ç”¨ä¸¤ç§ä¸åŒçš„æ ‡å¿—ï¼ˆå°½é‡åŒºåˆ†é¢œè‰²ï¼Œå¦‚çº¢xä¸ç»¿oï¼‰æ¥åŒºåˆ†æ­£è´Ÿæ ·æœ¬ï¼Œåœ¨åæ ‡è½´ä¸Šæ ‡å‡ºï¼Œå¹¶ä¸”ç”¨å®çº¿æ¥ç»˜åˆ¶å®é™…çš„è§„åˆ™ï¼Œç”¨è™šçº¿æ¥ç»˜åˆ¶æˆ‘ä»¬ç®—æ³•å¾—åˆ°çš„è§„åˆ™ã€‚æœ€åå¯ä»¥å¾—åˆ°å¾ˆæ˜æ˜¾çš„å¯è§†åŒ–æ•ˆæœã€‚</p><h3 id="å¯è§†åŒ–ç»“æœ"><a href="#å¯è§†åŒ–ç»“æœ" class="headerlink" title="å¯è§†åŒ–ç»“æœ"></a>å¯è§†åŒ–ç»“æœ</h3><p>éšæœºæ ·æœ¬ä¸º20ä¸ªï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/%5BNJ07J%7E9%29%286ZV0%2846%28S%40%29LN.png" alt="20"><br>ä¿®æ­£æ¬¡æ•°ï¼š 3209</p><p>éšæœºæ ·æœ¬ä¸º50ä¸ªï¼š<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/NLYU%7EZSV%609S%29557%602RBG8%409.png" alt="50"></p><p>ä¿®æ­£æ¬¡æ•°ï¼š 2268</p><p>éšæœºæ ·æœ¬ä¸º100ä¸ªï¼š</p><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/71K%251Y%7B%7D3K1HMRY%5DR%60K%29%25E8.png" alt="100"></p><p>ä¿®æ­£æ¬¡æ•°ï¼š 4540</p><p>ä»å›¾ä¸­å¯ä»¥çœ‹åˆ°è™½ç„¶çº¢çº¿ä¸ä¸€å®šä¸è“çº¿é‡åˆï¼Œä½†æ˜¯ä¾ç„¶å¾ˆå¥½çš„åˆ†å‰²äº†æ ·æœ¬ã€‚å®é™…ä¸Šç›¸é‡åˆæ˜¯å¾ˆå›°éš¾çš„ï¼Œæ ·æœ¬è¶Šæ˜¯å¤šè¶Šæ›´æœ‰å¯èƒ½ç›¸ä¼¼ï¼Œå¦‚ä¸‹å›¾ï¼Œæ ·æœ¬æ¬¡æ•°æé«˜åˆ°1000ï¼Œæˆ‘ä»¬å¯ä»¥è¯´æ¨æ–­çš„è§„åˆ™ä¸åŸå…ˆçš„è§„åˆ™å·²ç»åŸºæœ¬ä¸€è‡´äº†ã€‚<br><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/EQPB1J%7B5LY%7B%5B4V%24%7BK0N6%29CU.png" alt="1000"></p><p>æˆ‘ä»¬ä»è¿™é‡Œçœ‹ä¸åˆ°ä¿®æ­£æ¬¡æ•°ä¸æ ·æœ¬ä¸ªæ•°ä¹‹é—´çš„å…³ç³»ï¼Œå› ä¸ºæœ¬æ¥ä»–ä»¬å…³ç³»å°±ä¸å¤Ÿå¤§ï¼Œç”šè‡³ä¸€å®šç¨‹åº¦ä¸Šå¯ä»¥è¯´æ˜¯â€è¿æ°”â€ï¼Œä½†æ˜¯ç®—æ³•ç»ˆç©¶ä¼šåœæ­¢ï¼Œç”±ä¸Šä¸€ç¯‡åšå®¢çš„è¯æ˜ä¹Ÿå¯çŸ¥é“ï¼Œå¦‚æœRä¸Pçš„æ¯”å€¼å¾ˆå°ï¼Œé‚£ä¹ˆå°±ç®—æ•°æ®å†å¤§ï¼Œä¹Ÿå¯ä»¥å¾ˆå¿«çš„å¾—åˆ°æƒ³è¦çš„è§„åˆ™ã€‚</p><p>ä¸‹é¢æ˜¯PLAå®ç°çš„ä»£ç ï¼š<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pla</span><span class="params">(datas)</span>:</span></span><br><span class="line">    size = len(datas)</span><br><span class="line">    <span class="keyword">if</span> size&lt;=<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    err_i = <span class="number">-1</span></span><br><span class="line"></span><br><span class="line">    dms = len(datas[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> dms == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    para = [<span class="number">0</span> <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>,dms)]</span><br><span class="line">    run_times = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line"></span><br><span class="line">        run_times+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, size):</span><br><span class="line">            p = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>, dms - <span class="number">1</span>):</span><br><span class="line">                p += para[x] * datas[i][x]</span><br><span class="line">            p += para[<span class="number">-1</span>]</span><br><span class="line">            <span class="keyword">if</span> p &lt;= <span class="number">0</span> <span class="keyword">and</span> datas[i][<span class="number">-1</span>] &gt; <span class="number">0</span> <span class="keyword">or</span> p &gt;= <span class="number">0</span> <span class="keyword">and</span> datas[i][<span class="number">-1</span>] &lt; <span class="number">0</span>:<span class="comment">#ignore datas[i][-1] == 0</span></span><br><span class="line">                err_i = i;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">if</span> err_i != <span class="number">-1</span>:</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">0</span>, dms - <span class="number">1</span>):</span><br><span class="line">                para[x] += datas[err_i][<span class="number">-1</span>] * datas[err_i][x]  <span class="comment"># update the parameters</span></span><br><span class="line">            para[<span class="number">-1</span>] += datas[err_i][<span class="number">-1</span>]</span><br><span class="line">            err_i = <span class="number">-1</span>;</span><br><span class="line">        <span class="keyword">else</span>:<span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [para,run_times]</span><br></pre></td></tr></table></figure></p><p>å…¨éƒ¨pythonä»£ç å¯ä»¥åœ¨<a href="https://github.com/MyEvolution/PLA" target="_blank" rel="noopener">PLA</a>æ‰¾åˆ°ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> code </tag>
            
            <tag> machine learning </tag>
            
            <tag> visualization </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>markdown+mathjaxæ˜¾ç¤ºå…¬å¼ï¼Œè‹¦é€¼çš„æ‰¾bugè¿‡ç¨‹ </title>
      <link href="/2018/07/29/markdown-mathjax%E6%98%BE%E7%A4%BA%E5%85%AC%E5%BC%8F%EF%BC%8C%E8%8B%A6%E9%80%BC%E7%9A%84%E6%89%BEbug%E8%BF%87%E7%A8%8B/"/>
      <url>/2018/07/29/markdown-mathjax%E6%98%BE%E7%A4%BA%E5%85%AC%E5%BC%8F%EF%BC%8C%E8%8B%A6%E9%80%BC%E7%9A%84%E6%89%BEbug%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>åœ¨è®°å½•è‡ªå·±å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œéš¾å…ä¼šé‡åˆ°å…¬å¼ï¼Œæˆªå›¾çš„åŠæ³•ä¸æ˜¯é•¿ä¹…ä¹‹è®¡ã€‚äºæ˜¯æˆ‘ç»™å®ƒåŠ ä¸Šäº†æ¸²æŸ“å…¬å¼çš„è¿‡ç¨‹ã€‚</p><p>æ•´ä¸ªè¿‡ç¨‹éå¸¸å´å²–ã€‚<br><a id="more"></a></p><p>å…¶å®å¾ˆç®€å•ï¼Œåœ¨æ™®é€šçš„é¡µé¢ä¸Šå¼•å…¥ä¸€ä¸ªjsæ–‡ä»¶ï¼Œå°±å¯ä»¥å°†latexä»£ç æ¸²æŸ“æˆå…¬å¼ã€‚ä¸ºäº†æ–¹ä¾¿ç¼–å†™ï¼Œæˆ‘ç›´æ¥åœ¨æ–‡ç« ä¸­å¼•å…¥äº†è¿™ä¸ªjsï¼Œåœ¨vs codeä¸Šæ¸²æŸ“æ˜¯éå¸¸æˆåŠŸçš„ã€‚ä½†æ˜¯åœ¨åšå®¢ä¸Šæ€ä¹ˆå¼„ä¹Ÿæ²¡æ³•æˆåŠŸã€‚ç™¾åº¦çš„è¯æœ‰å¾ˆå¤šç›¸å…³çš„åšå®¢æ¥ä»‹ç»æ€ä¹ˆå»åšï¼Œä¸»è¦æ˜¯å› ä¸ºhexoçš„æ¸²æŸ“å™¨ä¸mathjaxä¸­éƒ¨åˆ†è¯­æ³•æœ‰å†²çªã€‚æˆ‘ä¹Ÿå¤©çœŸçš„ç›¸ä¿¡æˆ‘çš„é—®é¢˜å°±æ˜¯è¿™äº›æ„æˆçš„ï¼Œåæ¥æˆ‘å‘ç°å³ä½¿æ˜¯æœ€ç®€å•çš„å…¬å¼ï¼ˆæ²¡æœ‰ä¸‹æ ‡ï¼‰ä¹Ÿæ— æ³•æ¸²æŸ“æˆåŠŸï¼Œæ­¤æ—¶æˆ‘çš„blogé‡Œå·²ç»åŠ äº†å¾ˆå¤šåˆ«çš„æ²¡ç”¨åŒ…äº†ï¼Œä¹Ÿåˆ é™¤äº†å¾ˆå¤šå¯èƒ½æœ‰ç”¨çš„åŒ…ï¼Œæ€ä¹ˆè®¾ç½®ä¹Ÿæ²¡åŠæ³•ã€‚</p><p>æ˜¨æ™šå°±çº ç»“äº†å¾ˆä¹…ï¼Œä»Šå¤©æ²¡åŠæ³•äº†é‡æ–°è£…äº†ä¸‹blogï¼Œå…ˆè§£å†³æœ€ç®€å•å…¬å¼æ˜¾ç¤ºçš„é—®é¢˜ï¼ˆç›´æ¥æ‰“å¼€nextä¸»é¢˜ä¸­çš„mathjaxå¼€å…³ï¼‰ï¼ŒæˆåŠŸäº†ï¼Œç„¶åæˆ‘é‡åˆ°çš„é—®é¢˜ï¼Œæ‰æ˜¯åˆ«äººé‡åˆ°çš„é‚£äº›é—®é¢˜ã€‚æ…¢æ…¢è§£å†³ï¼Œåæ¥å‘ç°è¿˜æ˜¯æœ‰éƒ¨åˆ†é”™è¯¯ï¼Œæœ‰ä¸¤å¤„é—®é¢˜ï¼š</p><p>1.è¿™æ¬¡æ˜¯latexä¸markdownçš„å†²çªï¼šmarkdownä¸­çš„*æ˜¯æ–œä½“ï¼Œå› æ­¤å°½é‡ä¹˜æ³•ä¸è¦ä½¿ç”¨å®ƒï¼Œå¯ä»¥ä½¿ç”¨{a \times b}ï¼ˆ$a \times b$ï¼‰æ¥ä»£æ›¿æ¥ä»£æ›¿ a*b,æˆ–è€…å°±æ˜¯å¯¹*è¿›è¡Œè½¬ä¹‰ï¼Œæˆ–è€…ç›´æ¥çœç•¥ã€‚</p><p>2.åœ¨ä½¿ç”¨èŒƒå¼æ—¶||a||ï¼Œä¼¼ä¹ç›´æ¥ä½¿ç”¨â€˜|â€™è¿™ä¸ªç¬¦å·ä¹Ÿä¼šå‡ºé”™ï¼Œæ€»ä¹‹æœ‰èŒƒå¼çš„åœ°æ–¹çš„å…¬å¼æ˜¾ç¤ºæ˜¯å¾ˆæ··ä¹±çš„ï¼Œå¯èƒ½ä¹Ÿæœ‰å†²çªï¼ˆåœ¨latexæ˜¯å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™ä¸ªç¬¦å·çš„ï¼‰ï¼Œå› æ­¤ä½¿ç”¨{\Vert a \Vert}ï¼ˆ${\Vert a \Vert}$ï¼‰æ¥ä»£æ›¿ã€‚</p><p>æœ€åï¼Œæµè§ˆå™¨å“åº”github pageçš„é€Ÿåº¦ä¸å¿«ï¼Œå¯èƒ½å·²ç»deployäº†ä½†æ˜¯ä¾ç„¶æ²¡æœ‰ä½“ç°å‡ºæ›´æ–°çš„ç»“æœã€‚</p><p>å“ï¼ŒæŠ˜è…¾äº†ä¸€å¤©ï¼Œæ„Ÿè§‰æ”¶è·è¿œè¿œä¸å¤ŸèŠ±è´¹çš„æ—¶é—´ï¼Œä¸è¿‡æˆ‘ç»å¸¸é‡åˆ°è¿™æ ·çš„é—®é¢˜ï¼Œæ‰¾bugçš„èƒ½åŠ›è¿˜æœ‰å¾…æé«˜ã€‚</p><h2 id="P-S-Markdownä¸latexå¸¸ç”¨è¯­æ³•"><a href="#P-S-Markdownä¸latexå¸¸ç”¨è¯­æ³•" class="headerlink" title="P.S. Markdownä¸latexå¸¸ç”¨è¯­æ³•"></a>P.S. Markdownä¸latexå¸¸ç”¨è¯­æ³•</h2><h3 id="markdownè¯­æ³•"><a href="#markdownè¯­æ³•" class="headerlink" title="markdownè¯­æ³•"></a>markdownè¯­æ³•</h3><ul><li>æ ‡é¢˜ï¼š</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ä¸€å·å­—ä½“ #</span><br><span class="line">## äºŒå·å­—ä½“ ## </span><br><span class="line">### ä¸‰å·å­—ä½“ ###</span><br></pre></td></tr></table></figure><h1 id="ä¸€å·å­—ä½“"><a href="#ä¸€å·å­—ä½“" class="headerlink" title="ä¸€å·å­—ä½“"></a>ä¸€å·å­—ä½“</h1><h2 id="äºŒå·å­—ä½“"><a href="#äºŒå·å­—ä½“" class="headerlink" title="äºŒå·å­—ä½“"></a>äºŒå·å­—ä½“</h2><h3 id="ä¸‰å·å­—ä½“"><a href="#ä¸‰å·å­—ä½“" class="headerlink" title="ä¸‰å·å­—ä½“"></a>ä¸‰å·å­—ä½“</h3><ul><li>åŠ ç²—ï¼š</li></ul><p>è¦åŠ ç²—çš„æ–‡å­—å·¦å³åˆ†åˆ«ç”¨ä¸¤ä¸ª*å·åŒ…èµ·æ¥</p><ul><li>æ–œä½“ï¼š</li></ul><p>è¦å€¾æ–œçš„æ–‡å­—å·¦å³åˆ†åˆ«ç”¨ä¸€ä¸ª*å·åŒ…èµ·æ¥</p><ul><li>æ–œä½“åŠ ç²—ï¼š</li></ul><p>è¦å€¾æ–œå’ŒåŠ ç²—çš„æ–‡å­—å·¦å³åˆ†åˆ«ç”¨ä¸‰ä¸ª*å·åŒ…èµ·æ¥</p><ul><li>åˆ é™¤çº¿ï¼š</li></ul><p>è¦åŠ åˆ é™¤çº¿çš„æ–‡å­—å·¦å³åˆ†åˆ«ç”¨ä¸¤ä¸ª~~å·åŒ…èµ·æ¥</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">**è¿™æ˜¯åŠ ç²—çš„æ–‡å­—**</span><br><span class="line">*è¿™æ˜¯å€¾æ–œçš„æ–‡å­—*`</span><br><span class="line">***è¿™æ˜¯æ–œä½“åŠ ç²—çš„æ–‡å­—***</span><br><span class="line">~~è¿™æ˜¯åŠ åˆ é™¤çº¿çš„æ–‡å­—~~</span><br></pre></td></tr></table></figure><p><strong>è¿™æ˜¯åŠ ç²—çš„æ–‡å­—</strong></p><p><em>è¿™æ˜¯å€¾æ–œçš„æ–‡å­—</em></p><p><strong><em>è¿™æ˜¯æ–œä½“åŠ ç²—çš„æ–‡å­—</em></strong></p><p><del>è¿™æ˜¯åŠ åˆ é™¤çº¿çš„æ–‡å­—</del></p><ul><li>å¼•ç”¨ï¼š</li></ul><p>åœ¨å¼•ç”¨çš„æ–‡å­—å‰åŠ &gt;å³å¯</p> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; å¼•ç”¨çš„å†…å®¹</span><br></pre></td></tr></table></figure><blockquote><p>å¼•ç”¨çš„å†…å®¹</p></blockquote><ul><li>åˆ†å‰²çº¿ï¼š<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">***</span><br></pre></td></tr></table></figure></li></ul><hr><hr><ul><li>è¶…é“¾æ¥ï¼š</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[name](url)</span><br></pre></td></tr></table></figure><p><a href="https://www.google.com" target="_blank" rel="noopener">Google</a></p><ul><li>å›¾ç‰‡ï¼š</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">![name](imgurl)</span><br></pre></td></tr></table></figure><p><img src="https://evolution-video.oss-cn-beijing.aliyuncs.com/images/IMG_8234.PNG" alt="name"></p><ul><li>åˆ—è¡¨ï¼š</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- åˆ—è¡¨å†…å®¹</span><br><span class="line">+ åˆ—è¡¨å†…å®¹</span><br><span class="line">* åˆ—è¡¨å†…å®¹</span><br></pre></td></tr></table></figure><pre><code>æ³¨æ„ï¼š- + * è·Ÿå†…å®¹ä¹‹é—´éƒ½è¦æœ‰ä¸€ä¸ªç©ºæ ¼</code></pre><ul><li>åˆ—è¡¨å†…å®¹</li></ul><ul><li>åˆ—è¡¨å†…å®¹</li></ul><ul><li><p>åˆ—è¡¨å†…å®¹</p></li><li><p>è¡¨æ ¼ï¼š</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">è¡¨å¤´|è¡¨å¤´|è¡¨å¤´</span><br><span class="line">---|:--:|---:</span><br><span class="line">å†…å®¹|å†…å®¹2|å†…å®¹</span><br><span class="line">å†…å®¹|å†…å®¹100|å†…å®¹</span><br></pre></td></tr></table></figure><p>ç¬¬äºŒè¡Œåˆ†å‰²è¡¨å¤´å’Œå†…å®¹ã€‚<br>æ–‡å­—é»˜è®¤å±…å·¦<br>-ä¸¤è¾¹åŠ ï¼šè¡¨ç¤ºæ–‡å­—å±…ä¸­<br>-å³è¾¹åŠ ï¼šè¡¨ç¤ºæ–‡å­—å±…å³<br>æ³¨ï¼šåŸç”Ÿçš„è¯­æ³•ä¸¤è¾¹éƒ½è¦ç”¨ | åŒ…èµ·æ¥ã€‚æ­¤å¤„çœç•¥</p><div class="table-container"><table><thead><tr><th>è¡¨å¤´</th><th style="text-align:center">è¡¨å¤´</th><th style="text-align:right">è¡¨å¤´</th></tr></thead><tbody><tr><td>å†…å®¹</td><td style="text-align:center">å†…å®¹2</td><td style="text-align:right">å†…å®¹</td></tr><tr><td>å†…å®¹</td><td style="text-align:center">å†…å®¹100</td><td style="text-align:right">å†…å®¹</td></tr></tbody></table></div><ul><li>ä»£ç ï¼š</li></ul><p><code>å•è¡Œä»£ç </code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">å¤šè¡Œä»£ç </span><br></pre></td></tr></table></figure></p><p>vscodeä¸­å‰é¢åŠ åˆ¶è¡¨ç¬¦è‡ªåŠ¨ä¸ºä»£ç æ ¼å¼ã€‚<br>å¯ä»¥åœ¨æ ‡è®°åæ ‡è®°å‡ºä»£ç è¯­è¨€ï¼Œç”¨æ¥é«˜äº®ï¼š<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"hello,world"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><h3 id="Latexè¯­æ³•"><a href="#Latexè¯­æ³•" class="headerlink" title="Latexè¯­æ³•"></a>Latexè¯­æ³•</h3><ul><li>å¸Œè…Šå­—æ¯ï¼š</li></ul><div class="table-container"><table><thead><tr><th>æ˜¾ç¤º</th><th>å‘½ä»¤</th><th>æ˜¾ç¤º</th><th>å‘½ä»¤</th></tr></thead><tbody><tr><td>Î±</td><td>\alpha</td><td>Î²</td><td>\beta</td></tr><tr><td>Î³</td><td>\gamma</td><td>Î´</td><td>\delta</td></tr><tr><td>Îµ</td><td>\epsilon</td><td>Î¶</td><td>\zeta</td></tr><tr><td>Î·</td><td>\eta</td><td>Î¸</td><td>\theta</td></tr><tr><td>Î¹</td><td>\iota</td><td>Îº</td><td>\kappa</td></tr><tr><td>Î»</td><td>\lambda</td><td>Î¼</td><td>\mu</td></tr><tr><td>Î½</td><td>\nu</td><td>Î¾</td><td>\xi</td></tr><tr><td>Ï€</td><td>\pi</td><td>Ï</td><td>\rho</td></tr><tr><td>Ïƒ</td><td>\sigma</td><td>Ï„</td><td>\tau</td></tr><tr><td>Ï…</td><td>\upsilon</td><td>Ï†</td><td>\phi</td></tr><tr><td>Ï‡</td><td>\chi</td><td>Ïˆ</td><td>\psi</td></tr><tr><td>Ï‰</td><td>\omega</td><td></td></tr></tbody></table></div><p>å‘½ä»¤é¦–å­—æ¯å¤§å†™åˆ™ä¸ºæ˜¾ç¤ºä¸ºå¤§å†™ã€‚</p><ul><li>å­—æ¯ä¿®é¥°ï¼š</li></ul><p>ä¸Šæ ‡ï¼š^</p><p>ä¸‹æ ‡ï¼š_</p><p>ä¸¾ä¾‹ï¼šC_n^2 å‘ˆç°ä¸º $C_n^2$</p><p>çŸ¢é‡ï¼š</p><p>\vec aå‘ˆç°ä¸º $\vec a$</p><p>\overrightarrow{xy}å‘ˆç°ä¸º$\overrightarrow{xy}$</p><ul><li>åˆ†ç»„:</li></ul><p>ä½¿ç”¨{}å°†å…·æœ‰ç›¸åŒç­‰çº§çš„å†…å®¹æ‰©å…¥å…¶ä¸­ï¼Œæˆç»„å¤„ç†</p><p> ä¸¾ä¾‹ï¼š10^{10}å‘ˆç°ä¸º$10^{10}$ï¼Œè€Œ10^10å‘ˆç°ä¸º$10^10$</p><ul><li>æ‹¬å·ï¼š<br>å°æ‹¬å·ï¼š()å‘ˆç°ä¸º()<br>ä¸­æ‹¬å·ï¼š[]å‘ˆç°ä¸º[]<br>å°–æ‹¬å·ï¼š\langle,\rangleå‘ˆç°ä¸º$\langle,\rangle$</li></ul><p>å¤§æ‹¬å·ä¸ºä¸åˆ†ç»„ç¬¦å·{}ç›¸åŒºåˆ«ï¼Œä½¿ç”¨è½¬ä¹‰å­—ç¬¦\</p><p>ä½¿ç”¨\left(æˆ–\right)ä½¿ç¬¦å·å¤§å°ä¸é‚»è¿‘çš„å…¬å¼ç›¸é€‚åº”ï¼›è¯¥è¯­å¥é€‚ç”¨äºæ‰€æœ‰æ‹¬å·ç±»å‹</p><p>(\frac{x}{y})å‘ˆç°ä¸º$(\frac{x}{y})$</p><p>è€Œ\left(\frac{x}{y}\right)å‘ˆç°ä¸º$\left(\frac{x}{y}\right)$</p><ul><li>æ±‚å’Œï¼š</li></ul><p>æ±‚å’Œç¬¦å·\sumæ˜¾ç¤ºä¸º$âˆ‘$</p><p>ä¸¾ä¾‹:\sum_{i=0}^n æ˜¾ç¤ºä¸º $\sum_{i=0}^n$</p><ul><li>æé™ï¼š</li></ul><p>æé™ç¬¦å·\limæ˜¾ç¤ºä¸º$lim$</p><p>ä¸¾ä¾‹:\lim_{x\to\infty} </p><p>$<br>\lim_{x\to\infty}<br>$</p><ul><li>ç§¯åˆ†ï¼š</li></ul><div class="table-container"><table><thead><tr><th>å‘½ä»¤</th><th>æ˜¾ç¤º</th></tr></thead><tbody><tr><td>\int</td><td>âˆ«</td></tr><tr><td>\iint</td><td>âˆ¬</td></tr><tr><td>\iiint</td><td>âˆ­</td></tr><tr><td>\iiiint</td><td>âˆ¬âˆ¬</td></tr><tr><td>\oint</td><td>âˆ®</td></tr></tbody></table></div><p>ä¸¾ä¾‹ï¼š\int_0^\infty{fxdx} æ˜¾ç¤ºä¸º<script type="math/tex">\int_0^\infty{f(x)dx}</script></p><ul><li>åˆ†å¼ï¼š</li></ul><p>\frac{å…¬å¼1}{å…¬å¼2}æ˜¾ç¤ºä¸º$\frac{å…¬å¼1}{å…¬å¼2}$</p><p>ä¸¾ä¾‹:\frac a b $\frac a b$</p><ul><li><p>æ ¹å·ï¼š<br>\sqrt[x]{y}æ˜¾ç¤ºä¸º$\sqrt[x]{y}$</p></li><li><p>å¸¸è§å‡½æ•°ï¼š</p></li></ul><p>\å‡½æ•°å</p><p>ä¸¾ä¾‹:\sin x,\ln x,\log_n^2 5,\max(A,B,C)æ˜¾ç¤ºä¸º</p><script type="math/tex; mode=display">\sin x,\ln x,\log_n^2,\max(A,B,C)</script><p>æš‚æ—¶å†™è¿™ä¹ˆäº›ï¼Œè¿˜æœ‰å¾ˆå¤šå…¶ä»–ç”¨æ³•ï¼Œéœ€è¦å¹³æ—¶çš„ç§¯ç´¯ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> åšå®¢å»ºè®¾ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> markdown </tag>
            
            <tag> LaTex </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>æœºå™¨å­¦ä¹ â€”â€”PLA</title>
      <link href="/2018/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94PLA/"/>
      <url>/2018/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E2%80%94%E2%80%94PLA/</url>
      
        <content type="html"><![CDATA[<p>å¯¹ç ”ç©¶ç”Ÿè¦è·Ÿçš„å¯¼å¸ˆè¿˜ä¸ç¡®å®šï¼Œæš‘å‡æ‰“ç®—å­¦ä¹ ç‚¹ä¸“ä¸šè¯¾ä¸è‹±è¯­ï¼Œä¹Ÿè¿Ÿè¿Ÿæ²¡æœ‰åšå¥½ã€‚æˆ‘æƒ³ä¸ç®¡å“ªä¸ªå®éªŒå®¤åº”è¯¥éƒ½ä¸ä¼šç¦»å¼€æœºå™¨å­¦ä¹ å§ã€‚å› æ­¤å¼€å§‹çœ‹è¿™æ–¹é¢çš„ä¸œè¥¿ã€‚</p><a id="more"></a><p>ä»Šå¤©æ¥è§¦äº†ä¸€ä¸ªç®—æ³•å«<strong>PLA</strong>ï¼ˆPercetron Learning Algorithmï¼‰,ç”¨æ¥åšçº¿æ€§åˆ†ç±»çš„ç®—æ³•ã€‚å®ƒåº”ç”¨çš„å‰ææ˜¯æ ·æœ¬é›†æ˜¯çº¿æ€§å¯åˆ†çš„ã€‚ç”¨äºŒç»´ç‰¹å¾å€¼çš„æ ·æœ¬æ¥ä¸¾ä¾‹å­ï¼š</p><p><img src="https://ss1.bdstatic.com/70cFuXSh_Q1YnxGkpoWK1HF6hhy/it/u=793659404,4189322929&amp;fm=27&amp;gp=0.jpg" alt="linear separable"></p><p>æˆ‘ä»¬è¦çš„å°±æ˜¯é€šè¿‡ç®—æ³•æ¥æ‰¾åˆ°è¿™æ¡çº¿ã€‚å¯¹äºäºŒç»´çš„æ ·æœ¬ï¼ˆç‰¹å¾åˆ†åˆ«ä¸º$x_1$,$x_2$ï¼‰,åˆ™åˆ†ç±»ç»“æœä¸º-1ï¼Œ+1ï¼Œåˆ†åˆ«è¡¨ç¤ºä¸ºnegativeï¼Œpositiveï¼ˆè¿™ä¸å¦ä¸€ç§å¸¸ç”¨çš„åˆ†ç±»ç®—æ³•ä¸ä¸€è‡´ï¼‰ã€‚æ€æƒ³å°±æ˜¯æ¯ä¸ªç‰¹å¾å¯¹åº”ä¸€ä¸ªå‚æ•°ï¼Œä¹˜ç§¯ä¹‹åå¦‚æœå¤§äºæŸä¸ªé˜ˆå€¼ï¼Œåˆ™è®¾å®šä¸ºåˆ†ç±»ç»“æœä¸º+1ï¼Œå°äºåˆ™ä¸º-1ã€‚è‹¥ä¸ºç‰¹å¾æ·»åŠ ä¸€ä¸ªç‰¹å¾$ x_0 $æ’ç­‰äºä¸€ï¼Œåˆ™ç”¨å‘é‡åŒ–å¯ä»¥å°†åˆ†ç±»è¿‡ç¨‹å†™æˆå¦‚ä¸‹å½¢å¼ï¼š</p><script type="math/tex; mode=display">result(X) = sign(W{X^T})</script><p>å…¶ä¸­Wä¸ºç‰¹å¾çš„å‚æ•°å‘é‡ï¼Œæˆ‘ä»¬ä½¿ç”¨$y$æ¥è¡¨ç¤ºæ ·æœ¬çœŸå®ç±»åˆ«ã€‚</p><p>æ•´ä¸ªç®—æ³•çš„æ€æƒ³å…¶å®å¾ˆç®€å•ï¼Œåˆšå¼€å§‹ç”»å‡ºä¸€æ¡çº¿ï¼Œå¦‚æœåˆ†é”™äº†ï¼Œåˆ™å¾€æ­£ç¡®çš„æ–¹å‘æ—‹è½¬è¿™æ¡çº¿ã€‚ä½†æ˜¯å¦‚ä½•æ—‹è½¬è¿™æ¡çº¿ï¼Œæ—‹è½¬å¤šå°‘è§’åº¦ï¼Œè¿˜æ˜¯å¾ˆæœ‰æ„æ€çš„ã€‚ä¸€èˆ¬çš„æƒ³æ³•éƒ½æ˜¯ä»å›¾ç›´è§‚ä¸Šæ¥çœ‹ï¼Œåˆ©ç”¨ä»£ä»·å‡½æ•°ï¼ˆcost functionï¼‰æ¥è§£å†³ï¼Œä¸è¿‡å¯¹äºè¿™ä¸ªç®€å•çš„çº¿æ€§åˆ†ç±»ï¼Œç”¨ä»£ä»·å‡½æ•°è¿›è¡Œæ¢¯åº¦ä¸‹é™è¿˜æ˜¯ç›¸å¯¹æ¥è¯´è®¡ç®—é‡è¿˜æ˜¯æ¯”è¾ƒå¤§çš„ã€‚è¿™ä¸ªç®—æ³•è®©æˆ‘è§‰å¾—å‰å®³çš„åœ°æ–¹åœ¨äºå®ƒå‘é‡åŒ–ä¸å…‰æ˜¯ä¸ºäº†æé«˜è®¡ç®—æ•ˆç‡ï¼Œè€Œæ˜¯ä»å‘é‡çš„è§’åº¦æ¥è€ƒè™‘ä¸€æ­¥æ­¥å‘ç»“æœé€¼è¿‘çš„ï¼šé‡åˆ°çš„åˆ†ç±»é”™è¯¯æœ‰ä¸¤ç§æƒ…å†µï¼Œå¦‚æœ$y$åº”è¯¥æ˜¯æ­£ï¼Œä½†æ˜¯$W{X^T}$å¾—åˆ°çš„æ˜¯è´Ÿçš„ï¼Œä»å‘é‡è§’åº¦æ¥è¯´ï¼Œ$W$ä¸$X$çš„å¤¹è§’å¤ªå¤§ï¼Œå› æ­¤å†…ç§¯ä¸ºè´Ÿï¼Œæˆ‘ä»¬è¦å‡å°ä¸¤ä¸ªå‘é‡çš„è§’åº¦ï¼Œå¯ä»¥ä»¤$W=W+X$,ç›¸åï¼Œ$y$åº”è¯¥æ˜¯è´Ÿï¼Œç»“æœå´ä¸ºæ­£ï¼Œé‚£ä¹ˆ$W$ä¸$X$çš„å¤¹è§’å¤ªå°ï¼Œå¯ä»¥ä»¤$W=W-X$ï¼Œè¿™æ ·å°±å¢å¤§äº†å¤¹è§’ï¼Œä»ä¸‹å›¾å¯ä»¥å¾ˆç›´è§‚çš„çœ‹å‡ºæ¥è¿™ä¸ªé“ç†ï¼š</p><p><img src="https://gss3.bdstatic.com/7Po3dSag_xI4khGkpoWK1HF6hhy/baike/c0%3Dbaike72%2C5%2C5%2C72%2C24/sign=71339aceab773912d02b8d339970ed7d/b3b7d0a20cf431ad4cdf9f4a4936acaf2edd98b0.jpg" alt="å‘é‡åŠ å‡"></p><p>åˆå¹¶ä¸¤ç§æƒ…å†µï¼Œåˆ™æ¯æ¬¡é‡åˆ°è¢«é”™è¯¯åˆ†ç±»çš„æ ·æœ¬$X_n$ï¼Œæˆ‘ä»¬å¯¹å‚æ•°å‘é‡$W_t$è¿›è¡Œå¦‚ä¸‹æ›´æ–°ï¼ˆå…¶ä¸­$t$è¡¨ç¤ºä¿®æ­£çš„æ¬¡æ•°ï¼‰ï¼š<script type="math/tex">W_{t+1} = W_t + y_nX_n</script><br>ä¸‹é¢ä¸»è¦æ¥è¯æ˜ä¸€ä¸‹ï¼Œå¯¹äºçº¿æ€§å¯åˆ†çš„æ ·æœ¬æ¥è¯´ï¼Œè¿™ä¸ªç®—æ³•ä¸€å®šä¼šåœæ­¢ï¼Œæ‰¾åˆ°é‚£æ¡ç¬¦åˆçš„å‚æ•°å‘é‡$W_f$ã€‚æˆ‘ä»¬å¯ä»¥çŸ¥é“ï¼š</p><script type="math/tex; mode=display">y_n{W_f}^TX_n >= _{min}[ y_nX_nW_f^T]>0</script><p>è€Œ</p><script type="math/tex; mode=display">W_{t+1}W_f^T = W_tW_f^t + y_nX_nW_f^T >= W_tW_f^T + _{min}[ y_nX_nW_f^T]</script><p>å› æ­¤æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display">W_{t+1}W_f^T >= W_tW_f^T >= W_0W_f^T +(t+1) *_{min}[ y_nX_nW_f^T]</script><p>å› ä¸ºæˆ‘ä»¬æ¯æ¬¡æ‰¾åˆ°ä¸€ä¸ªå‡ºé”™ç‚¹æ‰è¿›è¡Œä¿®æ­£ï¼Œå‡ºé”™ç‚¹ä¸ºï¼ˆ$X_n,y_n$ï¼‰ï¼Œå¯ä»¥çŸ¥é“$ y_nW_tX_n^T&lt;0 $ï¼Œåˆ™ï¼š</p><script type="math/tex; mode=display">\Vert W_{t+1} \Vert ^2 = \Vert W_t \Vert ^2 + \Vert y_nX_n \Vert ^2+2y_nW_tX_n^T>= \Vert W_t \Vert ^2+_{max}[ \Vert X_n \Vert ^2]</script><p>å› æ­¤æˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š</p><script type="math/tex; mode=display"> \Vert W_{t+1} \Vert ^2 = \Vert W_t \Vert ^2 +  \Vert y_nX_n \Vert ^2+2 \times y_nW_tX_n^T>= \Vert W_0 \Vert ^2+ (t+1) \times _{max}[ \Vert X_n \Vert ^2]</script><p>æœ€åï¼Œå‡è®¾æœ€å¼€å§‹$W_0=0$ï¼Œé€šè¿‡æ­£è§„åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥ç®—å‡º$W_t$ä¸$W_f$ä¹‹é—´çš„è§’åº¦æ˜¯ä¸æ–­é€¼è¿‘çš„ï¼ˆè§’åº¦é‡åˆæ—¶å€™ä¹Ÿå°±å¾—åˆ°äº†æ­£ç¡®çš„å‚æ•°ï¼Œå®é™…ä¸Šè§’åº¦ç›¸åŒåæˆ‘ä»¬ä¸åœ¨ä¹å‘é‡çš„æ¨¡çš„å¤§å°ï¼Œå› ä¸ºå®ƒä¸ä¼šå½±å“$W_t^TX_n$çš„ç¬¦å·ï¼‰ï¼š</p><script type="math/tex; mode=display">\frac{W_tW_f^T}{ \Vert W_t \Vert  \Vert W_f \Vert } >= \frac{t \times _{min}[y_nW_fX_n]}{\sqrt{t} \times _{max} \Vert X_n \Vert  \Vert W_f \Vert } = \sqrt{t} \frac q R</script><p>å…¶ä¸­ï¼š$q = _{min}[y_nW_fX_n],R^2 = ( _{max} \Vert X_n \Vert  )^2$ï¼Œè€Œæ­£è§„åŒ–åå†…ç§¯æ˜¯å°äºç­‰äº1çš„ï¼ˆæ­¤æ—¶æ–¹å‘ä¸€è‡´ï¼‰ï¼Œå¯ä»¥å¾—åˆ°$ t&lt;=\frac {R^2} {q^2} $ï¼Œå¯ä»¥å¾—åˆ°ï¼Œæœ€å¤šç»è¿‡$\frac {R^2} {q^2} $æ¬¡ä¿®æ­£å³å¯å¾—åˆ°æ­£ç¡®ç»“æœã€‚</p><p>åˆ°è¿™é‡Œå°±è¯æ˜ç»“æŸäº†,ä»¥åä¼šä¸Šä¼ ç›¸å…³çš„ä»£ç ã€‚</p><p>å¯ä»¥çœ‹åˆ°è™½ç„¶ç®—æ³•å®ç°å¾ˆç®€å•ï¼Œä½†å…¶ä¸­çš„æ¨ç†è¿˜æ˜¯ä¸å®¹æ˜“çš„ã€‚</p>]]></content>
      
      
      <categories>
          
          <category> æœºå™¨å­¦ä¹  </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
            <tag> classification </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hello World</title>
      <link href="/2018/07/25/hello-world/"/>
      <url>/2018/07/25/hello-world/</url>
      
        <content type="html"><![CDATA[<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æ—¶é—´è¿‡çš„çœŸå¿«ï¼Œè½¬çœ¼é—´ä»å¤§å­¦æ¯•ä¸šå·²ç»ä¸€ä¸ªæœˆäº†ã€‚<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;åœ¨è¿™ä¸¤ä¸ªæœˆçš„æ—¶é—´é‡Œï¼Œæˆ‘ä¸€ç›´æ˜¯â€œç¤¾ä¼šäººâ€ï¼Œå› æ­¤äº«å—ä¸åˆ°é˜¿é‡Œäº‘çš„å­¦ç”Ÿä¼˜æƒ ï¼Œä½œä¸ºä¸€ä¸ªç©·äººï¼Œæˆ‘å¾ˆæœæ–­åœ°æ”¾å¼ƒäº†æˆ‘çš„æ¢¦æƒ³ï¼šå…³æ‰äº†ä¹‹å‰è‡ªå·±ä¸€æ­¥ä¸€æ­¥æ­å‡ºæ¥çš„åšå®¢ã€‚<br><a id="more"></a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ä¹‹å‰çš„åšå®¢æ˜¯ç”¨javaä¸€æ­¥ä¸€æ­¥å †å‡ºæ¥ï¼Œåˆ°äº†æœ€ååŠŸèƒ½ä¹Ÿæ¯”è¾ƒå®Œå–„äº†ï¼Œè™½ç„¶çœ‹çš„äººä¸å¤šï¼ˆæ•°é‡ä¸ºzeroï¼‰ï¼Œä½†æ˜¯æˆ‘æ¯å¤©ä¹Ÿä¼šä¹ æƒ¯æ€§çš„ä¸Šå»è½¬è½¬ï¼Œæœ‰ä»€ä¹ˆæƒ³è¯´çš„ä¹Ÿä¼šå†™è¿›å»ï¼Œæ…¢æ…¢æˆäº†ä¸€ä¸ªä¹ æƒ¯ã€‚æœ€è¿‘åæ¥å¤©æ²¡äº†å®ƒï¼Œæˆ‘æœ‰æ—¶å€™æƒ³è®°å½•ç‚¹å•¥ï¼Œä¹Ÿä¸çŸ¥é“å¾€å“ªé‡Œå»å†™ï¼Œä¸è¿‡ä»Šå¤©æ­å¥½äº†è¿™ä¸ªï¼Œå´åˆä¸çŸ¥é“å†™ä»€ä¹ˆäº†ã€‚<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æ‰€ä»¥è¿™ä¸¤å¤©æˆ‘æƒ³ç€é‡æ–°å¼„ä¸€ä¸ªï¼Œå¾®åšï¼Œcsdnï¼Œç©ºé—´ç­‰ä¸ä¼šç”¨çš„ï¼Œåˆ«é—®æˆ‘ä¸ºä»€ä¹ˆï¼Œæˆ‘ä¹Ÿä¸çŸ¥é“ï¼Œå¯èƒ½å› ä¸ºä»–ä»¬ä¸å¤ŸæŠ˜è…¾å§ğŸ˜‚ã€‚ä½†æ˜¯æˆ‘ç°åœ¨ä¹Ÿæ²¡é’±ä¹°å°æœåŠ¡å™¨ï¼Œæ¯•ç«Ÿå°±æˆ‘ä¸€ä¸ªäººç”¨ï¼Œå®åœ¨æ˜¯æœ‰ç‚¹æµªè´¹ã€‚æ—©å¬è¯´å¾ˆå¤šäººç”¨GitHubæ­åšå®¢ï¼Œäºæ˜¯æˆ‘å°±æ¥å°è¯•ä¸‹è¿™ä¸ªã€‚<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æ­å¾—å·®ä¸å¤šä¹‹åï¼Œæˆ‘ä¹Ÿæœ‰ç‚¹æ˜ç™½äº†å®ƒçš„åŸç†ï¼šåœ¨githubä¸Šï¼Œæ¯ä¸ªrepoéƒ½å¯ä»¥æœ‰github pageé¡µé¢ï¼Œå®ƒæ˜¯ä¸€ä¸ªé™æ€é¡µé¢ï¼Œç”¨æ¥å±•ç°è¿™ä¸ªé¡¹ç›®çš„ä¸€äº›ä¸œè¥¿ã€‚åˆ©ç”¨githubæ­å»ºåšå®¢å°±æ˜¯åŸºäºè¿™ä¸ªé¡µé¢ã€‚å…ˆé€šè¿‡Hexoå†™å¥½æ–‡ç« ç„¶åç”Ÿæˆå±•ç¤ºå®ƒçš„é¡µé¢ï¼ˆåŒ…æ‹¬æ–‡ç« çš„åˆ†ç±»ï¼Œæ£€ç´¢ç­‰éƒ½å·²ç»æœ¬åœ°ç”Ÿæˆç»“æœï¼‰ä¸Šä¼ åˆ°è‡ªå·±çš„githubä¸Šå»ï¼Œç„¶ååˆ©ç”¨è¿™ä¸ªpageå°±å¯ä»¥è®¿é—®äº†ã€‚æ‰€ä»¥å³ä½¿github pageæ˜¯é™æ€é¡µé¢ï¼Œä¾ç„¶å¯ä»¥å®ç°åšå®¢çš„æ­å»ºï¼Œè¿™é‡Œé¢ç¦»ä¸å¼€Hexoçš„åŠŸåŠ³ï¼Œæ‰€æœ‰â€œåŠ¨æ€â€çš„ä¸šåŠ¡åœ¨è‡ªå·±çš„ç”µè„‘ä¸Šå·²ç»å®Œæˆäº†ã€‚<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;è¿™ä¸ªåšæ³•è¿˜çœŸæ˜¯æŒºç‰›é€¼çš„ã€‚<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;è‡³äºæ€ä¹ˆæ­å»ºåšå®¢ï¼Œç½‘ä¸Šä¸€æŸ¥ä¸€å¤§å †ï¼Œç…§ç€æ¥å°±å¥½ï¼Œå°±ä¸è¯´äº†ã€‚å› ä¸ºä¸ç”¨è‡ªå·±å†™ä»€ä¹ˆä»£ç ï¼Œç›¸å¯¹æ¥è¯´è¿˜æ˜¯å¾ˆå®¹æ˜“çš„ã€‚<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;æœ€åï¼ŒæŒ‰ç…§æƒ¯ä¾‹è¯´å¥ï¼š </p><h1 id="Hello-world"><a href="#Hello-world" class="headerlink" title="Hello,world!"></a>Hello,world!</h1>]]></content>
      
      
      <categories>
          
          <category> çŒæ°´ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> casual note </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
